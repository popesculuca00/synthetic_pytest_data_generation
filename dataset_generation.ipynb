{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocess\n",
    "import concurrent.futures\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# from pytest_runner import \n",
    "from adjustment_utils import adjust_pytest, run_pytest\n",
    "from agent import CodeAgent\n",
    "from client_interface.client import ClientInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"vikp/python_functions_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>quality_prob</th>\n",
       "      <th>learning_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def covariance(x, y):\\n    \"\"\"Compute covarian...</td>\n",
       "      <td>0.985363</td>\n",
       "      <td>0.976558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def LikelihoodRatioSignificance(LLnull, LLalt,...</td>\n",
       "      <td>0.981207</td>\n",
       "      <td>0.775137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def spectralflux_wavelength_to_frequency(flux,...</td>\n",
       "      <td>0.977532</td>\n",
       "      <td>0.993036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def plot_contributions(ax, top_n, bar_dims, ba...</td>\n",
       "      <td>0.975866</td>\n",
       "      <td>0.601798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def scale_to_internal(vec, scaling_factor, sca...</td>\n",
       "      <td>0.975498</td>\n",
       "      <td>0.671538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58338</th>\n",
       "      <td>def add_assignment_as_district_col(partition):...</td>\n",
       "      <td>0.700075</td>\n",
       "      <td>0.625481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58339</th>\n",
       "      <td>def get_range(value):\\n    \"\"\"\\n    A simple t...</td>\n",
       "      <td>0.700075</td>\n",
       "      <td>0.654840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58340</th>\n",
       "      <td>def forwardsAdditiveError(image, template):\\n ...</td>\n",
       "      <td>0.700075</td>\n",
       "      <td>0.804175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58341</th>\n",
       "      <td>def calculate_iou(bbox1, bbox2):\\n    \"\"\"\\n   ...</td>\n",
       "      <td>0.700075</td>\n",
       "      <td>0.761028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58342</th>\n",
       "      <td>def byte_int(raw):\\n    \"\"\"Convert bytes from ...</td>\n",
       "      <td>0.700075</td>\n",
       "      <td>0.598899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58343 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    code  quality_prob  \\\n",
       "0      def covariance(x, y):\\n    \"\"\"Compute covarian...      0.985363   \n",
       "1      def LikelihoodRatioSignificance(LLnull, LLalt,...      0.981207   \n",
       "2      def spectralflux_wavelength_to_frequency(flux,...      0.977532   \n",
       "3      def plot_contributions(ax, top_n, bar_dims, ba...      0.975866   \n",
       "4      def scale_to_internal(vec, scaling_factor, sca...      0.975498   \n",
       "...                                                  ...           ...   \n",
       "58338  def add_assignment_as_district_col(partition):...      0.700075   \n",
       "58339  def get_range(value):\\n    \"\"\"\\n    A simple t...      0.700075   \n",
       "58340  def forwardsAdditiveError(image, template):\\n ...      0.700075   \n",
       "58341  def calculate_iou(bbox1, bbox2):\\n    \"\"\"\\n   ...      0.700075   \n",
       "58342  def byte_int(raw):\\n    \"\"\"Convert bytes from ...      0.700075   \n",
       "\n",
       "       learning_prob  \n",
       "0           0.976558  \n",
       "1           0.775137  \n",
       "2           0.993036  \n",
       "3           0.601798  \n",
       "4           0.671538  \n",
       "...              ...  \n",
       "58338       0.625481  \n",
       "58339       0.654840  \n",
       "58340       0.804175  \n",
       "58341       0.761028  \n",
       "58342       0.598899  \n",
       "\n",
       "[58343 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset[\"train\"])\n",
    "df = df.sort_values(by=\"quality_prob\", \n",
    "                    ascending=False).\\\n",
    "        reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>quality_prob</th>\n",
       "      <th>learning_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def covariance(x, y):\\n    \\n    n_samples, ho...</td>\n",
       "      <td>0.985363</td>\n",
       "      <td>0.976558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def LikelihoodRatioSignificance(LLnull, LLalt,...</td>\n",
       "      <td>0.981207</td>\n",
       "      <td>0.775137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def spectralflux_wavelength_to_frequency(flux,...</td>\n",
       "      <td>0.977532</td>\n",
       "      <td>0.993036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def plot_contributions(ax, top_n, bar_dims, ba...</td>\n",
       "      <td>0.975866</td>\n",
       "      <td>0.601798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def scale_to_internal(vec, scaling_factor, sca...</td>\n",
       "      <td>0.975498</td>\n",
       "      <td>0.671538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58338</th>\n",
       "      <td>def add_assignment_as_district_col(partition):...</td>\n",
       "      <td>0.700075</td>\n",
       "      <td>0.625481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58339</th>\n",
       "      <td>def get_range(value):\\n    \\n    return range(...</td>\n",
       "      <td>0.700075</td>\n",
       "      <td>0.654840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58340</th>\n",
       "      <td>def forwardsAdditiveError(image, template):\\n ...</td>\n",
       "      <td>0.700075</td>\n",
       "      <td>0.804175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58341</th>\n",
       "      <td>def calculate_iou(bbox1, bbox2):\\n    \\n    xm...</td>\n",
       "      <td>0.700075</td>\n",
       "      <td>0.761028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58342</th>\n",
       "      <td>def byte_int(raw):\\n    \\n    return int.from_...</td>\n",
       "      <td>0.700075</td>\n",
       "      <td>0.598899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58343 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    code  quality_prob  \\\n",
       "0      def covariance(x, y):\\n    \\n    n_samples, ho...      0.985363   \n",
       "1      def LikelihoodRatioSignificance(LLnull, LLalt,...      0.981207   \n",
       "2      def spectralflux_wavelength_to_frequency(flux,...      0.977532   \n",
       "3      def plot_contributions(ax, top_n, bar_dims, ba...      0.975866   \n",
       "4      def scale_to_internal(vec, scaling_factor, sca...      0.975498   \n",
       "...                                                  ...           ...   \n",
       "58338  def add_assignment_as_district_col(partition):...      0.700075   \n",
       "58339  def get_range(value):\\n    \\n    return range(...      0.700075   \n",
       "58340  def forwardsAdditiveError(image, template):\\n ...      0.700075   \n",
       "58341  def calculate_iou(bbox1, bbox2):\\n    \\n    xm...      0.700075   \n",
       "58342  def byte_int(raw):\\n    \\n    return int.from_...      0.700075   \n",
       "\n",
       "       learning_prob  \n",
       "0           0.976558  \n",
       "1           0.775137  \n",
       "2           0.993036  \n",
       "3           0.601798  \n",
       "4           0.671538  \n",
       "...              ...  \n",
       "58338       0.625481  \n",
       "58339       0.654840  \n",
       "58340       0.804175  \n",
       "58341       0.761028  \n",
       "58342       0.598899  \n",
       "\n",
       "[58343 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.code = df.code.str.replace(r'(\\'\\'\\'[\\s\\S]*?\\'\\'\\'|\\\"\\\"\\\"[\\s\\S]*?\\\"\\\"\\\")', \"\", regex=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tests(code):\n",
    "    try:\n",
    "\n",
    "        prompt = \"\"\"You are a useful code assistant. A user will give you a code representing a python file. \n",
    "Your task is to take said code and generate a complete working testing file using Pytest. \n",
    "You may assume the original code can be found in a `source.py` file residing in the same directory as the test file, you must import it as such.\n",
    "You *MUST* always provide the full test code with no other explanations.\n",
    "You *MUST* use only one assertion per test. Always aim for full code coverage. \n",
    "\"\"\"\n",
    "        from adjustment_utils import modify_pytest_code, run_pytest\n",
    "        from agent import CodeAgent\n",
    "        model = CodeAgent(prompt)\n",
    "\n",
    "        results = model.generate_response(code, n= 3)\n",
    "        test_results =[]\n",
    "        for result in results:\n",
    "            try:\n",
    "                res = run_pytest(code, result)\n",
    "                if not len(res[\"stderr\"]):\n",
    "                    if (res[\"failed_assertions\"] == 0) and (res[\"coverage\"] > 90):\n",
    "                        return (code, result)\n",
    "                    res[\"pytest_code\"] = result\n",
    "                    test_results.append(res)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            test_results = sorted(test_results, key=lambda x: x[\"coverage\"], reverse=True)\n",
    "\n",
    "            pytest_code = \"\"\n",
    "            adjusted_results = []\n",
    "            for test_result in test_results:\n",
    "\n",
    "                try:\n",
    "                    adjusted = modify_pytest_code(code, test_result[\"pytest_code\"])\n",
    "\n",
    "                    res = run_pytest(code, adjusted)\n",
    "                    res[\"pytest_code\"] = result\n",
    "\n",
    "                    if (not len(res[\"stderr\"])) and (res[\"failed_assertions\"]==0):\n",
    "                        if res[\"coverage\"] > 95:\n",
    "                            return (code, adjusted)\n",
    "                        adjusted_results.append(res)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "            pytest_code = max(adjusted_results, key=lambda x: x[\"coverage\"])[\"pytest_code\"]\n",
    "            return (code, pytest_code)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3ab14044964e6ab4fcb8ad427ff897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336dcdd6dffe4405962b2b47e75dd3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/966 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 46000_47000_dataset_results.csv with 737 100% coverage tests\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fb8b85262d433ca21683ec0a2fdb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f790619920c4645a5996d14ec10db02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 47000_48000_dataset_results.csv with 699 100% coverage tests\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e864e637af464a57b6202fbcb156c90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614fe17dfa8f4f00a025682fb61adbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/952 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 48000_49000_dataset_results.csv with 709 100% coverage tests\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f09d5b7ff474f4f8e4d4de5ff807398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc480f13f47547ffb27dbe3ad7c83f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/965 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 49000_50000_dataset_results.csv with 753 100% coverage tests\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524ea011cbb64ff39fcf169f59295d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4a8899a35d441aadee015dcb1019c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/941 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50000_51000_dataset_results.csv with 728 100% coverage tests\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210408dda9a7485dbc46a533924a7c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a53f74be9b847be9522a6c58901a834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/956 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 51000_52000_dataset_results.csv with 731 100% coverage tests\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861c8bed9b3e4214a3e37b124f0db572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bca552e6d894b69af63009e751c97b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/956 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 52000_53000_dataset_results.csv with 716 100% coverage tests\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2482e5a5f4404cf2ae6542c40227fbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3652a119356d4047b9ee64f6b4aaa192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/959 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 53000_54000_dataset_results.csv with 732 100% coverage tests\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8168e208f27346a896dfbc11ac6e647a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step_size = 1000\n",
    "\n",
    "for j in range(46_000, 59_000, step_size):\n",
    "\n",
    "\n",
    "    input_code = df.code[j:j+step_size]\n",
    "    num_tasks = len(input_code)\n",
    "\n",
    "    with multiprocess.Pool(24) as pool:\n",
    "        results = list(tqdm(pool.imap(create_tests, input_code), total=num_tasks))\n",
    "\n",
    "\n",
    "    results = [result for result in results if result is not None]\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.columns = [\"original_code\", \"pytest_code\"]\n",
    "    df_results = df_results.dropna().reset_index(drop=True)\n",
    "    # df_results[\"coverage\"] = df_results.apply(lambda x: run_pytest(x.iloc[0], x.iloc[1])[\"coverage\"], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    def get_cov(row):\n",
    "        if (row is None):\n",
    "            return (None, None, None)\n",
    "\n",
    "        code, pytest_code = row\n",
    "\n",
    "        if (code is None) or (pytest_code is None):\n",
    "            return (None, None, None)\n",
    "        \n",
    "        from adjustment_utils import run_pytest\n",
    "        \n",
    "        coverage = run_pytest(code, pytest_code, random_subdir=True)[\"coverage\"]\n",
    "        return (code, pytest_code, coverage)\n",
    "\n",
    "    num_tasks = len(results)\n",
    "\n",
    "    with multiprocess.Pool(24) as pool:\n",
    "        results_cov = list(tqdm(pool.imap(get_cov, results), total=num_tasks))\n",
    "\n",
    "    df_cov = pd.DataFrame(results_cov).dropna()\n",
    "    df_cov.columns = [\"original_code\", \"pytest_code\", \"coverage\"]\n",
    "    df_cov = df_cov.sort_values(by=\"coverage\", ascending=False).reset_index(drop=True)\n",
    "    df_cov.to_csv(f\"{j}_{j + step_size}_dataset_results.csv\", index=False)\n",
    "\n",
    "    print(f\"Saved {j}_{j + step_size}_dataset_results.csv with {df_cov[df_cov.coverage > 99].shape[0]} 100% coverage tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01madjustment_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m modify_pytest_code\n\u001b[1;32m----> 3\u001b[0m code, test_result \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m adjusted \u001b[38;5;241m=\u001b[39m modify_pytest_code(code, test_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytest_code\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "from adjustment_utils import modify_pytest_code\n",
    "\n",
    "code, test_result = results[0]\n",
    "\n",
    "adjusted = modify_pytest_code(code, test_result[\"pytest_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import sys\n",
      "sys.path.insert(0, '../')\n",
      "from source import extract_stresses_and_forces\n",
      "import numpy as np\n",
      "\n",
      "def test_extract_stresses_and_forces():\n",
      "    fit_data = ...\n",
      "    values = ...\n",
      "    args = ...\n",
      "    dft_forces, ip_forces, dft_stresses, ip_stresses = extract_stresses_and_forces(fit_data, values, args)\n",
      "    assert np.allclose(dft_forces, ip_forces), 'Forces are not the same'\n"
     ]
    }
   ],
   "source": [
    "print(adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TypeError(\"expected string or bytes-like object, got 'dict'\"),\n",
       " TypeError(\"expected string or bytes-like object, got 'dict'\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [idx for idx, res in enumerate(results) if res and run_pytest(res[0], res[1])[\"coverage\"] > 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_code</th>\n",
       "      <th>pytest_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def extract_stresses_and_forces(fit_data, valu...</td>\n",
       "      <td>import pytest\\nfrom source import extract_stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import torch\\n\\ndef mvdigamma(vec: torch.Float...</td>\n",
       "      <td>import torch\\nimport pytest\\nfrom source impor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>import torch\\n\\ndef amplitude_to_db(x, ref=1.0...</td>\n",
       "      <td>import torch\\nimport pytest\\nfrom source impor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def HHV_modified_Dulong(mass_fractions):\\n    ...</td>\n",
       "      <td>import pytest\\nimport sys\\nimport os\\nsys.path...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def intersect_ray_plane(o,w,p,n):\\n    \\n    #...</td>\n",
       "      <td># source.py\\n\\nimport torch\\n\\nclass Ray:\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>def HamCenter1D_Hamiltonian(t, u, PARAMETERS =...</td>\n",
       "      <td>import pytest\\nfrom source import *\\nimport nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>def update_means(means, X, perm_out):\\n    r\\n...</td>\n",
       "      <td>import os\\nimport pytest\\nimport numpy as np\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>def luminance_newhall1943(V, **kwargs):\\n    \\...</td>\n",
       "      <td>import sys\\nsys.path.append(\".\")  # This will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>import torch\\n\\ndef batchwise_cdist(samples1, ...</td>\n",
       "      <td>import pytest\\nimport torch\\nfrom torch.testin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>def get_bandwidth(n, var_ret, var_noise, kerne...</td>\n",
       "      <td># import the function to be tested\\nfrom sourc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>def project_ecef_vector_onto_basis(x, y, z, xx...</td>\n",
       "      <td># test_source.py\\n\\nimport pytest\\nimport os\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>def _adjust_ts_and_batch(data, batch_first, se...</td>\n",
       "      <td>import pytest\\nfrom source import _adjust_ts_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>import torch\\n\\ndef get_feat_size(block, spati...</td>\n",
       "      <td>import pytest\\nimport torch\\nfrom source impor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>def compute_loss(pred, gold, ntokens, criterio...</td>\n",
       "      <td>import sys\\nsys.path.append(\".\")  # To import ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>def nearest(items, pivot):\\n    \\n    return m...</td>\n",
       "      <td>import sys\\nsys.path.insert(0, '../')\\nfrom so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>def calc_ta_fwhm(freq, array_phase='P2C'):\\n  ...</td>\n",
       "      <td>import pytest\\nfrom source import calc_ta_fwhm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        original_code  \\\n",
       "0   def extract_stresses_and_forces(fit_data, valu...   \n",
       "1   import torch\\n\\ndef mvdigamma(vec: torch.Float...   \n",
       "2   import torch\\n\\ndef amplitude_to_db(x, ref=1.0...   \n",
       "3   def HHV_modified_Dulong(mass_fractions):\\n    ...   \n",
       "4   def intersect_ray_plane(o,w,p,n):\\n    \\n    #...   \n",
       "5   def HamCenter1D_Hamiltonian(t, u, PARAMETERS =...   \n",
       "6   def update_means(means, X, perm_out):\\n    r\\n...   \n",
       "7   def luminance_newhall1943(V, **kwargs):\\n    \\...   \n",
       "8   import torch\\n\\ndef batchwise_cdist(samples1, ...   \n",
       "9   def get_bandwidth(n, var_ret, var_noise, kerne...   \n",
       "10  def project_ecef_vector_onto_basis(x, y, z, xx...   \n",
       "11  def _adjust_ts_and_batch(data, batch_first, se...   \n",
       "12  import torch\\n\\ndef get_feat_size(block, spati...   \n",
       "13  def compute_loss(pred, gold, ntokens, criterio...   \n",
       "14  def nearest(items, pivot):\\n    \\n    return m...   \n",
       "15  def calc_ta_fwhm(freq, array_phase='P2C'):\\n  ...   \n",
       "\n",
       "                                          pytest_code  \n",
       "0   import pytest\\nfrom source import extract_stre...  \n",
       "1   import torch\\nimport pytest\\nfrom source impor...  \n",
       "2   import torch\\nimport pytest\\nfrom source impor...  \n",
       "3   import pytest\\nimport sys\\nimport os\\nsys.path...  \n",
       "4   # source.py\\n\\nimport torch\\n\\nclass Ray:\\n   ...  \n",
       "5   import pytest\\nfrom source import *\\nimport nu...  \n",
       "6   import os\\nimport pytest\\nimport numpy as np\\n...  \n",
       "7   import sys\\nsys.path.append(\".\")  # This will ...  \n",
       "8   import pytest\\nimport torch\\nfrom torch.testin...  \n",
       "9   # import the function to be tested\\nfrom sourc...  \n",
       "10  # test_source.py\\n\\nimport pytest\\nimport os\\n...  \n",
       "11  import pytest\\nfrom source import _adjust_ts_a...  \n",
       "12  import pytest\\nimport torch\\nfrom source impor...  \n",
       "13  import sys\\nsys.path.append(\".\")  # To import ...  \n",
       "14  import sys\\nsys.path.insert(0, '../')\\nfrom so...  \n",
       "15  import pytest\\nfrom source import calc_ta_fwhm...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [result for result in results if result is not None]\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.columns = [\"original_code\", \"pytest_code\"]\n",
    "df_results = df_results.dropna().reset_index(drop=True)\n",
    "# df_results[\"coverage\"] = df_results.apply(lambda x: run_pytest(x.iloc[0], x.iloc[1])[\"coverage\"], axis=1)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results.to_csv(\"3.5k_3.5k_dataset_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0504672a244147a80e3ef31bd0185a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_cov(row):\n",
    "    if (row is None):\n",
    "        return (None, None, None)\n",
    "\n",
    "    code, pytest_code = row\n",
    "\n",
    "    if (code is None) or (pytest_code is None):\n",
    "        return (None, None, None)\n",
    "    \n",
    "    from adjustment_utils import run_pytest, modify_pytest_code\n",
    "    \n",
    "    coverage = run_pytest(code, pytest_code, random_subdir=True)[\"coverage\"]\n",
    "    return (code, pytest_code, coverage)\n",
    "\n",
    "num_tasks = len(results)\n",
    "\n",
    "\n",
    "\n",
    "# def modify_code(row):\n",
    "#     if (row is None):\n",
    "#         return (None, None, None)\n",
    "#     from adjustment_utils import run_pytest, modify_pytest_code\n",
    "#     code, pytest_code = row\n",
    "\n",
    "#     try:\n",
    "#         working_test = modify_pytest_code(code, pytest_code)\n",
    "#     except Exception as e:\n",
    "#         return code, e\n",
    "    \n",
    "# #     return code, working_test\n",
    "\n",
    "\n",
    "# with multiprocess.Pool(24) as pool:\n",
    "#     modified_results = list(tqdm(pool.imap(modify_code, results[:100]), total=num_tasks))\n",
    "\n",
    "\n",
    "with multiprocess.Pool(24) as pool:\n",
    "    results_cov = list(tqdm(pool.imap(get_cov, results), total=num_tasks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"def fail_safe(temperature, neutrons_produced_per_second, threshold):\\n    \\n\\n    result = temperature * neutrons_produced_per_second\\n\\n    if result < threshold * 0.9:\\n        return 'LOW'\\n    if 0.9 * threshold <= result <= 1.1 * threshold:\\n        return 'NORMAL'\\n    return 'DANGER'\",\n",
       "  \"# This is the test_source.py file\\nimport pytest\\nfrom source import Source\\n\\nclass TestSource:\\n\\n    def test_fail_safe_low(self):\\n        assert Source.fail_safe(20, 50, 1000) == 'LOW'\\n\\n    def test_fail_safe_normal(self):\\n        assert Source.fail_safe(50, 50, 1000) == 'NORMAL'\\n\\n    def test_fail_safe_danger(self):\\n        assert Source.fail_safe(100, 50, 1000) == 'DANGER'\",\n",
       "  14.0),\n",
       " ('import torch\\n\\ndef psnr(original_img, resoluted_img):\\n    \\n\\n    # fisrt computing th emse\\n    resoluted_img = (resoluted_img * 255).round().clamp(0, 255) / 255\\n    subs = original_img - resoluted_img\\n    mse = subs.pow(2).mean([-3, -2, -1])\\n\\n    # formular for psnr (https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio)\\n    max_val = 1.0\\n    psnr = 20*torch.log10(torch.tensor(max_val)) - 10*torch.log10(mse)\\n    idx = torch.where(psnr > 120)[0]  # avoid overflow\\n    psnr[idx] = 120\\n    psnr = psnr.mean()\\n\\n    return psnr',\n",
       "  'import pytest\\nimport torch\\n\\nfrom source import psnr  # assuming the function is in source.py\\n\\ndef test_psnr():\\n    original_img = torch.rand((1, 3, 256, 256))\\n    resoluted_img = torch.rand((1, 3, 256, 256))\\n\\n    result = psnr(original_img, resoluted_img)\\n\\n    assert 0 <= result <= 120, \"The PSNR value is out of range\"',\n",
       "  100.0),\n",
       " ('def compute_empirical_pvalue(value, surrogates):\\n    \\n\\n    return sum(surrogates > value) / len(surrogates)',\n",
       "  '# test_source.py\\nimport pytest\\nimport numpy as np\\nfrom source import compute_empirical_pvalue\\n\\ndef test_compute_empirical_pvalue():\\n    # Test with some example values\\n    value = 5\\n    surrogates = np.array([1, 2, 3, 4, 5, 6])\\n    assert compute_empirical_pvalue(value, surrogates) == 0.5',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef distance2bbox(points, distance, max_shape=None):\\n    \\n    x1 = points[:, 0] - distance[:, 0]\\n    y1 = points[:, 1] - distance[:, 1]\\n    x2 = points[:, 0] + distance[:, 2]\\n    y2 = points[:, 1] + distance[:, 3]\\n    if max_shape is not None:\\n        x1 = x1.clamp(min=0, max=max_shape[1])\\n        y1 = y1.clamp(min=0, max=max_shape[0])\\n        x2 = x2.clamp(min=0, max=max_shape[1])\\n        y2 = y2.clamp(min=0, max=max_shape[0])\\n    return torch.stack([x1, y1, x2, y2], -1)',\n",
       "  'import pytest\\nimport torch\\nfrom source import distance2bbox\\n\\ndef test_distance2bbox():\\n    points = torch.tensor([[0, 0], [1, 2], [3, 4]])\\n    distance = torch.tensor([[1, 1, 1, 1]])\\n    max_shape = (5, 5)\\n    expected = torch.tensor([[0, 0, 1, 1], [0, 1, 1, 2], [0, 1, 2, 2]])\\n    result = distance2bbox(points, distance, max_shape)\\n    assert torch.allclose(result, expected)\\ntest_distance2bbox()',\n",
       "  100.0),\n",
       " ('def fixIoU(bb1, bb2):\\n    \\n    assert bb1[0] < bb1[2] and bb1[1] < bb1[3], print(bb1)\\n    assert bb2[0] < bb2[2] and bb2[1] < bb2[3], print(bb2)\\n\\n    # determine the coordinates of the intersection rectangle\\n    x_left = max(bb1[0], bb2[0]);  y_top = max(bb1[1], bb2[1])\\n    x_right = min(bb1[2], bb2[2]); y_bottom = min(bb1[3], bb2[3])\\n    if x_right < x_left or y_bottom < y_top: return 0.0\\n\\n    # The intersection of two axis-aligned bounding boxes is always an\\n    # axis-aligned bounding box\\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\\n    # compute the area of both AABBs\\n    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\\n    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\\n    # compute the fix intersection over union by taking the intersection\\n    fixiou = intersection_area / min(bb1_area, bb2_area)\\n    assert fixiou >= 0.0 and fixiou <= 1.0\\n    return fixiou',\n",
       "  'import pytest\\nimport source  # assuming source.py is in the same directory\\n\\nclass TestFixIoU:\\n\\n    def test_fixIoU(self):\\n        bb1 = [1, 2, 3, 4]\\n        bb2 = [0, 1, 2, 3]\\n        assert source.fixIoU(bb1, bb2) == 1.0\\n\\n        bb1 = [0, 0, 10, 10]\\n        bb2 = [5, 5, 15, 15]\\n        assert source.fixIoU(bb1, bb2) == 1.0\\n\\n        bb1 = [0, 0, 10, 10]\\n        bb2 = [11, 11, 20, 20]\\n        assert source.fixIoU(bb1, bb2) == 0.0\\n        \\n        bb1 = [5, 5, 10, 10]\\n        bb2 = [5, 5, 10, 10]\\n        assert source.fixIoU(bb1, bb2) == 0.25\\n\\n        bb1 = [0, 0, 0, 0]\\n        bb2 = [0, 0, 0, 0]\\n        assert source.fixIoU(bb1, bb2) == 0.0\\n\\n        bb1 = [10, 10, 20, 20]\\n        bb2 = [11, 11, 21, 21]\\n        assert source.fixIoU(bb1, bb2) == 0.0\\n\\n        bb1 = [0, 0, 10, 10]\\n        bb2 = [10, 10, 20, 20]\\n        assert source.fixIoU(bb1, bb2) == 0.125',\n",
       "  100.0),\n",
       " ('def clip_outliers(df, std_threshold: float = 3):\\n    \\n    df_std = df.std(axis=0, skipna=True)\\n    df_mean = df.mean(axis=0, skipna=True)\\n\\n    lower = df_mean - (df_std * std_threshold)\\n    upper = df_mean + (df_std * std_threshold)\\n    df2 = df.clip(lower=lower, upper=upper, axis=1)\\n\\n    return df2',\n",
       "  '# test_clip_outliers.py\\nimport pytest\\nimport os\\nimport pandas as pd\\nfrom source import clip_outliers\\n\\n@pytest.fixture\\ndef data():\\n    path = os.path.join(os.path.dirname(__file__), \\'data.csv\\')\\n    return pd.read_csv(path)\\n\\ndef test_clip_outliers_std_threshold_default(data):\\n    result = clip_outliers(data)\\n    assert not result.isnull().values.any(), \"Result contains NaN values\"',\n",
       "  14.0),\n",
       " ('def rotate_nmmpmat_block(denmat, orbital, phi=None, theta=None):\\n    \\n    from masci_tools.io.common_functions import get_wigner_matrix\\n\\n    if theta is None and phi is None:\\n        return denmat\\n\\n    if phi is None:\\n        phi = 0.0\\n    if theta is None:\\n        theta = 0.0\\n\\n    d_wigner = get_wigner_matrix(orbital, phi, theta)\\n    #Rotate the density matrix\\n    denmat = d_wigner.T.conj().dot(denmat.dot(d_wigner))\\n\\n    return denmat',\n",
       "  'import pytest\\nfrom source import rotate_nmmpmat_block\\nfrom masci_tools.io.common_functions import get_wigner_matrix\\nimport numpy as np\\n\\ndef test_rotate_nmmpmat_block():\\n    #denmat is a 2D array\\n    denmat = np.array([[1,2,3],[4,5,6],[7,8,9]])\\n    #orbital is an integer\\n    orbital = 1\\n    #phi is a float\\n    phi = 0.5\\n    #theta is a float\\n    theta = 0.5\\n\\n    result = rotate_nmmpmat_block(denmat, orbital, phi, theta)\\n    #We only have one assertion, so this will cover full code coverage\\n    assert isinstance(result, np.ndarray), \"The function did not return a numpy ndarray\"',\n",
       "  9.0),\n",
       " ('def electrode_distance(latlong_a, latlong_b, radius=100.0):\\n    \\n    import math\\n\\n    lat1, lon1 = latlong_a\\n    lat2, lon2 = latlong_b\\n\\n    dLat = math.radians(lat2 - lat1)\\n    dLon = math.radians(lon2 - lon1)\\n\\n    a = math.sin(dLat / 2) * math.sin(dLat / 2) + math.cos(\\n        math.radians(lat1)\\n    ) * math.cos(math.radians(lat2)) * math.sin(dLon / 2) * math.sin(dLon / 2)\\n\\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\\n    d = radius * c\\n    return d',\n",
       "  'import pytest\\nfrom source import electrode_distance\\n\\ndef test_electrode_distance():\\n    assert electrode_distance((40.7128, 74.0060), (34.0522, 118.8766)) > 3958.74',\n",
       "  100.0),\n",
       " ('def muted_color(color: str, alpha: float = 0.2):\\n    \\n\\n    return dict(\\n        color=color,\\n        muted_color=color,\\n        muted_alpha=alpha,\\n    )',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import muted_color\\n\\ndef test_muted_color():\\n    result = muted_color(\"blue\")\\n    assert result[\"color\"] == \"blue\"\\n    assert result[\"muted_color\"] == \"blue\"\\n    assert result[\"muted_alpha\"] == 0.2',\n",
       "  100.0),\n",
       " ('def tukey(data, k=1.5):\\n    r\\n    first_quartile = data.quantile(0.25)\\n    third_quartile = data.quantile(0.75)\\n    iqr = third_quartile - first_quartile\\n    return ((data < (first_quartile - k*iqr))\\n            | (data > (third_quartile + k*iqr)))',\n",
       "  '# source.py\\nimport pandas as pd\\n\\ndef tukey(data, k=1.5):\\n    first_quartile = data.quantile(0.25)\\n    third_quartile = data.quantile(0.75)\\n    iqr = third_quartile - first_quartile\\n    return ((data < (first_quartile - k*iqr))\\n            | (data > (third_quartile + k*iqr)))',\n",
       "  0.0),\n",
       " ('def format_encoding(encoding, batch_size, num_trgt, repeat=True):\\n    \\n\\n    # because we need to pair it with every target\\n    if repeat:\\n        encoding = encoding.repeat(1, num_trgt, 1)\\n    encoding_stacked = encoding.view(batch_size * num_trgt, -1)\\n\\n    return encoding_stacked',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")  # Adds the current directory to the python path\\nimport source  # Importing the source file\\nimport pytest  # Importing pytest\\n\\n\\ndef test_format_encoding():\\n    \"\"\"\\n    Test function for format_encoding.\\n    \"\"\"\\n    # Setup\\n    encoding = None  # Assign or generate the value for encoding here\\n    batch_size = 1  # Assign or generate the value for batch_size here\\n    num_trgt = 1  # Assign or generate the value for num_trgt here\\n    repeat = True  # Assign or generate the value for repeat here\\n\\n    # Call the function and assign the result to a variable\\n    result = source.format_encoding(encoding, batch_size, num_trgt, repeat)\\n\\n    # Assertion\\n    # Replace the assert statement with the actual condition you want to test\\n    assert result is not None, \"Expected a non-None result\"\\n\\n\\n# This is the standard call to run Pytest\\nif __name__ == \"__main__\":\\n    pytest.main([__file__])',\n",
       "  60.0),\n",
       " (\"def detect(detection_model, interpreter, input_tensor, session):\\n  \\n  input_details = interpreter.get_input_details()\\n  output_details = interpreter.get_output_details()\\n\\n  # We use the original model for pre-processing, since the TFLite model doesn't\\n  # include pre-processing.\\n  preprocessed_image, _ = detection_model.preprocess(input_tensor)\\n  interpreter.set_tensor(input_details[0]['index'], preprocessed_image.eval(session=session))\\n\\n  interpreter.invoke()\\n\\n  boxes = interpreter.get_tensor(output_details[0]['index'])\\n  classes = interpreter.get_tensor(output_details[1]['index'])\\n  scores = interpreter.get_tensor(output_details[2]['index'])\\n  return boxes, classes, scores\",\n",
       "  \"import sys\\nsys.path.insert(0, './')\\nfrom source import detect\\n\\ndef test_detect():\\n    detection_model = None\\n    interpreter = None\\n    input_tensor = None\\n    session = None\\n    try:\\n        boxes, classes, scores = detect(detection_model, interpreter, input_tensor, session)\\n        assert len(boxes) == 1\\n    except Exception as e:\\n        assert False, f'An unexpected error occurred: {e}'\",\n",
       "  20.0),\n",
       " (\"def load_pretrained_model(config, datasets, weights_filepath, model_filepath):\\n    \\n    if config.model_name == 'mt-lstm-crf':\\n        from ..models.multi_task_lstm_crf import MultiTaskLSTMCRF\\n        model = MultiTaskLSTMCRF(config, datasets)\\n    model.load(weights_filepath, model_filepath)\\n    model.compile()\\n\\n    return model\",\n",
       "  'import pytest\\nfrom pathlib import Path\\nfrom ..source import load_pretrained_model, Config, Datasets\\n\\nclass TestLoadPretrainedModel:\\n    \\n    @pytest.fixture\\n    def config(self):\\n        config = Config()\\n        config.model_name = \\'mt-lstm-crf\\'\\n        return config\\n    \\n    @pytest.fixture\\n    def datasets(self):\\n        datasets = Datasets()\\n        return datasets\\n    \\n    @pytest.fixture\\n    def weights_filepath(self):\\n        return Path(\"path_to_weights_file\")\\n    \\n    @pytest.fixture\\n    def model_filepath(self):\\n        return Path(\"path_to_model_file\")\\n    \\n    def test_load_pretrained_model(self, config, datasets, weights_filepath, model_filepath):\\n        model = load_pretrained_model(config, datasets, weights_filepath, model_filepath)\\n        assert model is not None',\n",
       "  0.0),\n",
       " ('def to_time_sec(time_str):\\n    \\n\\n    try:\\n        time_float = float(time_str)\\n    except ValueError:\\n        raise ValueError(\"argument must represent a floating point number\")\\n    if time_float < 0:\\n        raise ValueError(\"argument must be a non-negative floating point number\")\\n    return time_float',\n",
       "  'import pytest\\nfrom source import to_time_sec\\n\\ndef test_to_time_sec_valid_input():\\n    assert to_time_sec(\"12.34\") == 12.34\\n\\ndef test_to_time_sec_invalid_input():\\n    with pytest.raises(ValueError):\\n        to_time_sec(\"-1\")\\n\\ndef test_to_time_sec_non_numeric_input():\\n    with pytest.raises(ValueError):\\n        to_time_sec(\"abc\")',\n",
       "  100.0),\n",
       " ('def calculate_truncation_error(reduced_density_matrix_evals):\\n    \\n    result = 1.0-sum(reduced_density_matrix_evals)\\n    return result',\n",
       "  'import pytest\\nfrom source import calculate_truncation_error\\n\\ndef test_calculate_truncation_error():\\n    reduced_density_matrix_evals = [0.2, 0.3, 0.5]\\n    expected_result = 1.0 - sum(reduced_density_matrix_evals)\\n    result = calculate_truncation_error(reduced_density_matrix_evals)\\n    assert result == expected_result',\n",
       "  100.0),\n",
       " (\"def _choose_metric_function(key: str):\\n    \\n    out = lambda x: x\\n\\n    if key == 'mean':\\n        out = lambda x: x.mean()\\n\\n    elif key == 'median':\\n        out = lambda x: x.median()\\n\\n    elif key == 'sharpe':\\n        out = lambda x: x.mean()/x.std()\\n\\n    elif key == 'q1':\\n        out = lambda x: x.quantile(q=.25)\\n\\n    elif key == 'q3':\\n        out = lambda x: x.quantile(q=.75)\\n\\n    elif key == 'p10':\\n        out = lambda x: x.quantile(q=.10)\\n\\n    elif key == 'p90':\\n        out = lambda x: x.quantile(q=.90)\\n\\n    return out\",\n",
       "  \"import pytest\\nimport numpy as np\\nfrom source import _choose_metric_function\\n\\ndef test_metric_function():\\n    np.random.seed(0)\\n    data = np.random.rand(100)\\n    assert np.allclose(_choose_metric_function('mean')(data), np.mean(data))\\n    np.random.seed(0)\\n    data = np.random.rand(100)\\n    assert np.allclose(_choose_metric_function('median')(data), np.median(data))\\n    np.random.seed(0)\\n    data = np.random.rand(100)\\n    assert np.allclose(_choose_metric_function('sharpe')(data), np.mean(data) / np.std(data))\\n    np.random.seed(0)\\n    data = np.random.rand(100)\\n    assert np.allclose(_choose_metric_function('q1')(data), np.percentile(data, 25))\\n    np.random.seed(0)\\n    data = np.random.rand(100)\\n    assert np.allclose(_choose_metric_function('q3')(data), np.percentile(data, 75))\\n    np.random.seed(0)\\n    data = np.random.rand(100)\\n    assert np.allclose(_choose_metric_function('p10')(data), np.percentile(data, 10))\\n    np.random.seed(0)\\n    data = np.random.rand(100)\\n    assert np.allclose(_choose_metric_function('p90')(data), np.percentile(data, 90))\\n    with pytest.raises(ValueError):\\n        _choose_metric_function('invalid_key')\",\n",
       "  41.0),\n",
       " ('def zerocross(eigenvec):\\n    \\n    return (eigenvec[:-1, ...] * eigenvec[1:, ...] < 0).sum(axis=0)',\n",
       "  \"import numpy as np\\nimport source  # Assuming the original function is in 'source.py'\\nimport pytest\\n\\ndef test_zerocross():\\n    eigenvec = np.array([1, 2, 3, 4])\\n    assert source.zerocross(eigenvec) == 2  # There are two pairs of elements where the corresponding elements on either side of the index are negative\",\n",
       "  100.0),\n",
       " ('def binarize(x, values, threshold=None, included_in=\\'upper\\'):\\n    \\n    lower, upper = values\\n\\n    if threshold is None:\\n        threshold = (lower + upper) / 2.\\n\\n    x = x.copy()\\n    if included_in == \\'lower\\':\\n        x[x <= threshold] = lower\\n        x[x > threshold] = upper\\n    elif included_in == \\'upper\\':\\n        x[x < threshold] = lower\\n        x[x >= threshold] = upper\\n    else:\\n        raise ValueError(\\'included_in must be \"lower\" or \"upper\"\\')\\n    return x',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import binarize\\n\\ndef test_binarize():\\n    x = np.array([1, 2, 3, 4, 5])\\n    values = (0, 10)\\n    result = binarize(x, values)\\n    assert np.array_equal(result, np.array([0, 0, 0, 10, 10])), \"Test failed!\"\\n\\ndef test_binarize_with_threshold():\\n    x = np.array([1, 2, 3, 4, 5])\\n    values = (0, 10)\\n    threshold = 3\\n    result = binarize(x, values, threshold)\\n    assert np.array_equal(result, np.array([0, 0, 0, 3, 3])), \"Test failed!\"\\n\\ndef test_binarize_lower_included():\\n    x = np.array([1, 2, 3, 4, 5])\\n    values = (0, 10)\\n    result = binarize(x, values, threshold=3, included_in=\\'lower\\')\\n    assert np.array_equal(result, np.array([0, 0, 0, 3, 10])), \"Test failed!\"',\n",
       "  92.0),\n",
       " ('import torch\\n\\ndef ndc_rays(H, W, focal, near, rays_o, rays_d):\\n    \\n    # Shift ray origins to near plane\\n    t = -(near + rays_o[..., 2]) / rays_d[..., 2]\\n    rays_o = rays_o + t[..., None] * rays_d\\n\\n    # Projection\\n    o0 = -1./(W/(2.*focal)) * rays_o[..., 0] / rays_o[..., 2]\\n    o1 = -1./(H/(2.*focal)) * rays_o[..., 1] / rays_o[..., 2]\\n    o2 = 1. + 2. * near / rays_o[..., 2]\\n\\n    d0 = -1./(W/(2.*focal)) * \\\\\\n        (rays_d[..., 0]/rays_d[..., 2] - rays_o[..., 0]/rays_o[..., 2])\\n    d1 = -1./(H/(2.*focal)) * \\\\\\n    (rays_d[..., 1]/rays_d[..., 2] - rays_o[..., 1]/rays_o[..., 2])\\n    d2 = -2. * near / rays_o[..., 2]\\n\\n    rays_o = torch.stack([o0, o1, o2], -1)\\n    rays_d = torch.stack([d0, d1, d2], -1)\\n\\n    return rays_o, rays_d',\n",
       "  'import torch\\nimport pytest\\nfrom source import ndc_rays\\n\\n@pytest.fixture\\ndef rays_o():\\n    return torch.randn(2, 3)\\n\\n@pytest.fixture\\ndef rays_d():\\n    return torch.randn(2, 3)\\n\\ndef test_ndc_rays(rays_o, rays_d):\\n    ndc_rays(100, 100, 1.0, 2.0, rays_o, rays_d)',\n",
       "  100.0),\n",
       " ('def flatten(array, connectivity):\\n    \\n    if array.ndim == 2 or not connectivity.custom:\\n        return array\\n    else:\\n        out = array.reshape((array.shape[0], -1))\\n        assert out.base is array\\n        return out',\n",
       "  \"import pytest\\nfrom source import flatten\\n\\ndef test_flatten():\\n    array = [[1, 2], [3, 4]]\\nconnectivity = 'custom'\\n    output = flatten(array, connectivity)\\n    assert output.shape == (2, 2)\\n\\ndef test_flatten_2d():\\n    array = [[1, 2]]\\n    connectivity = 'not custom'\\n    output = flatten(array, connectivity)\\n    assert output.shape == (1, 2)\\n\\ndef test_flatten_3d():\\n    array = [[1, 2], [3, 4]]\\n    connectivity = 'not custom'\\n    output = flatten(array, connectivity)\\n    assert output.shape == (2, 2)\",\n",
       "  0.0),\n",
       " ('def f_measure(precision, recall, beta=1.0):\\n    \\n\\n    if precision == 0 and recall == 0:\\n        return 0.0\\n\\n    return float((1 + beta**2)*precision*recall/((beta**2)*precision + recall))',\n",
       "  'import pytest\\nfrom .source import f_measure\\n\\ndef test_f_measure_precision_recall_zero():\\n    assert f_measure(0, 0) == 0.0\\n\\ndef test_f_measure_precision_zero_recall_not_zero():\\n    assert f_measure(0, 0.5) == 0.0\\n\\ndef test_f_measure_recall_zero_precision_not_zero():\\n    assert f_measure(0.5, 0) == 0.0\\n\\ndef test_f_measure_precision_recall_not_zero():\\n    assert f_measure(0.5, 0.5) == 1.0\\n\\ndef test_f_measure_example():\\n    assert f_measure(0.9, 0.8) > 0.85',\n",
       "  0.0),\n",
       " ('def condition_to_cellsize(extent, cell_size, logger=None):\\n    \\n\\n    rl_adjustment = 0.\\n    tb_adjustment = 0.\\n\\n    # Calculate the cell modulus\\n    rl_mod = (extent[2] - extent[0]) % cell_size\\n    tb_mod = (extent[3] - extent[1]) % cell_size\\n\\n    if rl_mod > 0:\\n        rl_adjustment = (cell_size - rl_mod) / 2.0\\n    if tb_mod > 0:\\n        tb_adjustment = (cell_size - tb_mod) / 2.0\\n\\n    if logger is not None:\\n        logger.debug(\\n            \"Checking how well the cell size is fitted to the domain:\"\\n            \"\\\\t\\\\nRight-Left modulus: {0:.2f}\"\\n            \"\\\\t\\\\nTop - Bottom modulus: {1:.2f}\".format(rl_mod, tb_mod))\\n\\n    # Adjust in both directions to maintain common center\\n    result = [\\n        extent[0] - rl_adjustment,  # Left\\n        extent[1] - tb_adjustment,  # Bottom\\n        extent[2] + rl_adjustment,  # Right\\n        extent[3] + tb_adjustment  # Top\\n    ]\\n\\n    rl_mod = (result[2] - result[0]) % cell_size  # Modulus right to left\\n    tb_mod = (result[3] - result[1]) % cell_size  # Modulus top to bottom\\n\\n    if (rl_adjustment != 0 or tb_adjustment != 0) and logger is not None:\\n\\n        logger.warning(\" Modeling domain extents are ill fit for cell size {}.\"\\n                       \"\\\\nExpanding to fit whole cells...\".format(cell_size))\\n\\n        logger.debug(\"Checking how well our cell size is fitted to our domain:\"\\n                     \"\\\\t\\\\nRight-Left modulo: {0:.2f}\"\\n                     \"\\\\t\\\\nTop - Bottom modulo: {1:.2f}\".format(rl_mod, tb_mod))\\n\\n    return result',\n",
       "  'import logging\\nfrom source import condition_to_cellsize\\n\\ndef test_condition_to_cellsize():\\n    logger = logging.getLogger(__name__)\\n    extent = [1, 2, 10, 9]\\n    cell_size = 3\\n    result = condition_to_cellsize(extent, cell_size, logger)\\n    assert result == [3, 2, 10, 9], \"Test failed: check the function \\'condition_to_cellsize\\'\"',\n",
       "  94.0),\n",
       " ('import torch\\n\\ndef eval_acc(pred, labels, mask=None):\\n    r\\n\\n    if mask is not None:\\n        pred, labels = pred[mask], labels[mask]\\n        if pred is None or labels is None:\\n            return 0.0\\n\\n    acc = (torch.argmax(pred, dim=1) == labels).float().sum() / len(pred)\\n\\n    return acc',\n",
       "  'import sys\\nsys.path.append(\".\")\\nimport source  # assuming source.py is in the same directory\\nimport torch\\n\\ndef test_eval_acc():\\n    pred = torch.tensor([[0.1, 0.9], [0.8, 0.2]])\\n    labels = torch.tensor([0, 1])\\n    mask = torch.tensor([True, False])\\n    assert source.eval_acc(pred, labels, mask) == 0.5\\n\\ntest_eval_acc()',\n",
       "  33.0),\n",
       " ('def combine_predictions_list(predictions_list, index_list=None):\\n    \\n    Predictions = type(predictions_list[0])\\n    combined_predictions = Predictions.combine(predictions_list, index_list)\\n    return combined_predictions',\n",
       "  '# test_source.py\\nimport pytest\\nimport source  # assuming the original code is in a file named \\'source.py\\'\\n\\ndef test_combine_predictions_list():\\n    predictions_list = [\"test1\", \"test2\"]\\n    index_list = [0, 1]\\n    expected_result = \"combined\"\\n\\n    result = source.combine_predictions_list(predictions_list, index_list)\\n\\n    assert result == expected_result, \"The function did not return the expected result\"',\n",
       "  75.0),\n",
       " ('def lin_interp(x, x0, x1, y0, y1):\\n    \\n    y = y0 + (y1-y0)*(x-x0)/(x1-x0)\\n    \\n    return y',\n",
       "  'import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\nfrom source import lin_interp  # Importing the function from source.py\\n\\ndef test_lin_interp():\\n    assert lin_interp(0, 0, 10, 0, 100) == 0  # Testing when x is 0\\n    assert lin_interp(10, 0, 10, 0, 100) == 100  # Testing when x is 10\\n    assert lin_interp(5, 0, 10, 0, 100) == 50  # Testing when x is 5',\n",
       "  100.0),\n",
       " ('def luminance_to_contrast_ratio(luminance1, luminance2):\\n    \\n    (l1, l2) = sorted((luminance1, luminance2), reverse=True)\\n    return (l1 + 0.05) / (l2 + 0.05)',\n",
       "  'import pytest\\nfrom source import luminance_to_contrast_ratio\\n\\ndef test_luminance_to_contrast_ratio():\\n    assert luminance_to_contrast_ratio(100, 200) == 2.605',\n",
       "  100.0),\n",
       " ('def coordinateBounds(N):\\n    r\\n    if N % 2 == 0:\\n        return -0.5, 0.5 * (N - 2) / N\\n    else:\\n        return -0.5 * (N - 1) / N, 0.5 * (N - 1) / N',\n",
       "  'import sys\\nsys.path.append(\".\")\\nimport source\\n\\ndef test_coordinateBounds():\\n    assert source.coordinateBounds(4) == (-0.5, 1.5)\\n    assert source.coordinateBounds(5) == (-1.5, 1.5)',\n",
       "  40.0),\n",
       " ('def index_duplicated(index, keep=\"first\"):\\n    \\n    return index.to_series().duplicated(keep=keep).to_tensor()',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")\\nimport source  # Assuming source.py is in the same directory\\nimport pytest\\n\\ndef test_index_duplicated():\\n    index = [\"a\", \"b\", \"a\", \"c\", \"b\", \"a\"]\\n    expected_output = [False, False, True, False, True, False]\\n    assert (source.index_duplicated(index) == expected_output).all()',\n",
       "  100.0),\n",
       " ('def can_absorb(left, right):\\n    r\\n    return left.can_absorb(right) or right.can_absorb(left)',\n",
       "  \"import pytest\\nfrom source import can_absorb\\n\\ndef test_can_absorb():\\n    left = SomeClass()\\n    right = SomeOtherClass()\\n    result = can_absorb(left, right)\\n    assert result == expected_value, 'The function did not return the expected value'\",\n",
       "  33.0),\n",
       " ('def transfer_function_Rec1886_to_linear(v):\\n    \\n\\n    g = 2.4\\n    Lw = 1\\n    Lb = 0\\n\\n    # Ignoring legal to full scaling for now.\\n    # v = (1023.0*v - 64.0)/876.0\\n\\n    t = pow(Lw, 1.0 / g) - pow(Lb, 1.0 / g)\\n    a = pow(t, g)\\n    b = pow(Lb, 1.0 / g) / t\\n\\n    return a * pow(max((v + b), 0.0), g)',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\nfrom source import transfer_function_Rec1886_to_linear\\n\\ndef test_transfer_function_Rec1886_to_linear():\\n    v = 500\\n    assert transfer_function_Rec1886_to_linear(v) == 1023.0\",\n",
       "  100.0),\n",
       " ('def days_in_month_365(month, year=0):\\n    \\n\\n    days_in_months = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\\n    return list(range(1, days_in_months[month-1] + 1))',\n",
       "  'import sys\\nsys.path.append(\".\")\\nimport source\\nimport pytest\\n\\ndef test_days_in_month_365():\\n    assert source.days_in_month_365(1, 2022) == [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]',\n",
       "  100.0),\n",
       " ('def scalar_search_armijo(phi, phi0, derphi0, c1=1e-4, alpha0=1, amin=0, max_backtrack_count=10):\\n    \\n    count = 0\\n\\n    phi_a0 = phi(alpha0)\\n    if phi_a0 <= phi0 + c1*alpha0*derphi0:\\n        return alpha0, phi_a0, count\\n\\n    # Otherwise compute the minimizer of a quadratic interpolant:\\n\\n    # alpha1 = -(derphi0) * alpha0**2 / 2.0 / (phi_a0 - phi0 - derphi0 * alpha0)\\n    alpha1 = -derphi0.mul(alpha0.pow(2)).div(2.0).div(phi_a0.sub(phi0).sub(derphi0.mul(alpha0)))\\n    phi_a1 = phi(alpha1)\\n\\n    if (phi_a1 <= phi0 + c1*alpha1*derphi0):\\n        return alpha1, phi_a1, count\\n\\n    # Otherwise loop with cubic interpolation until we find an alpha which\\n    # satisfies the first Wolfe condition (since we are backtracking, we will\\n    # assume that the value of alpha is not too small and satisfies the second\\n    # condition.\\n\\n    # original assumption: we are assuming alpha>0 is a descent direction\\n    # Yibo: I change to also handle negative alpha1 \\n    while alpha1 != amin and count < max_backtrack_count:       \\n        factor = alpha0.pow(2) * alpha1.pow(2) * (alpha1-alpha0)\\n        a = alpha0.pow(2) * (phi_a1 - phi0 - derphi0*alpha1) - \\\\\\n            alpha1.pow(2) * (phi_a0 - phi0 - derphi0*alpha0)\\n        a = a / factor\\n        b = -alpha0.pow(3) * (phi_a1 - phi0 - derphi0*alpha1) + \\\\\\n            alpha1.pow(3) * (phi_a0 - phi0 - derphi0*alpha0)\\n        b = b / factor\\n\\n        # (-b + np.sqrt(abs(b.pow(2) - 3 * a * derphi0))) / (3.0*a)\\n        alpha2 = (b.pow(2).sub(a.mul(derphi0).mul(3))).abs().sqrt().sub(b).div( a.mul(3.0) )\\n        phi_a2 = phi(alpha2)\\n\\n        if (phi_a2 <= phi0 + c1*alpha2*derphi0):\\n            return alpha2, phi_a2, count\\n\\n        if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2/alpha1) < 0.96:\\n            alpha2 = alpha1 / 2.0\\n\\n        alpha0 = alpha1\\n        alpha1 = alpha2\\n        phi_a0 = phi_a1\\n        phi_a1 = phi_a2\\n\\n        count += 1\\n\\n    # Failed to find a suitable step length\\n    return alpha1, phi_a1, count',\n",
       "  'import sys\\nsys.path.append(\".\")  # This is to import the file in the same directory\\nfrom source import scalar_search_armijo\\nimport pytest\\n\\n@pytest.fixture\\ndef params():\\n    phi = lambda alpha: alpha**2  # test function\\n    phi0 = 0\\n    derphi0 = 0\\n    return phi, phi0, derphi0\\n\\ndef test_scalar_search_armijo(params):\\n    phi, phi0, derphi0 = params\\n    alpha, phi_a, count = scalar_search_armijo(phi, phi0, derphi0)\\n    assert (alpha, count) == (0, 0), \"Test Failed: Expected alpha to be 0, count to be 0\"',\n",
       "  19.0),\n",
       " ('def get_lambda_mu(e_young, nu_poisson):\\n    \\n    lambda_ = e_young * nu_poisson / (1 - nu_poisson * nu_poisson)\\n    mu = 0.5 * e_young / (nu_poisson + 1)\\n    return mu, lambda_',\n",
       "  'import pytest\\nfrom source import get_lambda_mu\\n\\ndef test_get_lambda_mu():\\n    assert get_lambda_mu(1, 0.3) == (0.5, 1.0)',\n",
       "  100.0),\n",
       " ('def logistic_derivative(Z):\\n    \\n    return Z * (1 - Z)',\n",
       "  '# import the function we are testing\\nfrom source import logistic_derivative\\n\\n# Test class\\nclass TestSource:\\n\\n    def test_logistic_derivative(self):\\n        # Define input\\n        Z = 0.5\\n        # Define expected output\\n        expected = 0.5 * (1 - 0.5)\\n        # Assert that function equals expected\\n        assert logistic_derivative(Z) == expected',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef rotation_matrix_to_quaternion(rotation_matrix, eps=1e-6):\\n    \\n    if not torch.is_tensor(rotation_matrix):\\n        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\\n            type(rotation_matrix)))\\n\\n    if len(rotation_matrix.shape) > 3:\\n        raise ValueError(\\n            \"Input size must be a three dimensional tensor. Got {}\".format(\\n                rotation_matrix.shape))\\n    if not rotation_matrix.shape[-2:] == (3, 4):\\n        raise ValueError(\\n            \"Input size must be a N x 3 x 4  tensor. Got {}\".format(\\n                rotation_matrix.shape))\\n\\n    rmat_t = torch.transpose(rotation_matrix, 1, 2)\\n\\n    mask_d2 = rmat_t[:, 2, 2] < eps\\n\\n    mask_d0_d1 = rmat_t[:, 0, 0] > rmat_t[:, 1, 1]\\n    mask_d0_nd1 = rmat_t[:, 0, 0] < -rmat_t[:, 1, 1]\\n\\n    t0 = 1 + rmat_t[:, 0, 0] - rmat_t[:, 1, 1] - rmat_t[:, 2, 2]\\n    q0 = torch.stack([rmat_t[:, 1, 2] - rmat_t[:, 2, 1],\\n                      t0, rmat_t[:, 0, 1] + rmat_t[:, 1, 0],\\n                      rmat_t[:, 2, 0] + rmat_t[:, 0, 2]], -1)\\n    t0_rep = t0.repeat(4, 1).t()\\n\\n    t1 = 1 - rmat_t[:, 0, 0] + rmat_t[:, 1, 1] - rmat_t[:, 2, 2]\\n    q1 = torch.stack([rmat_t[:, 2, 0] - rmat_t[:, 0, 2],\\n                      rmat_t[:, 0, 1] + rmat_t[:, 1, 0],\\n                      t1, rmat_t[:, 1, 2] + rmat_t[:, 2, 1]], -1)\\n    t1_rep = t1.repeat(4, 1).t()\\n\\n    t2 = 1 - rmat_t[:, 0, 0] - rmat_t[:, 1, 1] + rmat_t[:, 2, 2]\\n    q2 = torch.stack([rmat_t[:, 0, 1] - rmat_t[:, 1, 0],\\n                      rmat_t[:, 2, 0] + rmat_t[:, 0, 2],\\n                      rmat_t[:, 1, 2] + rmat_t[:, 2, 1], t2], -1)\\n    t2_rep = t2.repeat(4, 1).t()\\n\\n    t3 = 1 + rmat_t[:, 0, 0] + rmat_t[:, 1, 1] + rmat_t[:, 2, 2]\\n    q3 = torch.stack([t3, rmat_t[:, 1, 2] - rmat_t[:, 2, 1],\\n                      rmat_t[:, 2, 0] - rmat_t[:, 0, 2],\\n                      rmat_t[:, 0, 1] - rmat_t[:, 1, 0]], -1)\\n    t3_rep = t3.repeat(4, 1).t()\\n\\n    mask_c0 = mask_d2 * mask_d0_d1\\n    mask_c1 = mask_d2 * (~mask_d0_d1)\\n    mask_c2 = (~mask_d2) * mask_d0_nd1\\n    mask_c3 = (~mask_d2) * (~mask_d0_nd1)\\n    mask_c0 = mask_c0.view(-1, 1).type_as(q0)\\n    mask_c1 = mask_c1.view(-1, 1).type_as(q1)\\n    mask_c2 = mask_c2.view(-1, 1).type_as(q2)\\n    mask_c3 = mask_c3.view(-1, 1).type_as(q3)\\n\\n    q = q0 * mask_c0 + q1 * mask_c1 + q2 * mask_c2 + q3 * mask_c3\\n    q /= torch.sqrt(t0_rep * mask_c0 + t1_rep * mask_c1 +  # noqa\\n                    t2_rep * mask_c2 + t3_rep * mask_c3)  # noqa\\n    q *= 0.5\\n    return q',\n",
       "  '',\n",
       "  0.0),\n",
       " ('def replace_masked(tensor, mask, value):\\n    \\n    mask = mask.unsqueeze(1).transpose(2, 1)\\n    reverse_mask = 1.0 - mask\\n    values_to_add = value * reverse_mask\\n    return tensor * mask + values_to_add',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import replace_masked\\nimport torch\\n\\ndef test_replace_masked():\\n    tensor = torch.randn(3, 3, 3)\\n    mask = torch.rand(3, 3, 3) > 0.5\\n    value = torch.tensor([1.0, 2.0, 3.0])\\n\\n    expected_output = tensor * mask + value * (1 - mask)\\n    assert torch.allclose(replace_masked(tensor, mask, value), expected_output)',\n",
       "  20.0),\n",
       " ('def lin_interp(x, x0, x1, y0, y1):\\n    \\n    y = y0 + (y1-y0)*(x-x0)/(x1-x0)\\n    \\n    return y',\n",
       "  'import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\nfrom source import lin_interp\\n\\ndef test_lin_interp_equal_x0_x1():\\n    assert lin_interp(5, 5, 5, 10, 20) == 15\\n\\ndef test_lin_interp_equal_y0_y1():\\n    assert lin_interp(5, 0, 10, 10, 10) == 10\\n\\ndef test_lin_interp_negative():\\n    assert lin_interp(5, -10, 0, 10, 20) == -5\\n\\ndef test_lin_interp_positive():\\n    assert lin_interp(5, 0, 10, -10, 20) == 5\\n\\ndef test_lin_interp_zero_x():\\n    assert lin_interp(0, 0, 10, 10, 20) == 10\\n\\ndef test_lin_interp_zero_y():\\n    assert lin_interp(5, 0, 10, 0, 0) == 0',\n",
       "  100.0),\n",
       " ('def lrt_pvalues(null_lml, alt_lmls, dof=1):\\n    \\n    from scipy.stats import chi2\\n    from numpy_sugar import epsilon\\n    from numpy import asarray, clip, inf\\n\\n    lrs = clip(-2 * null_lml + 2 * asarray(alt_lmls, float), epsilon.super_tiny, inf)\\n    pv = chi2(df=dof).sf(lrs)\\n    return clip(pv, epsilon.super_tiny, 1 - epsilon.tiny)',\n",
       "  'import pytest\\nfrom source import lrt_pvalues\\nfrom numpy import Inf, array, allclose\\nfrom numpy_sugar import epsilon\\nfrom scipy.stats import chi2\\n\\ndef test_lrt_pvalues():\\n    assert allclose(lrt_pvalues(0, array([0, 0])), array([0.5, 0.5]))\\n    assert allclose(lrt_pvalues(0, array([1, 1])), array([0.28257929798398398, 0.28257929798398398]))\\n    assert allclose(lrt_pvalues(3, array([2, 2])), array([0.054478740025899857, 0.054478740025899857]))\\n    assert lrt_pvalues(0, array([-1, 1]), dof=2) == Inf\\n    assert lrt_pvalues(0, array([1, -1]), dof=2) == Inf\\n    assert lrt_pvalues(1, array([2, 2]), dof=2) == Inf\\n    assert lrt_pvalues(0, array([1, 1]), dof=0) == Inf',\n",
       "  14.0),\n",
       " ('def magnitude(v):\\n    \\n    \\n    return sum(v)',\n",
       "  'import pytest\\nfrom source import magnitude\\n\\ndef test_magnitude():\\n    v = [3, 4, 5]\\n    expected_output = 12\\n    assert magnitude(v) == expected_output',\n",
       "  100.0),\n",
       " ('def monthly_soil_heat_flux(t_month_prev, t_month_next):\\n    \\n    return 0.07 * (t_month_next - t_month_prev)',\n",
       "  'import sys\\nsys.path.append(\".\") # this is to import source.py from the same directory\\nfrom source import monthly_soil_heat_flux\\n\\ndef test_monthly_soil_heat_flux():\\n    t_month_prev = 20\\n    t_month_next = 25\\n    assert monthly_soil_heat_flux(t_month_prev, t_month_next) == 0.07 * (t_month_next - t_month_prev)',\n",
       "  100.0),\n",
       " ('def dig2phys(signal, dmin, dmax, pmin, pmax):\\n    \\n    m = (pmax-pmin) / (dmax-dmin)\\n    b = pmax / m - dmax\\n    physical = m * (signal + b)\\n    return physical',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import dig2phys\\n\\nclass TestDig2phys:\\n    def test_positive_signal(self):\\n        assert dig2phys(1, 1, 10, 1, 20) > 1\\n\\n    def test_negative_signal(self):\\n        assert dig2phys(-1, -10, -1, -20, -30) < -1\\n\\n    def test_zero_signal(self):\\n        assert dig2phys(0, 0, 0, 0, 0) == 0\\n\\n    def test_positive_range(self):\\n        assert dig2phys(1, 1, 10, 1, 20) > 10\\n\\n    def test_negative_range(self):\\n        assert dig2phys(1, -10, -1, -20, -30) < -10\\n\\n    def test_at_dmin(self):\\n        assert dig2phys(dmin, dmin, dmin, pmin, pmin) == pmin\\n\\n    def test_at_dmax(self):\\n        assert dig2phys(dmax, dmax, dmax, pmax, pmax) == pmax',\n",
       "  100.0),\n",
       " ('def get_lengths_from_binary_sequence_mask(mask):\\n    \\n    return mask.long().sum(-1)',\n",
       "  'import source  # This is the python file containing the function to test\\nimport pytest\\n\\ndef test_get_lengths_from_binary_sequence_mask():\\n    mask = torch.tensor([[1, 1, 0, 0], [0, 0, 1, 1]])\\n    assert torch.allclose(source.get_lengths_from_binary_sequence_mask(mask), torch.tensor([2, 2]))',\n",
       "  50.0),\n",
       " (\"import torch\\n\\ndef grad_fun(x_est,y):\\n    \\n    # True during training, False during testing\\n    does_require_grad = x_est.requires_grad\\n\\n    with torch.enable_grad():\\n        # Necessary for using autograd\\n        x_est.requires_grad_(True)\\n        # Assuming uniform white noise, in the denoising case matrix A is the identity\\n        error = torch.sum((y - x_est)**2)\\n        # We retain the graph during training only\\n        grad_x = torch.autograd.grad(error, inputs=x_est, retain_graph=does_require_grad,\\n                                     create_graph=does_require_grad)[0]\\n    # Set requires_grad back to it's original state\\n    x_est.requires_grad_(does_require_grad)\\n\\n    return grad_x\",\n",
       "  'import torch\\nimport pytest\\nfrom source import grad_fun\\n\\ndef test_grad_fun():\\n    x_est = torch.tensor([1.0, 2.0], requires_grad=True)\\n    y = torch.tensor([0.5, 1.5])\\n    grad_x = grad_fun(x_est, y)\\n    assert grad_x is not None',\n",
       "  100.0),\n",
       " ('def fetch_bigtable_rows(big_table, keys, other_silly_variable=None):\\n    \\n    return None',\n",
       "  'import pytest\\nimport os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \\'..\\')))\\nfrom source import fetch_bigtable_rows\\n\\ndef test_fetch_bigtable_rows():\\n    big_table = {}  # This is a placeholder for the big table\\n    keys = [\\'key1\\', \\'key2\\']  # This is a list of keys we expect to retrieve from the big table\\n    result = fetch_bigtable_rows(big_table, keys)\\n    assert result is not None, \"The function should return a non-None value\"',\n",
       "  100.0),\n",
       " ('def combine_predictions_list(predictions_list, index_list=None):\\n    \\n    Predictions = type(predictions_list[0])\\n    combined_predictions = Predictions.combine(predictions_list, index_list)\\n    return combined_predictions',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))  # add parent directory to path\\nfrom source import Predictions  # import the source file\\n\\ndef test_combine_predictions_list():\\n    predictions_list = [Predictions() for _ in range(5)]  # create 5 instances of Predictions\\n    index_list = list(range(5))\\n    combined_predictions = combine_predictions_list(predictions_list, index_list)\\n    assert type(combined_predictions) is Predictions  # assert that the type of combined_predictions is Predictions\",\n",
       "  25.0),\n",
       " ('def _prediction_function(model_object, predictor_matrix_in_list):\\n    \\n\\n    return model_object.predict(\\n        predictor_matrix_in_list[0],\\n        batch_size=predictor_matrix_in_list[0].shape[0])',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import Model\\nimport numpy as np\\n\\ndef test_predict():\\n    model_object = Model()\\n    predictor_matrix_in_list = [np.random.random((10, 10))]\\n    prediction = _prediction_function(model_object, predictor_matrix_in_list)\\n    assert isinstance(prediction, np.ndarray), \"The prediction result is not a numpy array.\"',\n",
       "  50.0),\n",
       " ('def plot_rolloff(axe, angles, rolloff, err, mark, cl, lab=\"\"):\\n    \\n\\n    if lab:\\n        axe.errorbar(angles, rolloff,\\n                     yerr=err, marker=mark, linestyle=\"\", markerfacecolor=cl,\\n                     markeredgecolor=\"grey\", ecolor=\"grey\", elinewidth=1,\\n                     markersize=4, label=lab)\\n    else:\\n        axe.errorbar(angles, rolloff,\\n                     yerr=err, marker=mark, linestyle=\"\", markerfacecolor=cl,\\n                     markeredgecolor=\"grey\", ecolor=\"grey\", elinewidth=1,\\n                     markersize=4, label=lab)\\n\\n    axe.set_ylabel(\"Roll-off\")\\n\\n    return axe',\n",
       "  \"import pytest\\nfrom source import plot_rolloff\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\ndef test_plot_rolloff():\\n    # Mocking the inputs\\n    axe = plt.figure().gca()\\n    angles = np.array([1, 2, 3, 4, 5])\\n    rolloff = np.array([10, 20, 30, 40, 50])\\n    err = np.array([2, 2, 2, 2, 2])\\n    mark = 'o'\\n    cl = 'blue'\\n    lab = 'Test'\\n\\n    # Calling the function\\n    plot_rolloff(axe, angles, rolloff, err, mark, cl, lab)\\n\\n    # Asserting if function call does not crash\\n    assert True\",\n",
       "  83.0),\n",
       " ('def imgCrop(image, cropBox, boxScale=1):\\n    \\n\\n    # Calculate scale factors\\n    xDelta = max(cropBox[2] * (boxScale - 1), -10000)\\n    yDelta = max(cropBox[3] * (boxScale - 1), -10000)\\n\\n    # Convert cv box to PIL box [left, upper, right, lower]\\n    PIL_box = [cropBox[0] - xDelta, cropBox[1] - yDelta, cropBox[0] +\\n               cropBox[2] + xDelta, cropBox[1] + cropBox[3] + yDelta]\\n\\n    return image.crop(PIL_box)',\n",
       "  'import pytest\\nfrom PIL import Image\\nfrom source import imgCrop\\n\\ndef test_imgCrop():\\n    # Create a test image\\n    image = Image.new(\\'RGB\\', (100, 100))\\n    \\n    # Create a test crop box\\n    cropBox = [50, 50, 75, 75]\\n    \\n    # Test the function with default box scale\\n    result = imgCrop(image, cropBox)\\n    assert type(result) == Image.Image, \"The function did not return an Image object\"\\n\\n    # Test the function with custom box scale\\n    result = imgCrop(image, cropBox, boxScale=2)\\n    assert type(result) == Image.Image, \"The function did not return an Image object\"\\n\\n    # Test the function with negative box scale\\n    result = imgCrop(image, cropBox, boxScale=-1)\\n    assert type(result) == Image.Image, \"The function did not return an Image object\"\\n\\n    # Test the function with box scale of 0\\n    result = imgCrop(image, cropBox, boxScale=0)\\n    assert type(result) == Image.Image, \"The function did not return an Image object\"\\n\\n    # Test the function with box scale larger than 1\\n    result = imgCrop(image, cropBox, boxScale=3)\\n    assert type(result) == Image.Image, \"The function did not return an Image object\"',\n",
       "  100.0),\n",
       " ('def outer(u, v):\\n    \\n    return u.reshape(u.shape + (1, ) * v.ndim) * v',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import outer\\n\\ndef test_outer():\\n    u = np.array([1, 2, 3])\\n    v = np.array([4, 5, 6])\\n    assert np.array_equal(outer(u, v), np.array([[4, 10, 18], [8, 15, 24], [12, 20, 30]]))',\n",
       "  100.0),\n",
       " ('def add_offset_to_target(example_strings, targets, offset):\\n  \\n  labels = targets + offset\\n  return (example_strings, labels)',\n",
       "  '# test_source.py\\nimport pytest\\nimport source  # Assuming the source code is in a file named source.py\\n\\ndef test_add_offset_to_target():\\n  example_strings = [\"Hello\", \"World\"]\\n  targets = [5, 10]\\n  offset = 3\\n  assert source.add_offset_to_target(example_strings, targets, offset) == ([5, 10], [8, 13])\\n\\ndef test_add_offset_to_target_empty_input():\\n  example_strings = []\\n  targets = []\\n  offset = 3\\n  assert source.add_offset_to_target(example_strings, targets, offset) == ([], [])\\n\\ndef test_add_offset_to_target_negative_offset():\\n  example_strings = [1, 2, 3, 4, 5]\\n  targets = [0, 0, 0, 0, 0]\\n  offset = -2\\n  assert source.add_offset_to_target(example_strings, targets, offset) == ([1, 2, 3, 4, 5], [-2, -2, -2, -2, -2])',\n",
       "  67.0),\n",
       " ('def padSize(raw_value, pad):\\n    \\n    return (pad - (raw_value % pad)) % pad',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\\'.\\')\\nfrom source import padSize\\n\\ndef test_padSize_returns_correct_values():\\n    assert padSize(5, 2) == 1, \"Test case 1 failed\"\\n    assert padSize(10, 3) == 2, \"Test case 2 failed\"\\n    assert padSize(15, 5) == 0, \"Test case 3 failed\"\\n    assert padSize(20, 7) == 1, \"Test case 4 failed\"\\n    assert padSize(25, 10) == 5, \"Test case 5 failed\"',\n",
       "  100.0),\n",
       " ('def move_j(joints, accel, vel):\\n    \\n    # Check acceleration and velocity are non-negative and below a set limit\\n\\n    _j_fmt = \"[\" + (\"%.2f,\"*6)[:-1]+\"]\"\\n    _j_fmt = _j_fmt%tuple(joints)\\n    script = \"movej(%s, a = %.2f, v = %.2f)\\\\n\"%(_j_fmt,accel,vel)\\n    return script',\n",
       "  \"import pytest\\nfrom source import move_j\\n\\ndef test_move_j():\\n    joints = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\\n    accel = 0.5\\n    vel = 0.5\\n    expected_result = 'movej([0.00,0.00,0.00,0.00,0.00,0.00], a = 0.50, v = 0.50)\\\\n'\\n    assert move_j(joints, accel, vel) == expected_result\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef bellman(qf, targ_qf, targ_pol, batch, gamma, continuous=True, deterministic=True, sampling=1, reduction=\\'elementwise_mean\\'):\\n    \\n    if continuous:\\n        obs = batch[\\'obs\\']\\n        acs = batch[\\'acs\\']\\n        rews = batch[\\'rews\\']\\n        next_obs = batch[\\'next_obs\\']\\n        dones = batch[\\'dones\\']\\n\\n        targ_pol.reset()\\n        _, _, pd_params = targ_pol(next_obs)\\n        pd = targ_pol.pd\\n\\n        next_acs = pd.sample(pd_params, torch.Size([sampling]))\\n        next_obs = next_obs.expand([sampling] + list(next_obs.size()))\\n        targ_q, _ = targ_qf(next_obs, next_acs)\\n        next_q = torch.mean(targ_q, dim=0)\\n\\n        targ = rews + gamma * next_q * (1 - dones)\\n        targ = targ.detach()\\n        q, _ = qf(obs, acs)\\n\\n        ret = 0.5 * (q - targ)**2\\n        if reduction != \\'none\\':\\n            ret = torch.mean(\\n                ret) if reduction == \\'elementwise_mean\\' else torch.sum(ret)\\n        return ret\\n    else:\\n        raise NotImplementedError(\\n            \"Only Q function with continuous action space is supported now.\")',\n",
       "  \"import torch\\nimport pytest\\nfrom source import bellman\\n\\nclass TestBellman:\\n\\n    def test_continuous(self):\\n        qf = torch.nn.Sequential(torch.nn.Linear(2, 2), torch.nn.ReLU())\\n        targ_qf = torch.nn.Sequential(torch.nn.Linear(2, 2), torch.nn.ReLU())\\n        targ_pol = torch.nn.Sequential(torch.nn.Linear(2, 2), torch.nn.ReLU())\\n        \\n        batch = {\\n            'obs': torch.randn(1, 2),\\n            'acs': torch.randn(1, 2),\\n            'rews': torch.randn(1),\\n            'next_obs': torch.randn(1, 2),\\n            'dones': torch.tensor([0.])\\n        }\\n        gamma = 0.99\\n        \\n        result = bellman(qf, targ_qf, targ_pol, batch, gamma, continuous=True, deterministic=True, sampling=1, reduction='elementwise_mean')\\n        assert result.shape == torch.Size([1, 2])\\n\\n    def test_not_implemented(self):\\n        qf = torch.nn.Sequential(torch.nn.Linear(2, 2), torch.nn.ReLU())\\n        targ_qf = torch.nn.Sequential(torch.nn.Linear(2, 2), torch.nn.ReLU())\\n        targ_pol = torch.nn.Sequential(torch.nn.Linear(2, 2), torch.nn.ReLU())\\n        \\n        batch = {\\n            'obs': torch.randn(1, 2),\\n            'acs': torch.randn(1, 2),\\n            'rews': torch.randn(1),\\n            'next_obs': torch.randn(1, 2),\\n            'dones': torch.tensor([0.])\\n        }\\n        gamma = 0.99\\n        \\n        with pytest.raises(NotImplementedError):\\n            bellman(qf, targ_qf, targ_pol, batch, gamma, continuous=False, deterministic=True, sampling=1, reduction='elementwise_mean')\",\n",
       "  43.0),\n",
       " ('import torch\\n\\ndef ndc_rays(H, W, focal, near, rays_o, rays_d):\\n    \\n    # Shift ray origins to near plane\\n    t = -(near + rays_o[..., 2]) / rays_d[..., 2]\\n    rays_o = rays_o + t[..., None] * rays_d\\n\\n    # Projection\\n    o0 = -1./(W/(2.*focal)) * rays_o[..., 0] / rays_o[..., 2]\\n    o1 = -1./(H/(2.*focal)) * rays_o[..., 1] / rays_o[..., 2]\\n    o2 = 1. + 2. * near / rays_o[..., 2]\\n\\n    d0 = -1./(W/(2.*focal)) * \\\\\\n        (rays_d[..., 0]/rays_d[..., 2] - rays_o[..., 0]/rays_o[..., 2])\\n    d1 = -1./(H/(2.*focal)) * \\\\\\n        (rays_d[..., 1]/rays_d[..., 2] - rays_o[..., 1]/rays_o[..., 2])\\n    d2 = -2. * near / rays_o[..., 2]\\n\\n    rays_o = torch.stack([o0, o1, o2], -1)\\n    rays_d = torch.stack([d0, d1, d2], -1)\\n\\n    return rays_o, rays_d',\n",
       "  'import pytest\\nimport torch\\nfrom source import ndc_rays\\n\\ndef test_ndc_rays():\\n    # Test case 1\\n    H = 1080\\n    W = 1920\\n    focal = 500\\n    near = 0.1\\n    rays_o = torch.tensor([0, 0, -1])\\n    rays_d = torch.tensor([0, 0, 1])\\n    expected = torch.tensor([[1.3393, 6.2441, -1.],\\n                             [1.3393, 5.2441, -1.],\\n                             [1.3393, 4.2441, -1.],\\n                             [1.3393, 3.2441, -1.],\\n                             [1.3393, 2.2441, -1.]])\\n    result = ndc_rays(H, W, focal, near, rays_o, rays_d)\\n    assert torch.allclose(result, expected, atol=1e-4)',\n",
       "  100.0),\n",
       " ('def sma(df, price, sma, n):\\n    \\n    df = df.copy()\\n    df[sma] = df[price].rolling(window=n).mean()\\n\\n    return df',\n",
       "  \"import pytest\\nimport pandas as pd\\nimport os\\n\\n# Import the source file\\ncurrent_dir = os.path.dirname(__file__)\\nsys.path.append(os.path.join(current_dir, '../'))\\nfrom source import sma\\n\\ndef test_sma_function():\\n    # Create a test DataFrame\\n    df = pd.DataFrame({'Close': [10, 20, 30, 40, 50]})\\n    result = sma(df, 'Close', 'SMA', 2)\\n\\n    # Perform an assertion to check the result\\n    assert result['SMA'].tolist() == [10, 15.0, 20.0, 25.0, 30.0]\",\n",
       "  0.0),\n",
       " (\"def _findlims(data, scale):\\n    \\n    energies, wfuncs = data['energies'], data['wfuncs'].T\\n    minval = min(scale * wfuncs[0] + energies[0])\\n    maxval = max(scale * wfuncs[-1] + energies[-1])\\n    extraspace = (maxval - minval) / 10\\n    xlim = (data['xcoords'][0], data['xcoords'][-1])\\n    ylim = (minval - extraspace, maxval + extraspace)\\n\\n    return xlim, ylim\",\n",
       "  \"import pytest\\nimport numpy as np\\nfrom source import _findlims\\n\\ndef test_findlims():\\n    data = {'energies': np.array([1, 2, 3]), 'wfuncs': np.array([4, 5, 6]), 'xcoords': np.array([7, 8, 9])}\\nscale = 10\\n    xlim, ylim = _findlims(data, scale)\\n    assert np.array_equal(xlim, (7, 9))\\n    assert np.isclose(ylim[0], 3.9)\\n    assert np.isclose(ylim[1], 23.9)\",\n",
       "  0.0),\n",
       " ('def boyer_freqs(ups_r, ups_theta, ups_phi, gamma, aa, slr, ecc, x, M=1):\\n    \\n\\n    omega_r = ups_r / gamma\\n    omega_theta = ups_theta / gamma\\n    omega_phi = ups_phi / gamma\\n\\n    return omega_r, omega_theta, omega_phi',\n",
       "  \"import pytest\\nfrom source import boyer_freqs\\n\\ndef test_boyer_freqs():\\n    result = boyer_freqs(1, 1, 1, 1, 1, 1, 1, 1)\\n    assert result[0] == 0.1, 'Ups_r not returning correct value'\\n    assert result[1] == 0.1, 'Ups_theta not returning correct value'\\n    assert result[2] == 0.1, 'Ups_phi not returning correct value'\",\n",
       "  100.0),\n",
       " ('def inverse_precess(a, b):\\n    \\n    return a[:-1], b[1:]',\n",
       "  'import pytest\\nimport source  # assuming the original code is in a file named source.py\\n\\nclass TestInversePrecess:\\n\\n    def test_inverse_precess(self):\\n        a = \"Hello\"\\n        b = \"World\"\\n        expected_output = (\"Hell\", \"orld\")\\n        assert source.inverse_precess(a, b) == expected_output',\n",
       "  100.0),\n",
       " (\"import numpy\\n\\ndef _alignedPartialProfile(data, rowRange, colRange, axis, method):\\n    \\n    assert axis in (0, 1)\\n    assert len(data.shape) == 3\\n    assert rowRange[0] < rowRange[1]\\n    assert colRange[0] < colRange[1]\\n    assert method in ('mean', 'sum')\\n\\n    nimages, height, width = data.shape\\n\\n    # Range aligned with the integration direction\\n    profileRange = colRange if axis == 0 else rowRange\\n\\n    profileLength = abs(profileRange[1] - profileRange[0])\\n\\n    # Subset of the image to use as intersection of ROI and image\\n    rowStart = min(max(0, rowRange[0]), height)\\n    rowEnd = min(max(0, rowRange[1]), height)\\n    colStart = min(max(0, colRange[0]), width)\\n    colEnd = min(max(0, colRange[1]), width)\\n\\n    if method == 'mean':\\n        _fct = numpy.mean\\n    elif method == 'sum':\\n        _fct = numpy.sum\\n    else:\\n        raise ValueError('method not managed')\\n\\n    imgProfile = _fct(data[:, rowStart:rowEnd, colStart:colEnd], axis=axis + 1,\\n                      dtype=numpy.float32)\\n\\n    # Profile including out of bound area\\n    profile = numpy.zeros((nimages, profileLength), dtype=numpy.float32)\\n\\n    # Place imgProfile in full profile\\n    offset = - min(0, profileRange[0])\\n    profile[:, offset:offset + imgProfile.shape[1]] = imgProfile\\n\\n    return profile\",\n",
       "  \"import numpy\\nimport pytest\\nfrom source import _alignedPartialProfile\\n\\ndef test_alignedPartialProfile():\\n    # Test 1: 3D array, axis = 0, range(0,5), method = 'mean'\\n    data = numpy.random.rand(10, 10, 10)\\n    rowRange = (2, 5)\\n    colRange = (3, 8)\\n    axis = 0\\n    method = 'mean'\\n    expected_output = _alignedPartialProfile(data, rowRange, colRange, axis, method)\\n    assert expected_output.shape == (10, 6)\\n\\n    # Test 2: 3D array, axis = 1, range(0,5), method = 'sum'\\n    data = numpy.random.rand(10, 10, 10)\\n    rowRange = (2, 5)\\n    colRange = (3, 8)\\n    axis = 1\\n    method = 'sum'\\n    expected_output = _alignedPartialProfile(data, rowRange, colRange, axis, method)\\n    assert expected_output.shape == (10, 10, 6)\\n\\n    # Test 3: Exception case when method is neither 'mean' nor 'sum'\\n    data = numpy.random.rand(10, 10, 10)\\n    rowRange = (2, 5)\\n    colRange = (3, 8)\\n    axis = 0\\n    method = 'avg'\\n    with pytest.raises(ValueError):\\n        _alignedPartialProfile(data, rowRange, colRange, axis, method)\\n\\n    # Test 4: Exception case when ranges are not aligned with the integration direction\\n    data = numpy.random.rand(10, 10, 10)\\n    rowRange = (2, 8)\\n    colRange = (3, 5)\\n    axis = 0\\n    method = 'mean'\\n    with pytest.raises(ValueError):\\n        _alignedPartialProfile(data, rowRange, colRange, axis, method)\",\n",
       "  88.0),\n",
       " ('def rgb2hex(tup, max=1):\\n    \\n    tup = [int(c * 255 / max) for c in tup]\\n    color = \"\"\\n    for c in tup:\\n        if c < 16:\\n            c = \"0\" + hex(c)[-1:]\\n        else:\\n            c = hex(c)[-2:]\\n        color += c\\n    return color.upper()',\n",
       "  \"import pytest\\nimport os\\nimport source\\n\\ndef test_rgb2hex():\\n    assert source.rgb2hex((0, 0, 0), max=1) == '000000', 'Failed with (0,0,0)'\\n    assert source.rgb2hex((1, 1, 1), max=1) == 'FFFFFF', 'Failed with (1,1,1)'\\n    assert source.rgb2hex((0.5, 0.5, 0.5), max=1\\n    ) == '7F7F7F', 'Failed with (0.5,0.5,0.5)'\\n    assert source.rgb2hex((0, 0, 0), max=0.5) == '000000', 'Failed with (0,0,0)'\\n    assert source.rgb2hex((0.3, 0.4, 0.6), max=1) != 'FFFFFF', 'Failed with (0.3,0.4,0.6)'\",\n",
       "  100.0),\n",
       " (\"def get_metrics(pipeline, filter_dict=None, collection_name='model_tracker', log=False):\\n    \\n\\n    if filter_dict is None:\\n        # Specify model_uuid at least.\\n        filter_dict = dict(model_uuid = pipeline.params.model_uuid)\\n    # Temporarily add collection_name key. The model tracker will use this key internally and pop it from the dict.\\n    filter_dict['collection_name'] = collection_name\\n    client_wrapper = pipeline.client_wrapper\\n    # call next(gen) to get the next item.\\n    gen = client_wrapper.get_metrics_generator(filter_dict=filter_dict, log=log)\\n    pipeline.log.warning('Successfully created metrics generator.')\\n    return gen\",\n",
       "  \"import pytest\\nfrom source import get_metrics # Assuming the function is in source.py\\n\\nclass TestGetMetrics:\\n\\n    def test_get_metrics(self, mocker):\\n        # Mock the required objects\\n        class MockPipeline:\\n            def __init__(self):\\n                self.params = {'model_uuid': 'some_uuid'}\\n                self.client_wrapper = mocker.MagicMock()\\n                self.log = mocker.MagicMock()\\n\\n        mock_pipeline = MockPipeline()\\n\\n        # Mock the return value of the mock pipeline's client_wrapper's get_metrics_generator method.\\n        mock_metrics_generator = mocker.MagicMock()\\n        type(mock_pipeline.client_wrapper).get_metrics_generator = mocker.PropertyMock(return_value=mock_metrics_generator)\\n\\n        # Call the function\\n        result = get_metrics(mock_pipeline)\\n\\n        # Check that the returned value is the mock metrics generator\\n        assert result == mock_metrics_generator\",\n",
       "  12.0),\n",
       " ('def from_scanpy(adata, cell_type_identifier, covariate_key):\\n    \\n\\n    # Calculate cell counts for the sample\\n    cell_counts = adata.obs[cell_type_identifier].value_counts()\\n\\n    # extracting covariates from uns\\n    covs = adata.uns[covariate_key]\\n\\n    return cell_counts, covs',\n",
       "  \"import os\\nimport pytest\\nimport anndata\\nfrom source import from_scanpy\\n\\n# Create a mock AnnData object for testing\\n# Mock values are defined based on the function requirements\\nadata = anndata.AnnData(obs=dict(cell_type=['type1', 'type2', 'type1', 'type2']),\\n                        uns=dict(covariate=['cov1', 'cov2', 'cov2', 'cov1']))\\n\\ndef test_from_scanpy():\\n    cell_counts, covs = from_scanpy(adata, 'cell_type', 'covariate')\\n    \\n    # Using assert statement to check if the function returns the expected result\\n    assert cell_counts.to_list() == ['type1', 'type2']\\n    assert covs.to_list() == ['cov1', 'cov2']\",\n",
       "  0.0),\n",
       " (\"def box_coco_to_yolo(bbox_coco, img):\\n    \\n\\n    bbox_yolo = []\\n    bbox_yolo.append((bbox_coco[0] + 0.5 * bbox_coco[2])/img['width'])\\n    bbox_yolo.append((bbox_coco[1] + 0.5 * bbox_coco[3])/img['height'])\\n    bbox_yolo.append(bbox_coco[2]/img['width'])\\n    bbox_yolo.append(bbox_coco[3]/img['height'])\\n\\n    return bbox_yolo\",\n",
       "  \"# test_box_coco_to_yolo.py\\n\\nimport pytest\\nfrom source import box_coco_to_yolo\\n\\ndef test_box_coco_to_yolo():\\n    bbox_coco = [10, 20, 30, 40] # suppose these are the values of bbox_coco\\n    img = {'width': 100, 'height': 200} # suppose these are the values of img\\n\\n    assert box_coco_to_yolo(bbox_coco, img) == [25.0, 40.0, 30.0, 40.0]\",\n",
       "  100.0),\n",
       " ('def linear(x, a=1, b=0):\\n    \\n    return a * x + b',\n",
       "  \"# test_source.py\\n\\nimport pytest\\nfrom source import linear    # Assuming the original code is in 'source.py'\\n\\ndef test_linear():\\n    assert linear(5, 1, 0) == 5\",\n",
       "  100.0),\n",
       " ('def cap_contributions(s_contrib, threshold=0.1):\\n    \\n    mask = s_contrib.abs() >= threshold\\n    return mask',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import cap_contributions\\n\\ndef test_cap_contributions():\\n    s_contrib = [1, -2, 3, -4, 5, -6, 7, -8, 9, -10]\\n    threshold = 5\\n    expected_output = [True, False, True, False, True, False, True, False, True, False]\\n    assert cap_contributions(s_contrib, threshold) == expected_output',\n",
       "  67.0),\n",
       " ('def b128_decode(data):\\n    \\n\\n    n = 0\\n    i = 0\\n    while True:\\n        d = int(data[2 * i:2 * i + 2], 16)\\n        n = n << 7 | d & 0x7F\\n        if d & 0x80:\\n            n += 1\\n            i += 1\\n        else:\\n            return n',\n",
       "  '# Import the necessary package\\nimport pytest\\n\\n# Import the source code file\\nfrom source import b128_decode\\n\\n# Define the test cases\\n@pytest.mark.parametrize(\"test_input, expected_output\", [\\n    (\\'00\\', 0),\\n    (\\'01\\', 1),\\n    (\\'07\\', 1),\\n    (\\'08\\', 2),\\n    (\\'09\\', 3),\\n    (\\'FF\\', 127),\\n    (\\'81\\', 128),\\n    (\\'82\\', 129),\\n    (\\'FE\\', 254),\\n    (\\'FFFE\\', 255),\\n    (\\'8000\\', 128),\\n    (\\'8001\\', 129),\\n    (\\'FE00\\', 254),\\n    (\\'FE01\\', 255),\\n    (\\'FFFF\\', 255)\\n])\\ndef test_b128_decode(test_input, expected_output):\\n    # Decoding the input\\n    output = b128_decode(test_input)\\n    # Asserting the result\\n    assert output == expected_output',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef boxes_to_cornels(box_xy, box_wh):\\n    \\n\\n    box_mins = box_xy - box_wh / 2.\\n    box_maxes = box_xy + box_wh / 2.\\n\\n    return torch.cat(\\n        (\\n            box_mins,\\n            box_maxes\\n        ),\\n        dim=2\\n    )',\n",
       "  'import pytest\\nimport torch\\nfrom source import boxes_to_corners  # noqa\\n\\ndef test_boxes_to_corners():\\n    box_xy = torch.tensor([[1, 2], [3, 4], [5, 6]])\\n    box_wh = torch.tensor([[10, 10], [20, 20], [30, 30]])\\n\\n    expected_output = torch.tensor([[0, 0, 9, 9], [0, 0, 19, 19], [0, 0, 29, 29]])\\n\\n    assert torch.allclose(boxes_to_corners(box_xy, box_wh), expected_output)',\n",
       "  40.0),\n",
       " ('import torch\\n\\ndef atomic(species, coordinates, model, AEVC, device=None):\\n    \\n\\n    if device is None:\\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n    # Move data to device and add batch dimension\\n    species = species.to(device).unsqueeze(0)\\n    coordinates = (\\n        coordinates.clone().detach().requires_grad_(True).to(device).unsqueeze(0)\\n    )\\n\\n    aevs = AEVC.forward((species, coordinates)).aevs\\n\\n    atomic_contributions = model._forward_atomic(species, aevs)\\n\\n    assert atomic_contributions.shape == species.shape\\n\\n    return atomic_contributions',\n",
       "  'import pytest\\nimport torch\\nfrom source import atomic\\n\\ndef test_atomic():\\n    species = torch.randint(low=0, high=10, size=(10,))\\n    coordinates = torch.rand((10, 3))\\n    model = ...\\n    AEVC = ...\\n    expected_output = torch.rand_like(species)\\n\\n    def mock_forward(species, coordinates):\\n        return expected_output\\n    AEVC.forward = mock_forward\\n    output = atomic(species, coordinates, model, AEVC)\\n    assert torch.allclose(output, expected_output)',\n",
       "  20.0),\n",
       " ('def combine_predictions_list(predictions_list, index_list=None):\\n    \\n    Predictions = type(predictions_list[0])\\n    combined_predictions = Predictions.combine(predictions_list, index_list)\\n    return combined_predictions',\n",
       "  '# test_source.py\\n\\nimport sys\\nsys.path.append(\".\")  # Adds the current directory to the python path\\n\\nfrom source import combine_predictions_list\\n\\ndef test_combine_predictions_list():\\n    # Arrange\\n    predictions_list = [\"test_1\", \"test_2\"]\\n    index_list = [0, 1]\\n\\n    # Act\\n    result = combine_predictions_list(predictions_list, index_list)\\n\\n    # Assert\\n    assert result == \"Combined predictions: test_1, test_2 at indices: 0, 1\"',\n",
       "  75.0),\n",
       " ('def dxlAngleToDegrees(dxl_angle):\\n    \\n    angle_degrees = round(dxl_angle / 1023. * 300. - 150.0, 1)\\n    return angle_degrees',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\\nfrom source import dxlAngleToDegrees\\n\\ndef test_dxlAngleToDegrees_within_range():\\n    assert dxlAngleToDegrees(300) == 150.0\\n\\ndef test_dxlAngleToDegrees_at_min():\\n    assert dxlAngleToDegrees(0) == -150.0\\n\\ndef test_dxlAngleToDegrees_at_max():\\n    assert dxlAngleToDegrees(1023) == 150.0\\n\\ndef test_dxlAngleToDegrees_zero():\\n    assert dxlAngleToDegrees(511) == 0.0\\n\\ndef test_dxlAngleToDegrees_negative():\\n    assert dxlAngleToDegrees(-100) == -150.0\",\n",
       "  100.0),\n",
       " ('def is_iterable(obj):\\n    \\n    try:\\n        iter(obj)\\n    except TypeError:\\n        return False\\n    return True',\n",
       "  'import source # this assumes that the source code file is named \\'source.py\\'\\nimport pytest\\n\\ndef test_is_iterable():\\n    assert source.is_iterable([1, 2, 3]) == True\\n    assert source.is_iterable(\"Hello\") == True\\n    assert source.is_iterable(123) == False\\n    assert source.is_iterable(None) == False\\n    assert source.is_iterable({\\'a\\':1, \\'b\\':2}) == True',\n",
       "  100.0),\n",
       " ('def b128_decode(data):\\n    \\n\\n    n = 0\\n    i = 0\\n    while True:\\n        d = int(data[2 * i:2 * i + 2], 16)\\n        n = n << 7 | d & 0x7F\\n        if d & 0x80:\\n            n += 1\\n            i += 1\\n        else:\\n            return n',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \\'..\\')))\\n\\nfrom source import b128_decode  # assuming the function is in source.py file\\n\\ndef test_b128_decode():\\n    data = \"01\"\\n    assert b128_decode(data) == 1\\n\\n    data = \"0181\"\\n    assert b128_decode(data) == 129\\n\\n    data = \"00\"\\n    assert b128_decode(data) == 0\\n    \\n    data = \"00808000\"\\n    assert b128_decode(data) == 128\\n\\n    data = \"01818101\"\\n    assert b128_decode(data) == 10259',\n",
       "  80.0),\n",
       " ('def register_new_peak(background_index_left, background_index_right, keys):\\n    \\n    if background_index_right - background_index_left >= 5:\\n        peak = keys[background_index_left + 1][0], keys[background_index_right - 1][1]\\n    else:\\n        peak = keys[background_index_left][0], keys[background_index_right][1]\\n\\n    return peak',\n",
       "  \"from source import register_new_peak\\nimport pytest\\n\\ndef test_register_new_peak():\\n    keys = [(2, 3), (4, 5), (6, 7), (8, 9), (10, 11)]\\n    peak = register_new_peak(1, 3, keys)\\n    assert peak == (6, 7), 'The function did not return the expected peak'\\n    peak = register_new_peak(0, 4, keys)\\n    assert peak == (2, 3), 'The function did not return the expected peak'\\n    peak = register_new_peak(3, 4, keys)\\n    assert peak == (8, 9), 'The function did not return the expected peak'\\n    peak = register_new_peak(1, 1, keys)\\n    assert peak == (2, 3), 'The function did not return the expected peak'\\n    peak = register_new_peak(2, 2, keys)\\n    assert peak == (4, 5), 'The function did not return the expected peak'\",\n",
       "  80.0),\n",
       " ('def rgb_to_hues(rgb):\\n  \\n  hsv = filter.filter_rgb_to_hsv(rgb, display_np_info=False)\\n  h = filter.filter_hsv_to_h(hsv, display_np_info=False)\\n  return h',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import rgb_to_hues\\n\\ndef test_rgb_to_hues():\\n    # Here, we assume the expected output for a specific input.\\n    expected_output = 120\\n    # We feed the function with specific values to check if it returns the expected output.\\n    assert rgb_to_hues((255, 0, 0)) == expected_output',\n",
       "  50.0),\n",
       " ('def conduction(z, T1, qb, lam, z0):\\n    \\n\\n    # Temperature gradient\\n    dTdz = qb / lam\\n\\n    # Temperature\\n    T = (\\n        T1 +  # Constant background\\n        dTdz * (z0 - z)  # Simple gradient\\n    )\\n\\n    return T',\n",
       "  'from source import conduction\\nimport pytest\\n\\ndef test_conduction():\\n    assert conduction(0, 20, 1, 0.01, 0) == 20 # assuming the values provided are for z=0, this tests if the function returns the constant background temperature when z=0',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef subspace_overlap(A, B, C=None):\\n    \\n    if C is None:\\n        U = torch.cat([A, B], dim=0)\\n    else:\\n        U = torch.cat([A, B, C], dim=0)\\n    d = U.shape[0]\\n    eye = torch.eye(d, device=U.device)\\n    return torch.mean((torch.matmul(U, torch.transpose(U, 1, 0)) - eye).pow(2))',\n",
       "  'import torch\\nimport pytest\\n\\n# The function to test\\nfrom source import subspace_overlap\\n\\ndef test_subspace_overlap():\\n    A = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\\n    B = torch.tensor([[7, 8, 9], [10, 11, 12]], dtype=torch.float32)\\n    result = subspace_overlap(A, B)\\n    expected = torch.tensor(0.0, dtype=torch.float32)\\n    assert torch.allclose(result, expected)\\n\\n\\nif __name__ == \"__main__\":\\n    test_subspace_overlap()',\n",
       "  88.0),\n",
       " ('def crop_array(top, left, bottom, right, array):\\n    \\n    return array.copy()[top:bottom,left:right]',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\\nfrom source import crop_array\\n\\ndef test_crop_array():\\n    array = [[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15],[16,17,18,19,20]]\\n    assert crop_array(1, 1, 3, 3, array) == [[6,7,8],[11,12,13]]\",\n",
       "  100.0),\n",
       " ('def _get_spot_surface(image, spot_y, spot_x, radius_yx):\\n    \\n    # get boundaries of the surface surrounding the spot\\n    y_spot_min = max(0, int(spot_y - radius_yx))\\n    y_spot_max = min(image.shape[0], int(spot_y + radius_yx))\\n    x_spot_min = max(0, int(spot_x - radius_yx))\\n    x_spot_max = min(image.shape[1], int(spot_x + radius_yx))\\n\\n    # get the surface of the spot\\n    image_spot = image[y_spot_min:y_spot_max + 1,\\n                       x_spot_min:x_spot_max + 1]\\n\\n    return image_spot',\n",
       "  '',\n",
       "  0.0),\n",
       " ('def transform_point(p, affine):\\n    \\n    M = affine[:3, :3]\\n    offset = affine[:3, 3]\\n    return M.dot(p) + offset',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import transform_point\\n\\ndef test_transform_point():\\n    p = np.array([0, 0, 1])\\n    affine = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]])\\n    assert np.allclose(transform_point(p, affine), np.array([0, 0, 1]))',\n",
       "  100.0),\n",
       " ('def pixelToGeo(x, y, bbox, width, height):\\n    \\n\\n    x_g = bbox[0] + ((bbox[2]-bbox[0]) / width)*x    \\n    y_g = bbox[3] - ((bbox[3]-bbox[1]) / height)*y\\n\\n    return (x_g, y_g)',\n",
       "  'import sys\\nsys.path.append(\".\")  # To import source.py from the same directory\\nfrom source import pixelToGeo\\n\\ndef test_pixelToGeo_functionality():\\n    bbox = (1, 2, 3, 4)  # arbitrary values\\n    width, height = 5, 6  # arbitrary values\\n    x, y = 10, 20  # arbitrary values\\n    expected_result = (2.6, 8.33)  # calculated based on the function implementation\\n\\n    result = pixelToGeo(x, y, bbox, width, height)\\n\\n    assert result == expected_result',\n",
       "  100.0),\n",
       " ('def custom_score(game, player):\\n    \\n\\n    # This function calculates the heuristic as the sum of the distance to center\\n    # and the number of movement options left.\\n\\n    # The inverse distance to the center is always < 1, the difference between two\\n    # numbers of motion options is at least 1 apart. To make the inverse distance matter\\n    # at all, it has to be multiplied by a factor, whereas #my-moves has to be divided.\\n    k = 5.0\\n\\n\\n    if game.is_loser(player):\\n        return float(\"-inf\")\\n\\n    if game.is_winner(player):\\n        return float(\"inf\")\\n\\n    \\n    position = game.get_player_location(player)\\n\\n    deltax = position[0] - game.width/2\\n    deltay = position[1] - game.height/2\\n\\n    if deltax == 0 and deltay == 0:\\n        return float(\"inf\")\\n    else:\\n        return k * 1.0 / (abs(deltax) + abs(deltay)) + (1.0/k) * len(game.get_legal_moves(player))',\n",
       "  \"import os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\n\\nimport source  # assuming the source code file is in the same directory\\n\\ndef test_custom_score():\\n    game = source.Game()  # assuming Game is a class in source.py\\n    player = source.Player()  # assuming Player is a class in source.py\\n    score = source.custom_score(game, player)\\n\\n    # assuming that the score is a float\\n    assert abs(score - 0.0) < 1e-9  # we use an small number here to account for possible floating point inaccuracies\",\n",
       "  8.0),\n",
       " (\"def overlap_status(a, b):\\n    \\n    # Get the endpoints\\n    a1, a2 = a.min(), a.max()\\n    b1, b2 = b.min(), b.max()\\n\\n    # Do the comparison\\n    if a1 >= b1 and a2 <= b2:\\n        result = 'full'\\n    elif a2 < b1 or b2 < a1:\\n        result = 'none'\\n    else:\\n        result = 'partial'\\n\\n    return result\",\n",
       "  \"import sys\\nsys.path.append('.')\\nfrom source import overlap_status\\n\\ndef test_overlap_status_full():\\n    a = [1, 2, 3, 4]\\n    b = [3, 4, 5, 6]\\n    assert overlap_status(a, b) == 'full'\",\n",
       "  22.0),\n",
       " ('def ml_proxy(model, x, y, d, sample):\\n    \\n    id_ctrl_a = (sample == \"a\") & (d == 0)\\n    model.fit(x[id_ctrl_a,], y[id_ctrl_a])\\n    pred_ctrl = model.predict(x)\\n\\n    id_treat_a = (sample == \"a\") & (d == 1)\\n    model.fit(x[id_treat_a,], y[id_treat_a] - pred_ctrl[id_treat_a])\\n    pred_treat = model.predict(x)\\n\\n    b_hat = pred_ctrl  # baseline effect given X\\n    s_hat = pred_treat  # treatment effect given X\\n\\n    return b_hat, s_hat',\n",
       "  'import pytest\\nfrom source import ml_proxy  # assuming source.py is in the same directory\\n\\nclass TestMLProxy:\\n\\n    def test_ml_proxy(self):\\n        # We\\'ll need to create some test data here\\n        # For simplicity, we\\'ll just create some placeholder lists\\n        x = [[1], [2], [3]]\\n        y = [1, 2, 3]\\n        d = [0, 0, 1]\\n        sample = [\"a\", \"b\", \"a\"]\\n        \\n        # Test with sample data\\n        b_hat, s_hat = ml_proxy(None, x, y, d, sample)\\n        assert b_hat == [1, 2, 1]  # this is just an example, replace with actual expected values\\n        assert s_hat == [2, 3, 2]  # this is just an example, replace with actual expected values',\n",
       "  30.0),\n",
       " ('def saturn_rot_elements_at_epoch(T, d):\\n    \\n    ra = 40.589 - 0.036 * T\\n    dec = 83.537 - 0.004 * T\\n    W = 38.90 + 810.7939024 * d\\n\\n    return ra, dec, W',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport sys\\nsys.path.append(\".\")\\nfrom source import saturn_rot_elements_at_epoch\\n\\ndef test_saturn_rot_elements_at_epoch():\\n    T, d = 1, 0.5 # Test data\\n    expected_output = (40.589 - 0.036 * T, 83.537 - 0.004 * T, 38.90 + 810.7939024 * d)\\n    assert saturn_rot_elements_at_epoch(T, d) == expected_output',\n",
       "  100.0),\n",
       " ('def is_scalar(vect_array):\\n    \\n    if isinstance(vect_array, tuple):\\n        return False\\n    if isinstance(vect_array, list):\\n        return False\\n\\n    has_ndim_2 = vect_array.ndim == 2\\n    if not has_ndim_2:\\n        return False\\n    has_singleton_dim_1 = vect_array.shape[1] == 1\\n    return has_singleton_dim_1',\n",
       "  '# This is a test for the source.py file\\n\\nimport pytest\\nimport source  # We import the source file\\n\\nclass TestSource:\\n\\n    def test_is_scalar(self):\\n        # Given\\n        vect_array = source.vect_array  # We use the array from source.py\\n\\n        # When\\n        result = source.is_scalar(vect_array)\\n\\n        # Then\\n        assert result == True, \"The function did not correctly identify the scalar\"',\n",
       "  10.0),\n",
       " ('import torch\\n\\ndef gather_scatter_python(input, edges, directed: bool = False):\\n    \\n    if not (input.dim() == 2):\\n        raise ValueError(\"input can only have 2 dimensions.\")\\n    if not (edges.dim() == 2):\\n        raise ValueError(\"edges can only have 2 dimensions.\")\\n    if not (edges.shape[1] == 2):\\n        raise ValueError(\"edges must be of shape (num_edges, 2).\")\\n\\n    num_vertices, input_feature_dim = input.shape\\n    num_edges = edges.shape[0]\\n    output = torch.zeros_like(input)\\n    idx0 = edges[:, 0].view(num_edges, 1).expand(num_edges, input_feature_dim)\\n    idx1 = edges[:, 1].view(num_edges, 1).expand(num_edges, input_feature_dim)\\n\\n    output = output.scatter_add(0, idx0, input.gather(0, idx1))\\n    if not directed:\\n        output = output.scatter_add(0, idx1, input.gather(0, idx0))\\n    return output',\n",
       "  'import torch\\nimport pytest\\nfrom source import gather_scatter_python\\n\\ndef test_gather_scatter_python():\\n    # Test with a random input and edges\\n    input = torch.randn(4, 10)\\n    edges = torch.LongTensor([[0, 1], [1, 2], [2, 3]])\\n    result = gather_scatter_python(input, edges)\\n    expected = torch.zeros_like(input)\\n    expected[0, :] = input[1, :] + input[0, :]\\n    expected[1, :] = input[2, :] + input[1, :]\\n    expected[2, :] = input[3, :] + input[2, :]\\n    assert torch.allclose(result, expected)\\n\\n    # Test with a random input, edges and directed=True\\n    input = torch.randn(4, 10)\\n    edges = torch.LongTensor([[0, 1], [1, 2]])\\n    result = gather_scatter_python(input, edges, directed=True)\\n    expected = torch.zeros_like(input)\\n    expected[0, :] = input[1, :] + input[0, :]\\n    expected[1, :] = input[2, :] + input[1, :]\\n    assert torch.allclose(result, expected)\\n\\n    # Test with a random input, edges and directed=False\\n    input = torch.randn(4, 10)\\n    edges = torch.LongTensor([[0, 1], [1, 2], [2, 3]])\\n    result = gather_scatter_python(input, edges, directed=False)\\n    expected = torch.zeros_like(input)\\n    expected[0, :] = input[1, :] + input[0, :]\\n    expected[1, :] = input[2, :] + input[1, :]\\n    expected[2, :] = input[3, :] + input[2, :]\\n    expected[3, :] = input[0, :] + input[3, :]\\n    assert torch.allclose(result, expected)\\n\\n    # Test with a random input, edges of different type and directed=False\\n    input = torch.randn(4, 10)\\n    edges = torch.cuda.LongTensor([[0, 1], [1, 2], [2, 3]])\\n    result = gather_scatter_python(input, edges, directed=False)\\n    expected = torch.zeros_like(input)\\n    expected[0, :] = input[1, :] + input[0, :]\\n    expected[1, :] = input[2, :] + input[1, :]\\n    expected[2, :] = input[3, :] + input[2, :]\\n    expected[3, :] = input[0, :] + input[3, :]\\n    assert torch.allclose(result.cpu(), expected)\\n\\n    # Test with a random input, edges of different type and directed=True\\n    input = torch.randn(4, 10)\\n    edges = torch.cuda.LongTensor([[0, 1], [1, 2]])\\n    result = gather_scatter_python(input, edges, directed=True)\\n    expected = torch.zeros_like(input)\\n    expected[0, :] = input[1, :] + input[0, :]\\n    expected[1, :] = input[2, :] + input[1, :]\\n    assert torch.allclose(result.cpu(), expected)',\n",
       "  82.0),\n",
       " ('def overlaps(box1, box2):\\n    \\n    \\n    if not ((box1[0] <= box2[0] <= box1[2])\\n        or (box1[0] <= box2[2] <= box1[2])\\n        or (box2[0] <= box1[0] and box2[2] >= box1[2])):\\n        return False\\n    \\n    if not ((box1[1] <= box2[1] <= box1[3])\\n        or (box1[1] <= box2[3] <= box1[3])\\n        or (box2[1] <= box1[1] and box2[3] >= box1[3])):\\n        return False\\n    \\n    return True',\n",
       "  '# test_source.py\\n\\nfrom source import overlaps\\n\\n# Test Case 1: Boxes do not overlap\\ndef test_overlaps1():\\n    box1 = (1, 1, 3, 3)\\n    box2 = (4, 4, 6, 6)\\n    assert not overlaps(box1, box2)\\n\\n# Test Case 2: Boxes overlap in one dimension\\ndef test_overlaps2():\\n    box1 = (1, 1, 3, 3)\\n    box2 = (2, 2, 4, 4)\\n    assert overlaps(box1, box2)\\n\\n# Test Case 3: Box1 is inside box2\\ndef test_overlaps3():\\n    box1 = (2, 2, 5, 5)\\n    box2 = (1, 1, 6, 6)\\n    assert overlaps(box1, box2)\\n\\n# Test Case 4: Box2 is inside box1\\ndef test_overlaps4():\\n    box1 = (1, 1, 6, 6)\\n    box2 = (2, 2, 5, 5)\\n    assert overlaps(box1, box2)\\n\\n# Test Case 5: Box1 and Box2 are the same\\ndef test_overlaps5():\\n    box1 = (2, 2, 5, 5)\\n    box2 = (2, 2, 5, 5)\\n    assert overlaps(box1, box2)',\n",
       "  83.0),\n",
       " ('def format_sec_to_dhm(sec):\\n    \\n    rem_int, s_int = divmod(int(sec), 60)\\n    rem_int, m_int, = divmod(rem_int, 60)\\n    d_int, h_int, = divmod(rem_int, 24)\\n    return \"{}d {:02d}h {:02d}m\".format(d_int, h_int, m_int)',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\"./\") # To import source.py which is in the same directory\\nfrom source import format_sec_to_dhm\\n\\ndef test_format_sec_to_dhm():\\n    assert format_sec_to_dhm(0) == \"0d 00h 00m\"\\n    assert format_sec_to_dhm(60) == \"0d 01h 00m\"\\n    assert format_sec_to_dhm(3600) == \"0d 01h 00m\"\\n    assert format_sec_to_dhm(3660) == \"0d 01h 01m\"\\n    assert format_sec_to_dhm(45600) == \"1d 01h 01m\"\\n    assert format_sec_to_dhm(86400) == \"1d 00h 00m\"\\n    assert format_sec_to_dhm(86401) == \"1d 00h 01m\"\\n    assert format_sec_to_dhm(90060) == \"1d 02h 01m\"\\n    assert format_sec_to_dhm(330999) == \"4d 23h 59m\"',\n",
       "  100.0),\n",
       " ('def s_to_p(s):\\n    \\n    from scipy.stats import norm\\n    return norm.sf(s)',\n",
       "  'import pytest\\nfrom source import s_to_p\\n\\ndef test_s_to_p():\\n    assert s_to_p(0) == 0.5\\n    assert s_to_p(-10) == 1.0\\n    assert s_to_p(10) == 7.61985302416047e-24\\n    assert s_to_p(1) == 0.15865525393145707\\n    assert s_to_p(-1) == 0.8413447460685429',\n",
       "  100.0),\n",
       " ('def simple(y, x, N, beta, gamma):\\n    \\n    S = -beta * y[0] * y[1] / N\\n    I = beta * y[0] * y[1] / N - gamma * y[1]\\n    R = gamma * y[1]\\n    return S, I, R',\n",
       "  'import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\\nimport source\\n\\ndef test_simple():\\n    y = [1, 1]\\n    x = 0\\n    N = 1\\n    beta = 1\\n    gamma = 1\\n    assert source.simple(y,x,N,beta,gamma) == (1, -1, -1)',\n",
       "  100.0),\n",
       " ('def ideal_gas_law(T, Vm):\\n    \\n    R = 0.08205 #L*atm/K/mol\\n    return R * T / Vm',\n",
       "  '#!/usr/bin/env python\\n# -*- coding: utf-8 -*-\\n\"\"\"Tests for `ideal_gas_law` function from `source` module.\"\"\"\\n\\nimport pytest\\nimport sys\\nsys.path.insert(0, \\'.\\')  # add current directory to path\\nfrom source import ideal_gas_law\\n\\n\\ndef test_ideal_gas_law():\\n    \"\"\"Test ideal_gas_law function with known values.\"\"\"\\n    assert ideal_gas_law(298.15, 22.4) == 62.56817507686977\\n\\n\\ndef test_ideal_gas_law_exception():\\n    \"\"\"Test ideal_gas_law function with an exception.\"\"\"\\n    with pytest.raises(ZeroDivisionError):\\n        ideal_gas_law(0, 1)',\n",
       "  100.0),\n",
       " ('def dot(x, y):\\n    \\n    return x.dot(y)',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport source  # assuming source.py is in the same directory\\n\\ndef test_dot_product():\\n    x = source.Vector([1, 2, 3])\\n    y = source.Vector([4, 5, 6])\\n    assert source.dot(x, y) == 32',\n",
       "  50.0),\n",
       " ('import torch\\n\\ndef normalized_mean_square_error(output, target, reduction=\\'mean\\'):\\n    \\n\\n    nmse_a = torch.sqrt(torch.sum(torch.square(output - target), dim=-1))\\n    nmse_b = torch.sqrt(torch.sum(torch.square(target), dim=-1))\\n\\n    if reduction == \\'mean\\':\\n        nmse = torch.mean(nmse_a / nmse_b)\\n    elif reduction == \\'sum\\':\\n        nmse = torch.sum(nmse_a / nmse_b)\\n    elif reduction == \\'none\\':\\n        nmse = nmse_a / nmse_b\\n    else:\\n        raise Exception(\"The reduction values are \\'mean\\', \\'sum\\', and \\'none\\'.\")\\n    return nmse',\n",
       "  \"import pytest\\nimport torch\\n\\nfrom source import normalized_mean_square_error\\n\\ndef test_normalized_mean_square_error():\\n    output = torch.tensor([2, 4, 6, 8])\\n    target = torch.tensor([1, 3, 5, 7])\\n\\n    # Test with 'mean' reduction\\n    nmse = normalized_mean_square_error(output, target, 'mean')\\n    assert torch.isclose(nmse, torch.tensor(0.25))\\n\\n    # Test with 'sum' reduction\\n    nmse = normalized_mean_square_error(output, target, 'sum')\\n    assert torch.isclose(nmse, torch.tensor(0.25))\\n\\n    # Test with 'none' reduction\\n    nmse = normalized_mean_square_error(output, target, 'none')\\n    expected_nmse = torch.sqrt(torch.sum(torch.square(output - target), dim=-1)) / torch.sqrt(torch.sum(torch.square(target), dim=-1))\\n    assert torch.isclose(nmse, expected_nmse)\\n\\n    # Test with invalid reduction\\n    with pytest.raises(Exception):\\n        normalized_mean_square_error(output, target, 'invalid')\",\n",
       "  58.0),\n",
       " ('def predict_by_moving_avg_growth(stock, s, **_):\\n    \\n    stockPriceHistory = len(stock.price_history)\\n    if stockPriceHistory < s+1:\\n        return None\\n    else:\\n        ma = sum(stock.price_history[-s:]) / s\\n        growth = ma - sum(stock.price_history[-s-1:-1]) / s\\n        predicted = ma + (s/2+0.5)*growth\\n        if predicted > 0:\\n            return predicted\\n        else:\\n            return 0',\n",
       "  'import pytest\\nfrom source import Stock, predict_by_moving_avg_growth\\n\\nclass TestPredictByMovingAvgGrowth:\\n\\n    def setup_method(self, method):\\n        # Instantiate a Stock object for testing\\n        self.stock = Stock([10, 15, 5, 20, 30, 10])\\n\\n    def test_predict_by_moving_avg_growth_with_s_one(self):\\n        # Test when s = 1\\n        result = predict_by_moving_avg_growth(self.stock, 1)\\n        assert result == 12.5, \"Test Failed: The function did not predict correctly\"\\n\\n    def test_predict_by_moving_avg_growth_with_s_two(self):\\n        # Test when s = 2\\n        result = predict_by_moving_avg_growth(self.stock, 2)\\n        assert result == 18.75, \"Test Failed: The function did not predict correctly\"\\n\\n    def test_predict_by_moving_avg_growth_with_s_three(self):\\n        # Test when s = 3\\n        result = predict_by_moving_avg_growth(self.stock, 3)\\n        assert result == 22.833333333333334, \"Test Failed: The function did not predict correctly\"\\n\\n    def test_predict_by_moving_avg_growth_with_s_zero(self):\\n        # Test when s = 0, should return None\\n        result = predict_by_moving_avg_growth(self.stock, 0)\\n        assert result is None, \"Test Failed: The function did not predict correctly\"\\n\\n    def test_predict_by_moving_avg_growth_with_s_four(self):\\n        # Test when s = 4\\n        result = predict_by_moving_avg_growth(self.stock, 4)\\n        assert result == 28.571428571428572, \"Test Failed: The function did not predict correctly\"\\n\\n    def test_predict_by_moving_avg_growth_with_s_five(self):\\n        # Test when s = 5\\n        result = predict_by_moving_avg_growth(self.stock, 5)\\n        assert result == 34.285714285714286, \"Test Failed: The function did not predict correctly\"',\n",
       "  10.0),\n",
       " (\"def create_line(action, timestamp, y_min, y_max):\\n    \\n\\n    if action == 1:\\n        colour = 'blue'\\n    elif action == -1:\\n        colour = 'red'\\n\\n    return {\\n        'type': 'line',\\n        'x0': timestamp,\\n        'y0': y_min,\\n        'x1': timestamp,\\n        'y1': y_max,\\n        'line': {\\n            'color': colour,\\n            'width': 2,\\n            'dash': 'dot'\\n        }\\n    }\",\n",
       "  \"# test_source.py\\nimport pytest\\nfrom source import create_line\\n\\ndef test_create_line():\\n    result = create_line(1, 100, 20, 30)\\n    assert result == {'type': 'line', 'x0': 100, 'y0': 20, 'x1': 100, 'y1': 30, 'line': {'color': 'blue', 'width': 2, 'dash': 'dot'}}\\n\\ndef test_create_line_negative():\\n    result = create_line(-1, 100, 20, 30)\\n    assert result == {'type': 'line', 'x0': 100, 'y0': 20, 'x1': 100, 'y1': 30, 'line': {'color': 'red', 'width': 2, 'dash': 'dot'}}\",\n",
       "  100.0),\n",
       " ('def unsqueeze(x):\\n    \\n    b, c, h, w = x.size()\\n    x = x.view(b, c // 4, 2, 2, h, w)\\n    x = x.permute(0, 1, 4, 2, 5, 3).contiguous()\\n    x = x.view(b, c // 4, h * 2, w * 2)\\n\\n    return x',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\".\") # To find source.py in the same directory\\nfrom source import unsqueeze\\n\\ndef test_unsqueeze():\\n    x = unsqueeze(1)\\n    assert x == 1  # You may replace 1 with any value that matches the expected output of your function',\n",
       "  33.0),\n",
       " (\"def _point_cloud_error(src_pts, tgt_pts):\\n    \\n    from scipy.spatial.distance import cdist\\n    Y = cdist(src_pts, tgt_pts, 'euclidean')\\n    dist = Y.min(axis=1)\\n    return dist\",\n",
       "  \"import pytest\\nimport numpy as np\\nfrom scipy.spatial.distance import cdist\\nfrom source import _point_cloud_error\\n\\ndef test_point_cloud_error():\\n    src_pts = np.array([[1, 2], [3, 4], [5, 6]])\\n    tgt_pts = np.array([[7, 8], [9, 10], [11, 12]])\\n    result = _point_cloud_error(src_pts, tgt_pts)\\n    assert np.array_equal(result, np.array([8.074, 9.839, 10.828])), 'Failure on valid input'\\n    result = _point_cloud_error(np.array([]), np.array([]))\\n    assert np.array_equal(result, np.array([])), 'Failure on empty arrays'\\n    result = _point_cloud_error(np.array([[1, 2], [3, 4]]), np.array([[7, 8, 9], [10, 11, 12]]))\\n    assert np.array_equal(result, np.array([8.602, 9.839, 11.748])), 'Failure on arrays of different length'\\n    result = _point_cloud_error(np.array([[1, 2, 3], [4, 5, 6]]), np.array([[7, 8], [9, 10], [11, 12]]))\\n    assert np.array_equal(result, np.array([10.074, 11.366])), 'Failure on arrays of different dimensions'\",\n",
       "  100.0),\n",
       " ('def ls_are_valid_params(params, model):\\n    r\\n    (best_period) = params\\n    (min_period, max_period) = model.optimizer.period_range\\n    if ((0 < min_period) and (min_period <= best_period) and\\n        (best_period <= max_period)):\\n        are_valid = True\\n    else:\\n        are_valid = False\\n    return are_valid',\n",
       "  'import pytest\\nfrom source import ls_are_valid_params, Model  # Assuming Model is the class in source.py\\n\\nclass TestLsAreValidParams:\\n    def test_valid_input(self):\\n        model = Model()  # Instantiate your model here\\n        params = (10, 20)  # Replace with actual parameters \\n        assert ls_are_valid_params(params, model)\\n\\n    def test_invalid_input_below_min(self):\\n        model = Model()  # Instantiate your model here\\n        params = (5, 20)  # Replace with actual parameters \\n        assert not ls_are_valid_params(params, model)\\n\\n    def test_invalid_input_above_max(self):\\n        model = Model()  # Instantiate your model here\\n        params = (30, 20)  # Replace with actual parameters \\n        assert not ls_are_valid_params(params, model)\\n        \\n    def test_invalid_input_zero_min(self):\\n        model = Model()  # Instantiate your model here\\n        params = (0, 20)  # Replace with actual parameters \\n        assert not ls_are_valid_params(params, model)',\n",
       "  12.0),\n",
       " ('def _sigma_u_mle(df_resid, nobs, sigma_u):\\n    \\n    return sigma_u * df_resid / nobs',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\nfrom source import _sigma_u_mle\\n\\ndef test_sigma_u_mle():\\n    df_resid = 100   # suppose df_resid is 100\\n    nobs = 200   # suppose nobs is 200\\n    sigma_u = 1   # suppose sigma_u is 1\\n    assert abs(_sigma_u_mle(df_resid, nobs, sigma_u) - 0.5) < 1e-9  # since we expect the result to be 0.5\",\n",
       "  100.0),\n",
       " ('def compressability_moist_air(p, T, x_w):\\n    \\n    a_0, a_1, a_2 = 1.58123E-6, -2.9331E-8, 1.1043E-10\\n    b_0, b_1 = 5.707E-6, -2.051E-8\\n    c_0, c_1 = 1.9898E-4, -2.376E-6\\n    d, e = 1.83E-11, -0.765E-8\\n\\n    Z = 1 - (p/T)*(a_0 + a_1*T + a_2*(T**2)\\n                   + (b_0 + b_1*T)*x_w\\n                   + (c_0 + c_1*T)*(x_w**2)) + ((p/T)**2) * (d + e*(x_w**2))\\n                                                            # eq. 12 in [1]\\n    return Z',\n",
       "  'import pytest\\nfrom source import compressability_moist_air\\n\\ndef test_compressability_moist_air():\\n    # Test 1: when p=1, T=1, x_w=1\\n    assert pytest.approx(compressability_moist_air(1, 1, 1), 0.00001) == 0.99999\\n\\n    # Test 2: when p=2, T=2, x_w=2\\n    assert pytest.approx(compressability_moist_air(2, 2, 2), 0.00001) == 1.99999\\n\\n    # Test 3: when p=3, T=3, x_w=3\\n    assert pytest.approx(compressability_moist_air(3, 3, 3), 0.00001) == 2.99999',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef xy_wh_r_2_xy_sigma(xywhr):\\n    \\n    _shape = xywhr.shape\\n    assert _shape[-1] == 5\\n    xy = xywhr[..., :2]\\n    wh = xywhr[..., 2:4].clamp(min=1e-7, max=1e7).reshape(-1, 2)\\n    r = xywhr[..., 4]\\n    cos_r = torch.cos(r)\\n    sin_r = torch.sin(r)\\n    R = torch.stack((cos_r, -sin_r, sin_r, cos_r), dim=-1).reshape(-1, 2, 2)\\n    S = 0.5 * torch.diag_embed(wh)\\n\\n    sigma = R.bmm(S.square()).bmm(R.permute(0, 2,\\n                                            1)).reshape(_shape[:-1] + (2, 2))\\n\\n    return xy, sigma',\n",
       "  'import pytest\\nimport torch\\n\\nfrom source import xy_wh_r_2_xy_sigma\\n\\ndef test_xy_wh_r_2_xy_sigma():\\n    xywhr = torch.tensor([[[1, 2, 3, 4, 5], [5, 6, 7, 8, 9]]])  # you can replace this with any tensor of your choice\\n    xy, sigma = xy_wh_r_2_xy_sigma(xywhr)\\n    assert torch.allclose(xy, torch.tensor([[[1, 2], [5, 6]]]))  # the expected xy output\\n    assert torch.allclose(sigma, torch.tensor([[[1.41421356, 1.41421356, 1.41421356, 1.41421356], \\n                                                [1.41421356, 1.41421356, 1.41421356, 1.41421356]]]))  # the expected sigma output\\n\\nif __name__ == \"__main__\":\\n    test_xy_wh_r_2_xy_sigma()',\n",
       "  100.0),\n",
       " ('def is_square(X):\\n    \\n    return X.ndim >= 2 and X.shape[-2] == X.shape[-1]',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')\\nfrom source import is_square\\n\\ndef test_is_square():\\n    assert is_square(1) == False\\n    assert is_square([[1, 2], [3, 4]]) == True\\n    assert is_square([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) == True\\n    assert is_square([[1, 2], [3, 4], [5, 6]]) == False\",\n",
       "  100.0),\n",
       " ('def _calc_hgt(f):\\n    \\n    sfc = f.surfalt_m.load()\\n    dz = f.dz_m.load() * -1.0\\n    # These are negative in RRFS-CMAQ, but you resorted and are adding from the surface,\\n    # so make them positive.\\n    dz[:, 0, :, :] = dz[:, 0, :, :] + sfc  # Add the surface altitude to the first model level only\\n    z = dz.rolling(z=len(f.z), min_periods=1).sum()\\n    z.name = \"alt_msl_m_full\"\\n    z.attrs[\"long_name\"] = \"Altitude MSL Full Layer in Meters\"\\n    z.attrs[\"units\"] = \"m\"\\n    return z',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import Field\\nimport numpy as np\\n\\ndef test_calc_hgt():\\n    f = Field()\\n    f.surfalt_m = np.array([1000, 2000, 3000])\\n    f.dz_m = np.array([-100, -200, -300])\\n    \\n    expected_result = np.array([[1100, 1200, 1300],\\n                               [2200, 2300, 2400],\\n                               [3300, 3400, 3500]])\\n\\n    result = _calc_hgt(f)\\n    \\n    assert np.array_equal(result, expected_result), \"The result does not match the expected result.\"',\n",
       "  11.0),\n",
       " ('def convert_RGB_to_BGR(color):\\n    \\n    return color[2], color[1], color[0]',\n",
       "  'import pytest\\nimport source  # importing the source file\\n\\ndef test_convert_RGB_to_BGR():\\n    color = (255, 0, 0)  # RGB color code for red\\n    expected_output = (0, 0, 255)  # corresponding BGR color code\\n    assert source.convert_RGB_to_BGR(color) == expected_output',\n",
       "  100.0),\n",
       " ('def inside(rectangle, point):\\n    \\n    return rectangle.se.lon >= point.lon and \\\\\\n           point.lon >= rectangle.nw.lon and \\\\\\n           rectangle.nw.lat >= point.lat and \\\\\\n           rectangle.se.lat <= point.lat',\n",
       "  'import sys\\nsys.path.append(\".\")  # To import the module from the same directory\\nimport source  # The module that we\\'ll be testing\\nimport pytest  # Pytest framework to build the tests\\n\\nclass TestRectangle:\\n    def test_inside(self):\\n        # Creating instances of the class Point\\n        nw = source.Point(40.73, -74.01)\\n        se = source.Point(40.13, -71.10)\\n        rectangle = source.Rectangle(nw, se)\\n\\n        # Creating an instance of the class Point for testing\\n        point = source.Point(40.23, -73.30)\\n\\n        # Making an assertion\\n        assert source.inside(rectangle, point) == True',\n",
       "  50.0),\n",
       " ('def chopping_frequency(frequency1, frequency2):\\n    \\n    return 2 * (frequency2 - frequency1)',\n",
       "  '# test_source.py\\nimport source  # assuming the function is in source.py\\nimport pytest\\n\\ndef test_chopping_frequency():\\n    # create test data\\n    frequency1 = 5\\n    frequency2 = 7\\n\\n    # call the function with test data\\n    result = source.chopping_frequency(frequency1, frequency2)\\n\\n    # make an assertion\\n    assert result == 2 * (frequency2 - frequency1), \"The function did not return the expected result.\"',\n",
       "  100.0),\n",
       " ('def ColorLuminance(color):\\n    \\n    r = int(color[0:2], 16)\\n    g = int(color[2:4], 16)\\n    b = int(color[4:6], 16)\\n    return (299 * r + 587 * g + 114 * b) / 1000.0',\n",
       "  '# test_color.py\\n\\nimport pytest\\nimport source  # assuming the source code is in a file named source.py\\n\\ndef test_ColorLuminance():\\n    color = \"#FFFFFF\"\\n    assert source.ColorLuminance(color) == 1.0',\n",
       "  40.0),\n",
       " ('def accuracy(scores, targets, k):\\n    \\n    batch_size = targets.size(0)\\n    _, ind = scores.topk(k, 1, True, True)\\n    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\\n    correct_total = correct.view(-1).float().sum()  # 0D tensor\\n    return correct_total.item() * (100.0 / batch_size)',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")  # append src directory to path\\nimport source  # import the source file\\nimport pytest\\n\\ndef test_accuracy():\\n    scores = torch.Tensor([[1.2, 2.3, 0.4, 5.6], [2.1, 1.3, 4.5, 3.7]])\\n    targets = torch.Tensor([1, 0])\\n    k = 2\\n    assert source.accuracy(scores, targets, k) == 50.0',\n",
       "  17.0),\n",
       " ('def center_crop(data, shape):\\n    \\n    assert 0 < shape[0] <= data.shape[-2]\\n    assert 0 < shape[1] <= data.shape[-1]\\n    w_from = (data.shape[-2] - shape[0]) // 2\\n    h_from = (data.shape[-1] - shape[1]) // 2\\n    w_to = w_from + shape[0]\\n    h_to = h_from + shape[1]\\n    if len(data.shape)>2:\\n        return data[..., w_from:w_to, h_from:h_to]\\n    else:\\n        return data[w_from:w_to, h_from:h_to]',\n",
       "  '# Import necessary libraries\\nimport pytest\\nimport numpy as np\\n\\n# Import the source file\\nfrom source import center_crop\\n\\n# Define the test function\\ndef test_center_crop():\\n    # Test with random data and shape\\n    data = np.random.rand(100, 100)\\n    shape = (50, 50)\\n    result = center_crop(data, shape)\\n    assert result.shape == shape\\n\\n    # Test with random data and shape\\n    data = np.random.rand(100, 100, 3)\\n    shape = (50, 50, 3)\\n    result = center_crop(data, shape)\\n    assert result.shape == shape\\n\\n# Run the test\\ntest_center_crop()',\n",
       "  90.0),\n",
       " ('def dot(x, y):\\n    \\n    return x.dot(y)',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import dot\\n\\ndef test_dot_product():\\n    x = [1, 2, 3]\\n    y = [4, 5, 6]\\n    assert dot(x, y) == 32',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef cosine(query, candidates):\\n    \\n\\n    cands_size = candidates.size()\\n    cands_flat = candidates.view(-1, cands_size[-1])\\n    output_flat = torch.mv(cands_flat, query) / (torch.norm(cands_flat) * torch.norm(query))\\n    output = output_flat.view(*cands_size[:-1])\\n    return output',\n",
       "  'import torch\\nimport pytest\\nfrom source import cosine\\n\\ndef test_cosine():\\n    query = torch.tensor([1, 2, 3, 4])\\n    candidates = torch.tensor([[5, 6, 7, 8], [9, 10, 11, 12]])\\n    result = cosine(query, candidates)\\n    assert torch.allclose(result, torch.tensor([[31.62403884, 44.54421144]]), atol=0.0001)',\n",
       "  71.0),\n",
       " ('def remove_nodes(g, nids, ntype=None, store_ids=False):\\n    r\\n    g = g.clone()\\n    g.remove_nodes(nids, ntype=ntype, store_ids=store_ids)\\n    return g',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\"..\") # Adds the parent directory to the path to import the module\\nimport pytest\\nfrom source import remove_nodes\\n\\ndef test_remove_nodes():\\n    g = remove_nodes.Graph() # Assuming Graph() is a class in the source module\\n    nids = [1, 2, 3]\\n    result = remove_nodes(g, nids)\\n    assert len(result.nodes) == 0, \"Nodes were not removed correctly\"',\n",
       "  20.0),\n",
       " ('def calc_field(phi_k, kx_v, ky_v, kz_v):\\n    \\n\\n    E_kx = -1j * kx_v * phi_k\\n    E_ky = -1j * ky_v * phi_k\\n    E_kz = -1j * kz_v * phi_k\\n\\n    return E_kx, E_ky, E_kz',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import calc_field\\n\\ndef test_calc_field():\\n    phi_k = 1\\n    kx_v = 2\\n    ky_v = 3\\n    kz_v = 4\\n\\n    E_kx, E_ky, E_kz = calc_field(phi_k, kx_v, ky_v, kz_v)\\n\\n    assert E_kx == -2j * kx_v * phi_k\\n    assert E_ky == -2j * ky_v * phi_k\\n    assert E_kz == -2j * kz_v * phi_k',\n",
       "  100.0),\n",
       " (\"def boxcox_reverse(x_t, lam):\\n    \\n\\n    orig_scale = (lam*x_t + 1)**(1/lam)\\n    orig_scale = (orig_scale.fillna(0)).transpose('time', 'lat', 'lon')\\n\\n    return orig_scale\",\n",
       "  \"import pytest\\nimport numpy as np\\nimport xarray as xr\\nimport pandas as pd\\n\\nfrom source import boxcox_reverse\\n\\nclass TestBoxCoxReverse:\\n\\n    def test_boxcox_reverse(self):\\n        \\n        # Case 1: Normal operation with numeric data\\n        x_t = xr.DataArray(np.array([1, 2, 3, 4, 5]), \\n                          coords=[('time', np.arange(5)),('lat', np.arange(1)),('lon', np.arange(1))],\\n                          dims=['time', 'lat', 'lon'])\\n        lam = 1\\n        expected_result = xr.DataArray(np.array([1, 1.03664409, 1.1127973, 1.2043458, 1.3126094]), \\n                                     coords=[('time', np.arange(5)),('lat', np.arange(1)),('lon', np.arange(1))],\\n                                     dims=['time', 'lat', 'lon'])\\n        result = boxcox_reverse(x_t, lam)\\n        np.testing.assert_almost_equal(result, expected_result)\\n        \\n        # Case 2: Operation with NaN values\\n        x_t = xr.DataArray(np.array([1, np.nan, 3, 4, 5]), \\n                          coords=[('time', np.arange(5)),('lat', np.arange(1)),('lon', np.arange(1))],\\n                          dims=['time', 'lat', 'lon'])\\n        result = boxcox_reverse(x_t, lam)\\n        np.testing.assert_almost_equal(result, expected_result)\\n        \\n        # Case 3: Operation with negative values\\n        x_t = xr.DataArray(np.array([-1, -2, -3, -4, -5]), \\n                          coords=[('time', np.arange(5)),('lat', np.arange(1)),('lon', np.arange(1))],\\n                          dims=['time', 'lat', 'lon'])\\n        result = boxcox_reverse(x_t, lam)\\n        np.testing.assert_almost_equal(result, expected_result)\\n        \\n        # Case 4: Operation with 0\\n        x_t = xr.DataArray(np.array([0, 0, 0, 0, 0]), \\n                          coords=[('time', np.arange(5)),('lat', np.arange(1)),('lon', np.arange(1))],\\n                          dims=['time', 'lat', 'lon'])\\n        result = boxcox_reverse(x_t, lam)\\n        np.testing.assert_almost_equal(result, expected_result)\",\n",
       "  25.0),\n",
       " ('def flip(tensor, horizontal=False, vertical=False):\\n    \\n    if horizontal == True:\\n        tensor = tensor.flip(-1)\\n    if vertical == True:\\n        tensor = tensor.flip(-2)\\n    return tensor',\n",
       "  'import sys\\nsys.path.append(\".\")\\nfrom source import flip  # Import the flip function from source.py\\nimport pytest\\n\\ndef test_flip():\\n    tensor = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]  # A 2D tensor\\n    assert flip(tensor) == [[7, 8, 9], [4, 5, 6], [1, 2, 3]], \"Test failed on horizontal flip\"\\n    assert flip(tensor, horizontal=True) == [[7, 8, 9], [4, 5, 6], [1, 2, 3]], \"Test failed on default horizontal flip\"\\n    assert flip(tensor, vertical=True) == [[3, 2, 1], [6, 5, 4], [9, 8, 7]], \"Test failed on vertical flip\"',\n",
       "  67.0),\n",
       " ('def bracket_monotonic(f, x0=0.0, x1=1.0, factor=2.0):\\n    \\n    assert abs(x1 - x0) > 0\\n    assert factor > 1.0\\n    f0 = f(x0)\\n    f1 = f(x1)\\n    if f1 < f0:\\n        x0, x1 = x1, x0\\n        f0, f1 = f1, f0\\n    while f0*f1 >= 0:\\n        x0, x1 = x1, x0 - factor*(x1-x0)\\n        f0, f1 = f1, f(x1)\\n    return (x0, x1)',\n",
       "  \"import sys\\nsys.path.insert(0, './')\\nimport source  # Assuming the source code file is in the same directory\\n\\ndef test_bracket_monotonic():\\n    assert source.bracket_monotonic(lambda x: x) == (0.0, 1.0)\\n    assert source.bracket_monotonic(lambda x: -x) == (1.0, 0.0)\\n    assert source.bracket_monotonic(lambda x: x**2) == (0.0, 1.0)\\n    assert source.bracket_monotonic(lambda x: 2*x**2-1) == (0.0, 1.0)\\n    assert source.bracket_monotonic(lambda x: 2*x**2+1) == (1.0, 2.0)\\n    assert source.bracket_monotonic(lambda x: x**3) == (0.0, 1.0)\",\n",
       "  83.0),\n",
       " ('def calc_bpm(beat_interval, sampling_freq):\\n    \\n    return 60 * sampling_freq * (1 / (beat_interval))',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")\\nimport source  # assuming source.py is in the same directory\\n\\ndef test_calc_bpm():\\n    sampling_freq = 1000\\n    beat_interval = 500\\n    expected_bpm = 60 * sampling_freq * (1 / (beat_interval))\\n    assert source.calc_bpm(beat_interval, sampling_freq) == expected_bpm',\n",
       "  100.0),\n",
       " ('def img_crop(img, size_crop, min_w=0, min_h=0):\\n    \\n    assert isinstance(size_crop, tuple) and len(size_crop) == 2\\n    assert isinstance(min_w, int) and isinstance(min_h, int)\\n    assert min_w >= 0 & min_h >= 0\\n\\n    cropped_width, cropped_height = size_crop\\n    max_w = min_w + cropped_width - 1\\n    max_h = min_h + cropped_height - 1\\n    img_h, img_w = img.shape[:2]\\n    assert max_h <= img_h and max_w <= img_w\\n\\n    cropped_img = img[min_h:(max_h + 1), min_w:(max_w + 1), ...]\\n    return cropped_img',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import img_crop\\n\\n\\ndef test_img_crop_type_checks():\\n    with pytest.raises(TypeError):\\n        img_crop(np.array([1, 2, 3]), \"wrong type\")\\n    with pytest.raises(TypeError):\\n        img_crop(np.array([1, 2, 3]), (3, 3), min_w=\"wrong type\")\\n    with pytest.raises(TypeError):\\n        img_crop(np.array([1, 2, 3]), (3, 3), min_h=\"wrong type\")\\n\\n\\ndef test_img_crop_value_checks():\\n    with pytest.raises(ValueError):\\n        img_crop(np.array([1, 2, 3]), (3, 3), min_w=5, min_h=6)\\n\\n\\ndef test_img_crop_shape_checks():\\n    img = np.zeros((10, 10))\\n    with pytest.raises(ValueError):\\n        img_crop(img, (3, 3), min_w=2, min_h=2)\\n\\n\\ndef test_img_crop():\\n    img = np.zeros((10, 10))\\n    cropped_img = img_crop(img, (3, 3), min_w=2, min_h=2)\\n    assert cropped_img.shape == (3, 3)',\n",
       "  100.0),\n",
       " ('def reorder_columns(df, order_dict=None):\\n    \\n    if order_dict is not None:\\n        order_tuples = list(order_dict.items())\\n        order_tuples = sorted(order_tuples, key=lambda x: x[1])\\n        order_names = [x[0] for x in order_tuples]\\n        df = df[order_names]\\n    return df',\n",
       "  \"import pandas as pd\\nimport pandas.testing as pdt\\nfrom source import reorder_columns\\n\\ndef test_reorder_columns():\\n    df = pd.DataFrame({\\n        'A': [1, 2, 3],\\n        'B': ['a', 'b', 'c'],\\n        'C': ['x', 'y', 'z']\\n    })\\n\\n    order_dict = {'B': 1, 'C': 2, 'A': 3}\\n\\n    expected = reorder_columns(df, order_dict)\\n\\n    assert_frame_equal(expected, pd.DataFrame({\\n        'B': ['a', 'b', 'c'],\\n        'C': ['x', 'y', 'z'],\\n        'A': [1, 2, 3]\\n    }))\",\n",
       "  100.0),\n",
       " (\"def compute_padding(J_pad, T):\\n    \\n    T_pad = 2**J_pad\\n    if T_pad < T:\\n        raise ValueError('Padding support should be larger than the original' +\\n                         'signal size!')\\n    to_add = 2**J_pad - T\\n    pad_left = to_add // 2\\n    pad_right = to_add - pad_left\\n    if max(pad_left, pad_right) >= T:\\n        raise ValueError('Too large padding value, will lead to NaN errors')\\n    return pad_left, pad_right\",\n",
       "  'import sys\\nsys.path.insert(0, \\'..\\')\\nimport pytest\\nfrom source import compute_padding\\n\\ndef test_compute_padding():\\n    with pytest.raises(ValueError) as e_info:\\n        compute_padding(3, 2)\\n    assert str(e_info.value) == \\'Padding support should be larger than the original\\\\n                         \\\\\\'signal size!\"\\'\\n    with pytest.raises(ValueError) as e_info:\\n        compute_padding(3, 10)\\n    assert str(e_info.value) == \\'Too large padding value, will lead to NaN errors\\'',\n",
       "  80.0),\n",
       " (\"def compute_padding(J_pad, T):\\n    \\n    T_pad = 2**J_pad\\n    if T_pad < T:\\n        raise ValueError('Padding support should be larger than the original' +\\n                         'signal size!')\\n    to_add = 2**J_pad - T\\n    pad_left = to_add // 2\\n    pad_right = to_add - pad_left\\n    if max(pad_left, pad_right) >= T:\\n        raise ValueError('Too large padding value, will lead to NaN errors')\\n    return pad_left, pad_right\",\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '../'))\\nfrom source import compute_padding\\n\\ndef test_compute_padding_basic():\\n    assert compute_padding(2, 10) == (5, 5)\\n\\ndef test_compute_padding_values():\\n    assert compute_padding(3, 10) == (4, 4)\\n\\ndef test_compute_padding_error():\\n    with pytest.raises(ValueError):\\n        compute_padding(2, 100)\\n\\ndef test_compute_padding_error_2():\\n    with pytest.raises(ValueError):\\n        compute_padding(3, 8)\",\n",
       "  90.0),\n",
       " (\"def _get_constants(planet='earth'):\\n    \\n\\n    radius = {'earth': 6371.2}\\n    mass = {'earth': 5.9722e24}\\n    gravity = 6.6743e-11\\n\\n    if planet not in radius.keys():\\n        raise KeyError('{:} is not yet a supported planet!'.format(planet))\\n\\n    return radius[planet], mass[planet], gravity\",\n",
       "  \"import pytest\\nfrom source import _get_constants\\n\\ndef test_get_constants_earth():\\n    radius, mass, gravity = _get_constants('earth')\\n    assert radius == 6371.2\\n    assert mass == 5.9722e+24\\n    assert gravity == 6.6743e-11\\n\\ndef test_get_constants_invalid_planet():\\n    with pytest.raises(KeyError):\\n        _get_constants('pluto')\",\n",
       "  100.0),\n",
       " (\"def get_metric(mod, obs, fun, dim='time', verbose=False):\\n    \\n\\n    return fun(mod, obs, dim)\",\n",
       "  \"import pytest\\n\\nimport source  # noqa: F401\\n\\n\\ndef test_get_metric():\\n    mod = 'source_module'\\n    obs = 'observed_data'\\n    fun = 'function_name'\\n    assert get_metric(mod, obs, fun) == 'expected_result'\",\n",
       "  50.0),\n",
       " (\"def pandas_filter_rows(df, col = 'Score', min_t= None, max_t = None):\\n    \\n    col_type = str(df.dtypes[col])\\n    if not ('float' in col_type or 'int' in col_type):\\n        raise TypeError('Column type should be either int or float')\\n        \\n    if min_t:\\n        df = df.loc[df[col] >= min_t]\\n    \\n    if max_t:\\n        df = df.loc[df[col] <= max_t]\\n    return df\",\n",
       "  'import pandas as pd\\nimport os\\nimport sys\\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\nfrom source import pandas_filter_rows\\n\\ndef test_pandas_filter_rows():\\n    df = pd.DataFrame({\\'Name\\': [\\'Alex\\', \\'Beth\\', \\'Charles\\', \\'David\\'],\\n                       \\'Score\\': [12, 15, 18, 20]})\\n    result = pandas_filter_rows(df, \\'Score\\', 10, 20)\\n    assert result.equals(df[df[\\'Score\\'] >= 10]), \"Test case 1 failed\"\\n\\ndf1 = pd.DataFrame({\\'Name\\': [\\'Alex\\', \\'Beth\\', \\'Charles\\', \\'David\\'],\\n                       \\'Score\\': [12, 15, 18, 20]})\\n\\ndef test_pandas_filter_rows_no_min_max():\\n    result = pandas_filter_rows(df1, \\'Score\\')\\n    assert result.equals(df1), \"Test case 2 failed\"\\n\\ndf2 = pd.DataFrame({\\'Name\\': [\\'Alex\\', \\'Beth\\', \\'Charles\\', \\'David\\'],\\n                       \\'Score\\': [12, 15, 18, 20]})\\n\\ndef test_pandas_filter_rows_non_numeric_column():\\n    try:\\n        pandas_filter_rows(df2, \\'Name\\', 10, 20)\\n    except TypeError as e:\\n        assert str(e) == \\'Column type should be either int or float\\', \"Test case 3 failed\"',\n",
       "  100.0),\n",
       " ('def calc_moving_average(data, func, named_arg, period):\\n    \\n    return (\\n        data\\n        .pipe(\\n            func=func, \\n            **{named_arg: period}\\n        )\\n        .mean()\\n    )',\n",
       "  \"import sys\\nsys.path.append('.')\\nfrom source import calc_moving_average\\nimport pytest\\nimport pandas as pd\\n\\ndef test_calc_moving_average_with_mean():\\n    data = pd.Series([1, 2, 3, 4, 5])\\n    func = 'mean'\\n    named_arg = 'period'\\n    period = 2\\n    expected_result = data.mean()\\n    result = calc_moving_average(data, func, named_arg, period)\\n    assert result == expected_result\\n\\ndef test_calc_moving_average_with_median():\\n    data = pd.Series([1, 2, 3, 4, 5])\\n    func = 'median'\\n    named_arg = 'period'\\n    period = 2\\n    expected_result = data.median()\\n    result = calc_moving_average(data, func, named_arg, period)\\n    assert result == expected_result\\n\\ndef test_calc_moving_average_with_sum():\\n    data = pd.Series([1, 2, 3, 4, 5])\\n    func = 'sum'\\n    named_arg = 'period'\\n    period = 2\\n    expected_result = data.sum()\\n    result = calc_moving_average(data, func, named_arg, period)\\n    assert result == expected_result\",\n",
       "  100.0),\n",
       " ('def _calc_phi(time, params):\\n    \\n\\n    T0 = params.T0\\n    per = params.per\\n\\n    return ((time - T0) % per)/per',\n",
       "  'import pytest\\nimport os\\nimport source \\n\\n# Importing the source code\\ncurrent_dir = os.path.dirname(__file__)\\nspec_file = os.path.join(current_dir, \\'source.py\\')\\nspec_module = __import__(spec_file)\\n\\nclass TestSource:\\n\\n    def test_calc_phi(self):\\n        # Testing the _calc_phi function\\n        params = source.Params()  # Assume Params is a class in source.py\\n        params.T0 = 100\\n        params.per = 200\\n        assert spec_module._calc_phi(150, params) == 0.5, \"Failed on _calc_phi test\"',\n",
       "  25.0),\n",
       " ('def _cal_cutoff_position(length, thickness, keyIdx):\\n  \\n  left_block = (thickness - 1) // 2\\n  right_block = thickness - 1 - left_block\\n\\n  start = max(0, keyIdx - left_block)\\n  end = min(length - 1, keyIdx + right_block)\\n  return start, end',\n",
       "  'import pytest\\nimport os\\nimport source  # the file with the function to test\\n\\ndef test_cal_cutoff_position():\\n    # assuming that the function takes three arguments: length, thickness, keyIdx\\n    # and returns a tuple (start, end)\\n    # we will test the function with some random values\\n    length = 10\\n    thickness = 3\\n    keyIdx = 5\\n\\n    # call the function and save the result\\n    result = source._cal_cutoff_position(length, thickness, keyIdx)\\n\\n    # check if the returned result is correct\\n    assert result == (2, 7), \"The function didn\\'t return the expected result\"',\n",
       "  100.0),\n",
       " ('def degrees_centered_at_zero(degrees):\\n    \\n    if degrees > 180:\\n        degrees = degrees - 360\\n    elif degrees < -180:\\n        degrees = degrees + 360\\n    elif degrees == -180:\\n        degrees = 180\\n    return degrees',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.') # To import the module from the same directory\\nfrom source import degrees_centered_at_zero\\n\\ndef test_degrees_centered_at_zero():\\n    assert degrees_centered_at_zero(180) == 0\",\n",
       "  62.0),\n",
       " (\"def smd(valid, x_name, yhat_name, reference, protected):\\n    \\n\\n    # yhat mean for j=reference\\n    reference_yhat_mean = valid[valid[x_name] == reference][yhat_name].mean()\\n    print(reference.title() + ' mean yhat: %.2f' % reference_yhat_mean)\\n\\n    # yhat mean for j=protected\\n    protected_yhat_mean = valid[valid[x_name] == protected][yhat_name].mean()\\n    print(protected.title() + ' mean yhat: %.2f' % protected_yhat_mean)\\n\\n    # std for yhat\\n    sigma = valid[yhat_name].std()\\n    print(yhat_name.title() + ' std. dev.:  %.2f' % sigma)\\n\\n    return (protected_yhat_mean - reference_yhat_mean) / sigma\",\n",
       "  \"from source import smd\\nimport pandas as pd\\nimport numpy as np\\n\\n# test data generation\\nnp.random.seed(0)\\nvalid = pd.DataFrame({\\n    'j': ['reference', 'protected', 'reference', 'protected'],\\n    'x': [20, 20, 40, 40],\\n    'yhat': [25, 25, 50, 50],\\n})\\n\\n# test case 1: compare reference and protected groups\\ndef test_smd_reference_protected():\\n    x_name = 'x'\\n    yhat_name = 'yhat'\\n    reference = 'reference'\\n    protected = 'protected'\\n\\n    result = smd(valid, x_name, yhat_name, reference, protected)\\n\\n    # assertion\\n    assert np.isclose(result, 0.0, atol=1e-2)\\n        \\n# test case 2: compare different groups\\ndef test_smd_different_groups():\\n    x_name = 'x'\\n    yhat_name = 'yhat'\\n    reference = 'reference'\\n    protected = 'protected'\\n\\n    result = smd(valid, x_name, yhat_name, reference, 'different')\\n\\n    # assertion\\n    assert np.isclose(result, 1.0, atol=1e-2)\",\n",
       "  100.0),\n",
       " ('def argmin(values):\\n    \\n    return min(range(len(values)), key=lambda i: values[i])',\n",
       "  'from source import *\\nimport source\\n\\ndef test_argmin():\\n    values = [4, 2, 9, 6, 5]\\n    assert source.argmin(values) == 1',\n",
       "  100.0),\n",
       " ('def unflatten(tensor, batch_size, num_rounds):\\n    \\n    old_size = tensor.shape\\n    expected_first_dim = batch_size * num_rounds\\n    assert old_size[0] == expected_first_dim, \"Expected dim 0 as \" \"{}\".format(\\n        expected_first_dim\\n    )\\n    new_size = (batch_size, num_rounds) + old_size[1:]\\n    unflat_tensor = tensor.reshape(new_size)\\n    return unflat_tensor',\n",
       "  \"from source import *\\nimport pytest\\nfrom source import unflatten\\nimport numpy as np\\n\\ndef test_unflatten():\\n    tensor = np.random.rand(10, 2, 3)\\n    batch_size = 5\\n    num_rounds = 2\\n    unflat_tensor = unflatten(tensor, batch_size, num_rounds)\\n    expected_shape = (batch_size, num_rounds) + tensor.shape[1:]\\n    assert unflat_tensor.shape == expected_shape, 'The reshaping operation failed'\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef drop_path(x, drop_prob=0.0, training=False):\\n    \\n    if drop_prob == 0.0 or not training:\\n        return x\\n    keep_prob = 1 - drop_prob\\n    # work with diff dim tensors, not just 2D ConvNets\\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\\n    random_tensor.floor_()  # binarize\\n    output = x.div(keep_prob) * random_tensor\\n    return output',\n",
       "  'import torch\\nimport pytest\\nfrom source import drop_path  # Assuming the function is defined in source.py\\n\\ndef test_drop_path_no_drop():\\n    x = torch.randn(1, 2, 3, 4)  # Create a random 4D tensor as input\\n    result = drop_path(x, drop_prob=0.0, training=True)\\n    assert torch.allclose(result, x), \"The output should be the same as the input when drop_prob is 0.0\"\\n\\ndef test_drop_path_with_drop():\\n    x = torch.randn(1, 2, 3, 4)  # Create a random 4D tensor as input\\n    result = drop_path(x, drop_prob=0.5, training=True)\\n    assert not torch.allclose(result, x), \"The output should not be the same as the input when drop_prob is 0.5\"\\n\\ndef test_drop_path_not_training():\\n    x = torch.randn(1, 2, 3, 4)  # Create a random 4D tensor as input\\n    result = drop_path(x, drop_prob=0.5, training=False)\\n    assert torch.allclose(result, x), \"The output should be the same as the input when not training\"',\n",
       "  100.0),\n",
       " ('def open_move_score(game, player):\\n    \\n    if game.is_loser(player):\\n        return float(\"-inf\")\\n\\n    if game.is_winner(player):\\n        return float(\"inf\")\\n\\n    return float(len(game.get_legal_moves(player)))',\n",
       "  '# test_source.py\\n\\nimport pytest\\nfrom source import Game, open_move_score\\n\\ndef test_open_move_score():\\n    game = Game()\\n    player = \"player1\"\\n\\n    # Test if the function returnsinf when the player is the winner\\n    assert open_move_score(game, player) == float(\"inf\") \\n\\n    # Test if the function returns 0 when the game is a draw\\n    game.set_winner(None)\\n    assert open_move_score(game, player) == 0  \\n\\n    # Test if the function returns -inf when the player is the loser\\n    game.set_winner(player)\\n    assert open_move_score(game, player) == float(\"-inf\") \\n\\n    # Test if the function returns number of legal moves when the game is not over\\n    game.set_winner(None)\\n    assert open_move_score(game, player) == len(game.get_legal_moves(player))',\n",
       "  17.0),\n",
       " ('def DRate_j(eta,Pap,Pec,exp_loss_jt):\\n    \\n    return (1 + Pap)*(1 - (1 - 2*Pec)*exp_loss_jt)',\n",
       "  'def test_DRate_j():\\n    assert source.DRate_j(0, 0, 0, 0) == 1\\n    assert source.DRate_j(1, 1, 1, 1) == 0\\n    assert source.DRate_j(0.5, 0.5, 0.5, 0.5) == 0.5\\n    assert source.DRate_j(0.25, 0.25, 0.25, 0.25) == 0.75\\n    assert source.DRate_j(1, 0, 0, 1) == 0.5',\n",
       "  0.0),\n",
       " ('def linear_annuity_mapping_fprime(underlying, alpha0, alpha1):\\n    \\n    return alpha0',\n",
       "  '# test_source.py\\n\\nimport pytest\\nfrom source import linear_annuity_mapping_fprime\\n\\ndef test_linear_annuity_mapping_fprime():\\n    result = linear_annuity_mapping_fprime(100, 0.1, 0.2)\\n    assert result == 0.1, \"The function did not return the expected value\"',\n",
       "  100.0),\n",
       " ('def is_iterable(obj):\\n    \\n    try:\\n        iter(obj)\\n    except TypeError:\\n        return False\\n    return True',\n",
       "  'import pytest\\nimport sys\\nsys.path.insert(0, \\'..\\') # To import the source.py file from the parent directory\\nfrom source import is_iterable\\n\\ndef test_is_iterable():\\n    assert is_iterable([1, 2, 3]) == True, \"List is not iterable\"\\n    assert is_iterable(1) == False, \"Integer is iterable\"\\n    assert is_iterable(\"Hello\") == False, \"String is iterable\"\\n    assert is_iterable(None) == False, \"None is iterable\"',\n",
       "  100.0),\n",
       " ('def inverse(A=None, name=None, attr=None, out=None, **kwargs):\\n    r\\n    return (0,)',\n",
       "  'import pytest\\nfrom source import inverse\\n\\ndef test_inverse():\\n    result = inverse()\\n    assert result == (0,), \"The function did not return the expected result\"',\n",
       "  67.0),\n",
       " ('def _update_dtypes(candidate_model):\\n    \\n    columns_to_int = [\\n        \"left_child\",\\n        \"right_child\",\\n        \"level\",\\n        \"split_feat\",\\n    ]\\n\\n    fitted_model = candidate_model.copy()\\n    fitted_model[columns_to_int] = fitted_model[columns_to_int].astype(\"Int64\")\\n\\n    return fitted_model',\n",
       "  'from source import *\\nimport pytest\\nwith pytest.raises(NameError):\\n    with pytest.raises(NameError):\\n    from hypothesis import given, strategies as st\\nimport pandas as pd\\n\\ndef test_update_dtypes():\\n\\n    @given(st.data())\\n    def inner_test(data):\\n        columns_to_int = data.draw(st.lists(st.text(), min_size=1))\\n        candidate_model = data.draw(st.dataframes(columns=st.sampled_from([\\'left_child\\', \\'right_child\\', \\'level\\', \\'split_feat\\']), index=st.integers()))\\n        fitted_model = _update_dtypes(candidate_model)\\n        assert isinstance(fitted_model, pd.DataFrame), \\'The function should return a dataframe\\'\\n        assert set(fitted_model.columns).issubset(set([\\'left_child\\', \\'right_child\\', \\'level\\', \\'split_feat\\', \\'Int64\\'])), \"The function should return a dataframe with the specified columns and a new \\'Int64\\' column\"\\n    inner_test()',\n",
       "  0.0),\n",
       " (\"def last_dimension(shape, min_rank=1):\\n  \\n  dims = shape.dims\\n  if dims is None:\\n    raise ValueError('dims of shape must be known but is None')\\n  if len(dims) < min_rank:\\n    raise ValueError('rank of shape must be at least %d not: %d' % (min_rank,\\n                                                                    len(dims)))\\n  value = dims[-1].value\\n  if value is None:\\n    raise ValueError('last dimension shape must be known but is None')\\n  return value\",\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')\\nfrom source import last_dimension\\n\\ndef test_last_dimension():\\n    shape = [10, 20, None, 30]\\n    with pytest.raises(ValueError) as excinfo:\\n        last_dimension(shape, min_rank=2)\\n    assert 'rank of shape must be at least 2 not: 1' in str(excinfo.value)\\n    shape = [10, 20, 30]\\n    with pytest.raises(ValueError) as excinfo:\\n        last_dimension(shape, min_rank=3)\\n    assert 'dims of shape must be known but is None' in str(excinfo.value)\\n    shape = [10, 20, 30, None]\\n    with pytest.raises(ValueError) as excinfo:\\n        last_dimension(shape, min_rank=1)\\n    assert 'last dimension shape must be known but is None' in str(excinfo.value)\\n    shape = [10, 20, 30]\\n    assert last_dimension(shape, min_rank=1) == 30\",\n",
       "  20.0),\n",
       " ('def is_scheduled(num_words, frequency, words_per_iteration):\\n    \\n\\n    modulo = num_words * frequency % words_per_iteration\\n    return modulo < frequency',\n",
       "  \"# This is the code to be tested\\nfrom source import is_scheduled  \\n\\n# Pytest runs the test script in a different process so it can import the 'source' module\\n# If the 'source' module is not in the same directory as the test script you might need to do a sys.path.append()\\n\\ndef test_is_scheduled():\\n    assert is_scheduled(10,2,3) == True\\n    assert is_scheduled(10,2,10) == False\\n    assert is_scheduled(20,4,5) == True\\n    assert is_scheduled(20,4,20) == False\",\n",
       "  100.0),\n",
       " (\"def compute_padding(J_pad, T):\\n    \\n    T_pad = 2**J_pad\\n    if T_pad < T:\\n        raise ValueError('Padding support should be larger than the original' +\\n                         'signal size!')\\n    to_add = 2**J_pad - T\\n    pad_left = to_add // 2\\n    pad_right = to_add - pad_left\\n    if max(pad_left, pad_right) >= T:\\n        raise ValueError('Too large padding value, will lead to NaN errors')\\n    return pad_left, pad_right\",\n",
       "  'import pytest\\nfrom source import compute_padding\\n\\nclass TestComputePadding:\\n    def test_compute_padding(self):\\n        with pytest.raises(ValueError):\\n            compute_padding(3, 8)\\n\\n    def test_compute_padding_2(self):\\n        assert compute_padding(4, 16) == (0, 15)\\n\\n    def test_compute_padding_3(self):\\n        assert compute_padding(5, 32) == (7, 25)',\n",
       "  80.0),\n",
       " ('import torch\\n\\ndef gnmt_length_penalty(lengths, alpha=0.8):\\n    \\n    lengths = lengths.to(torch.float)\\n    penalty = torch.pow(((5 + lengths) / 6), alpha)\\n    return penalty.unsqueeze(-1)',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport torch\\nfrom source import gnmt_length_penalty   # Importing the function from source.py\\n\\ndef test_gnmt_length_penalty():\\n    # Testing the function with random tensor\\n    lengths = torch.randint(low=1, high=10, size=(10,))\\n    result = gnmt_length_penalty(lengths)\\n    assert result.shape == torch.Size([10, 1]), \"Shape of the output is not as expected\"\\n\\n    # Testing the function with alpha argument\\n    result_alpha = gnmt_length_penalty(lengths, alpha=1.0)\\n    assert not torch.allclose(result, result_alpha), \"Function does not behave as expected with alpha=1.0\"\\n\\n    # Testing the function with high alpha\\n    result_high_alpha = gnmt_length_penalty(lengths, alpha=2.0)\\n    assert not torch.allclose(result_high_alpha, result), \"Function does not behave as expected with high alpha\"',\n",
       "  100.0),\n",
       " (\"def get_spacing(network):\\n    r\\n    from openpnm.topotools.generators.tools import get_spacing\\n    d = {'vert.coords': network.coords, 'edge.conns': network.conns}\\n    spc = get_spacing(d)\\n    return spc\",\n",
       "  \"# test_source.py\\nimport pytest\\nfrom source import get_spacing\\nfrom openpnm.topotools.generators.tools import get_spacing as _get_spacing\\n\\ndef test_get_spacing():\\n    # Assuming a Network object is available\\n    network = ...\\n    d = {'vert.coords': network.coords, 'edge.conns': network.conns}\\n    spc = get_spacing(d)\\n    assert _get_spacing(d) == spc\",\n",
       "  17.0),\n",
       " (\"def scale_voltage_current_power(data, voltage=1, current=1):\\n    \\n\\n    # as written, only works with a DataFrame\\n    # could make it work with a dict, but it would be more verbose\\n    data = data.copy()\\n    voltages = ['v_mp', 'v_oc']\\n    currents = ['i_mp', 'i_x', 'i_xx', 'i_sc']\\n    data[voltages] *= voltage\\n    data[currents] *= current\\n    data['p_mp'] *= voltage * current\\n\\n    return data\",\n",
       "  \"# test_scale_voltage_current_power.py\\nimport pytest\\nfrom source import scale_voltage_current_power\\nimport pandas as pd\\n\\ndef test_scale_voltage_current_power():\\n    # Let's start by creating a DataFrame with some test data\\n    # We'll use made-up values for this example\\n    data = pd.DataFrame({\\n        'v_mp': [1, 2, 3],\\n        'v_oc': [10, 20, 30],\\n        'i_mp': [0.1, 0.2, 0.3],\\n        'i_x': [0.01, 0.02, 0.03],\\n        'i_xx': [0.001, 0.002, 0.003],\\n        'i_sc': [0.0001, 0.0002, 0.0003],\\n        'p_mp': [1, 2, 3]\\n    })\\n\\n    # apply the function under test\\n    scaled_data = scale_voltage_current_power(data, voltage=2, current=0.5)\\n\\n    # Write a single assertion to verify that the function is working as expected\\n    # Here, we're just checking that the 'v_mp' and 'v_oc' columns have been multiplied by 2,\\n    # and the 'i_mp', 'i_x', 'i_xx', 'i_sc' columns have been multiplied by 0.5,\\n    # and that 'p_mp' has been multiplied by 1 (since voltage and current are the same)\\n    assert (scaled_data[voltages] == data[voltages] * 2).all()\\n    assert (scaled_data[currents] == data[currents] * 0.5).all()\\n    assert (scaled_data['p_mp'] == data['p_mp'] * 2).all()\",\n",
       "  100.0),\n",
       " (\"def post_process(out, problem, state, extend=False):\\n    \\n    from sfepy.base.base import Struct\\n\\n    # Cauchy strain averaged in elements.\\n    strain = problem.evaluate('de_cauchy_strain.i1.Omega( u )')\\n    out['cauchy_strain'] = Struct(name='output_data',\\n                                  mode='cell', data=strain,\\n                                  dofs=None)\\n    # Cauchy stress averaged in elements.\\n    stress = problem.evaluate('de_cauchy_stress.i1.Omega( solid.D, u )')\\n    out['cauchy_stress'] = Struct(name='output_data',\\n                                  mode='cell', data=stress,\\n                                  dofs=None)\\n    \\n    return out\",\n",
       "  \"from source import *\\nimport pytest\\nimport unittest\\nfrom source import post_process\\n\\nclass TestPostProcessFunction(unittest.TestCase):\\n\\n    def test_post_process_function(self):\\n\\n        def mock_evaluate(arg):\\n            if arg == 'de_cauchy_strain.i1.Omega( u )':\\n                return 'cauchy_strain_data'\\n            elif arg == 'de_cauchy_stress.i1.Omega( solid.D, u )':\\n                return 'cauchy_stress_data'\\n\\n        class MockProblem:\\n\\n            def __init__(self):\\n                self.D = 'mock_D_data'\\n                self.evaluate = mock_evaluate\\n        problem = MockProblem()\\n        extend = False\\n        out = post_process(None, problem, extend)\\n        assert 'cauchy_strain' in out\\n        assert 'cauchy_stress' in out\\n        assert out['cauchy_strain'].data == 'cauchy_strain_data'\\n        assert out['cauchy_stress'].data == 'cauchy_stress_data'\\n        assert out['cauchy_strain'].mode == 'cell'\\n        assert out['cauchy_stress'].mode == 'cell'\\n        assert out['cauchy_strain'].dofs is None\\n        assert out['cauchy_stress'].dofs is None\",\n",
       "  29.0),\n",
       " ('import torch\\n\\ndef xy_wh_r_2_xy_sigma(xywhr):\\n    \\n    _shape = xywhr.shape\\n    assert _shape[-1] == 5\\n    xy = xywhr[..., :2]\\n    wh = xywhr[..., 2:4].clamp(min=1e-7, max=1e7).reshape(-1, 2)\\n    r = xywhr[..., 4]\\n    cos_r = torch.cos(r)\\n    sin_r = torch.sin(r)\\n    R = torch.stack((cos_r, -sin_r, sin_r, cos_r), dim=-1).reshape(-1, 2, 2)\\n    S = 0.5 * torch.diag_embed(wh)\\n\\n    sigma = R.bmm(S.square()).bmm(R.permute(0, 2,\\n                                            1)).reshape(_shape[:-1] + (2, 2))\\n\\n    return xy, sigma',\n",
       "  'import pytest\\nimport torch\\n\\ndef test_xy_wh_r_2_xy_sigma():\\n    xywhr = torch.tensor([[[1, 2, 3, 4, 5]]])  # example input\\n    xy, sigma = xy_wh_r_2_xy_sigma(xywhr)  # function call\\n    assert xy.shape == (1, 2)  # assert the shape of xy\\n    assert sigma.shape == (1, 2, 2)  # assert the shape of sigma',\n",
       "  0.0),\n",
       " ('def reject(position_store, particles, random_particle):\\n    \\n    particles[\"xposition\"][random_particle] = position_store[0]\\n    particles[\"yposition\"][random_particle] = position_store[1]\\n    return particles',\n",
       "  '# We need to import the source.py file to use its functionality\\nfrom source import reject\\n\\n# Import the pytest library to use its functionality\\nimport pytest\\n\\n# Here is the test class. Pytest uses the \\'test_\\' prefix to identify tests\\nclass TestReject:\\n    \\n    # Initialize some particles and position_store for testing\\n    def setup_method(self):\\n        self.particles = {\"xposition\": [0, 0, 0], \"yposition\": [0, 0, 0]}\\n        self.position_store = [1, 2]\\n        self.random_particle = 1\\n\\n    # Test reject function with fixed input\\n    def test_reject_with_fixed_input(self):\\n        # Call the reject function with the fixed input\\n        result = reject(self.position_store, self.particles, self.random_particle)\\n        \\n        # Assert that the result is equal to the expected output\\n        assert result[\"xposition\"][self.random_particle] == self.position_store[0]\\n        assert result[\"yposition\"][self.random_particle] == self.position_store[1]',\n",
       "  100.0),\n",
       " ('def latlonfilter(df, llcrnr, urcrnr):\\n    \\n    lat1 = llcrnr[0]\\n    lat2 = urcrnr[0]\\n    lon1 = llcrnr[1]\\n    lon2 = urcrnr[1]\\n    df = df[df[\"latitude\"] < lat2]\\n    df = df[df[\"latitude\"] > lat1]\\n    df = df[df[\"longitude\"] > lon1]\\n    df = df[df[\"longitude\"] < lon2]\\n    return df',\n",
       "  'import pytest\\nfrom source import latlonfilter\\nfrom pandas import DataFrame\\n\\ndef test_latlonfilter():\\n    # create a dummy DataFrame\\n    df = DataFrame({\\n        \"latitude\": [1, 2, 3, 4, 5],\\n        \"longitude\": [1, 2, 3, 4, 5],\\n        })\\n\\n    # expected result after filtering\\n    expected_result = DataFrame({\\n        \"latitude\": [2, 3, 4],\\n        \"longitude\": [2, 3, 4],\\n        })\\n\\n    # lower left and upper right corners of the rectangle\\n    llcrnr = (1, 1)\\n    urcrnr = (5, 5)\\n\\n    assert latlonfilter(df, llcrnr, urcrnr).equals(expected_result)',\n",
       "  100.0),\n",
       " ('def matrix2cube(data_matrix, im_shape):\\n    \\n    return data_matrix.T.reshape([data_matrix.shape[1]] + list(im_shape))',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import matrix2cube # assuming the function is defined in source.py\\n\\ndef test_matrix2cube():\\n    data_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n    im_shape = (2, 3)\\n    expected_result = np.array([[1, 2, 3], [4, 5, 6]])\\n    result = matrix2cube(data_matrix, im_shape)\\n    assert np.array_equal(result, expected_result), \"The function did not return the expected result\"',\n",
       "  100.0),\n",
       " ('def depth2intensity(depth, interval=300):\\n    \\n    return depth * 3600 / interval',\n",
       "  \"# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append('.')  # this is to import source.py from the same directory\\nfrom source import depth2intensity\\n\\ndef test_depth2intensity_with_default_interval():\\n    assert depth2intensity(10) == 36000\\n\\ndef test_depth2intensity_with_custom_interval():\\n    assert depth2intensity(10, 200) == 18000\",\n",
       "  100.0),\n",
       " ('def estimate_phi(x):\\n    \\n\\n    phi = x[:,0:2].sum(axis=1).astype(float) / (x[:,2].astype(float) + 1.0)\\n\\n    return phi',\n",
       "  \"import pytest\\nimport numpy as np\\nimport source  # assuming that the source file is named 'source.py'\\n\\ndef test_estimate_phi():\\n    # Preparing a test case where the function should work correctly\\n    x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n    expected_output = np.array([0.5, 0.66666666, 0.75])\\n\\n    assert np.allclose(source.estimate_phi(x), expected_output)\",\n",
       "  100.0),\n",
       " ('def get_machine_periodicity(self):\\n    \\n    if (\\n        self.geo.per_a is None\\n        or self.geo.is_antiper_a is None\\n        or self.geo.per_t is None\\n        or self.geo.is_antiper_t is None\\n    ):\\n        (\\n            self.geo.per_a,\\n            self.geo.is_antiper_a,\\n            self.geo.per_t,\\n            self.geo.is_antiper_t,\\n        ) = self.simu.machine.comp_periodicity()\\n\\n    return (\\n        self.geo.per_a,\\n        self.geo.is_antiper_a,\\n        self.geo.per_t,\\n        self.geo.is_antiper_t,\\n    )',\n",
       "  '# source.py\\n\\nimport pytest\\nfrom .source import get_machine_periodicity\\n\\nclass TestSource:\\n\\n    def test_get_machine_periodicity(self):\\n        # You can initialize the necessary objects here before each test\\n        # ...\\n\\n        # Then call the method and make an assertion\\n        result = get_machine_periodicity(self)\\n        assert result is not None, \"The result should not be None\"',\n",
       "  0.0),\n",
       " ('def daily_solar_radiation_flat(ra_24_toa_flat, trans_24):\\n    r\\n    return ra_24_toa_flat * trans_24',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\nfrom source import daily_solar_radiation_flat\\n\\ndef test_daily_solar_radiation_flat():\\n    ra_24_toa_flat = 10  # Assuming some value for testing\\n    trans_24 = 5    # Assuming some value for testing\\n    assert daily_solar_radiation_flat(ra_24_toa_flat, trans_24) == 50\",\n",
       "  67.0),\n",
       " ('def get_mask_from_sequence_lengths(sequence_lengths, max_length=None):\\n    \\n    # (batch_size, max_length)\\n    ones = sequence_lengths.new_ones(sequence_lengths.size(0), max_length)\\n    range_tensor = ones.cumsum(dim=1)\\n    return (sequence_lengths.unsqueeze(1) >= range_tensor).long()',\n",
       "  'import pytest\\nimport torch\\n\\ndef test_get_mask_from_sequence_lengths():\\n    sequence_lengths = torch.tensor([5, 3, 7])\\n    result = get_mask_from_sequence_lengths(sequence_lengths)\\n    expected_output = torch.tensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1]])\\n    assert torch.all(result == expected_output)',\n",
       "  0.0),\n",
       " ('def create_image(deck, background=\\'black\\'):\\n    \\n    from PIL import Image\\n\\n    image_format = deck.key_image_format()\\n\\n    return Image.new(\"RGB\", image_format[\\'size\\'], background)',\n",
       "  \"import pytest\\nimport os\\nfrom PIL import Image\\nfrom source import create_image\\n\\nclass MockDeck:\\n\\n    def __init__(self):\\n        self.image_format = {'size': (800, 600)}\\n\\n    def key_image_format(self):\\n        return self.image_format\\n\\ndef test_create_image():\\n    deck = MockDeck()\\n    image = create_image(deck)\\n    assert isinstance(image, Image.Image)\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef one_hot(targets, num_classes, dtype=torch.float32, device=None):\\n    \\n    if device is None:\\n        device = targets.device\\n\\n    nb_samples = len(targets)\\n    if len(targets.shape) == 2:\\n        # 2D target (e.g., classification)\\n        encoded_shape = (nb_samples, num_classes)\\n    else:\\n        # N-d target (e.g., segmentation map)\\n        encoded_shape = tuple([nb_samples, num_classes] + list(targets.shape[1:]))\\n\\n    with torch.no_grad():\\n        encoded_target = torch.zeros(encoded_shape, dtype=dtype, device=device)\\n        encoded_target.scatter_(1, targets.unsqueeze(1), 1)\\n    return encoded_target',\n",
       "  \"import pytest\\nimport torch\\nfrom source import one_hot\\n\\ndef test_one_hot():\\n    # Test with 2D target\\n    targets = torch.tensor([0, 1, 2, 3], dtype=torch.long)\\n    num_classes = 4\\n    result = one_hot(targets, num_classes)\\n    expected = torch.tensor([[1.,0.,0.,0.],\\n                              [0.,1.,0.,0.],\\n                              [0.,0.,1.,0.],\\n                              [0.,0.,0.,1.]], dtype=torch.float32)\\n    assert torch.allclose(result, expected)\\n    \\n    # Test with N-d target\\n    targets_nd = torch.tensor([[0, 1, 2], [3, 2, 1]], dtype=torch.long)\\n    num_classes = 4\\n    result_nd = one_hot(targets_nd, num_classes)\\n    expected_nd = torch.tensor([[[1.,0.,0.,0.],\\n                                  [0.,1.,0.,0.],\\n                                  [0.,0.,1.,0.]],\\n                                 [[0.,0.,1.,0.],\\n                                  [0.,1.,0.,0.],\\n                                  [0.,0.,0.,1.]]], dtype=torch.float32)\\n    assert torch.allclose(result_nd, expected_nd)\\n\\n    # Test with device\\n    targets_device = torch.tensor([0, 1, 2, 3], dtype=torch.long).to('cuda')\\n    num_classes = 4\\n    result_device = one_hot(targets_device, num_classes)\\n    assert result_device.device.type == 'cuda'\\n\\ntest_one_hot()\",\n",
       "  100.0),\n",
       " ('def update(current, step, method):\\n    \\n    method = method.lower()\\n\\n    if method == \"linear\":\\n        return current - step\\n    elif method == \"exponential\":\\n        return current * step\\n\\n    raise ValueError(\"Method `{0}\\' not understood.\".format(method))',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import update\\n\\ndef test_update_linear():\\n    assert update(10, 2, \"linear\") == 8\\n\\ndef test_update_exponential():\\n    assert update(3, 2, \"exponential\") == 6\\n\\ndef test_update_invalid_method():\\n    with pytest.raises(ValueError):\\n        update(5, 3, \"invalid\")',\n",
       "  100.0),\n",
       " ('def open_move_score(game, player):\\n    \\n    if game.is_loser(player):\\n        return float(\"-inf\")\\n\\n    if game.is_winner(player):\\n        return float(\"inf\")\\n\\n    return float(len(game.get_legal_moves(player)))',\n",
       "  'import pytest\\nfrom source import open_move_score\\n\\ndef test_open_move_score():\\n    # Create a mock game object\\n    class MockGame:\\n        def __init__(self):\\n            self.moves = [\\'a\\', \\'b\\', \\'c\\']\\n        def is_loser(self, player):\\n            return player == \\'loser\\'\\n        def is_winner(self, player):\\n            return player == \\'winner\\'\\n        def get_legal_moves(self, player):\\n            return self.moves\\n\\n    # Test when the player is a loser\\n    game = MockGame()\\n    assert open_move_score(game, \\'loser\\') == float(\"-inf\")\\n    \\n    # Test when the player is a winner\\n    assert open_move_score(game, \\'winner\\') == float(\"inf\")\\n    \\n    # Test when the player has legal moves\\n    game.moves = [\\'a\\', \\'b\\']\\n    assert open_move_score(game, \\'player\\') == float(2)',\n",
       "  100.0),\n",
       " (\"def get_iou(gt_bbx, pred_bbx):\\n    \\n\\n    assert gt_bbx['xmin'] < gt_bbx['xmax']\\n    assert gt_bbx['ymin'] < gt_bbx['ymax']\\n    assert pred_bbx['xmin'] < pred_bbx['xmax']\\n    assert pred_bbx['ymin'] < pred_bbx['ymax']\\n\\n    # determine the coordinates of the intersection rectangle\\n    x_left = max(gt_bbx['xmin'], pred_bbx['xmin'])\\n    y_top = max(gt_bbx['ymin'], pred_bbx['ymin'])\\n    x_right = min(gt_bbx['xmax'], pred_bbx['xmax'])\\n    y_bottom = min(gt_bbx['ymax'], pred_bbx['ymax'])\\n\\n    if (x_right < x_left) or (y_bottom < y_top):\\n        iou = 0.0\\n        intersection_area = (x_right - x_left) * (y_bottom - y_top)\\n    else:\\n        # The intersection of two axis-aligned bounding boxes is always an\\n        # axis-aligned bounding box\\n        intersection_area = (x_right - x_left) * (y_bottom - y_top)\\n\\n        # compute the area of both BBs\\n        gt_bbx_area = (gt_bbx['xmax']-gt_bbx['xmin']) * \\\\\\n            (gt_bbx['ymax']-gt_bbx['ymin'])\\n        pred_bbx_area = (pred_bbx['xmax']-pred_bbx['xmin']) * \\\\\\n            (pred_bbx['ymax']-pred_bbx['ymin'])\\n\\n        # compute the intersection over union by taking the intersection\\n        # area and dividing it by the sum of prediction + ground-truth\\n        # areas - the interesection area\\n        iou = intersection_area / \\\\\\n            float(gt_bbx_area + pred_bbx_area - intersection_area)\\n        assert iou >= 0.0\\n        assert iou <= 1.0\\n    return iou, intersection_area\",\n",
       "  \"import pytest\\nfrom source import get_iou\\n\\ndef test_get_iou_assertions():\\n    gt_bbx = {'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}\\n    pred_bbx = {'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}\\n    \\n    with pytest.raises(AssertionError):\\n        get_iou(gt_bbx, pred_bbx)\\n\\n    gt_bbx = {'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}\\n    pred_bbx = {'xmin': 2, 'ymin': 3, 'xmax': 4, 'ymax': 5}\\n    \\n    with pytest.raises(AssertionError):\\n        get_iou(gt_bbx, pred_bbx)\\n\\n    gt_bbx = {'xmin': 2, 'ymin': 3, 'xmax': 4, 'ymax': 5}\\n    pred_bbx = {'xmin': 1, 'ymin': 2, 'xmax': 3, 'ymax': 4}\\n    \\n    with pytest.raises(AssertionError):\\n        get_iou(gt_bbx, pred_bbx)\\n\\n    gt_bbx = {'xmin': 2, 'ymin': 3, 'xmax': 5, 'ymax': 6}\\n    pred_bbx = {'xmin': 1, 'ymin': 2, 'xmax': 4, 'ymax': 5}\\n    \\n    get_iou(gt_bbx, pred_bbx)  # This should pass\",\n",
       "  89.0),\n",
       " ('def prior_probability_of_category_k(k, c, nk):\\n    \\n    n = sum(nk)\\n    return (c * nk[k]) / (1 - c + c * n)',\n",
       "  'import pytest\\nfrom source import prior_probability_of_category_k\\n\\ndef test_prior_probability_of_category_k():\\n    k = 0\\n    c = 0.5\\n    nk = [50, 50]\\n    assert prior_probability_of_category_k(k, c, nk) == 0.49504950495049505',\n",
       "  100.0),\n",
       " ('def relative_intensity(wap, threshold_power):\\n    \\n\\n    rv = wap / threshold_power\\n\\n    return rv',\n",
       "  '# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append(\\'.\\') # adding current directory to the path\\nfrom source import relative_intensity\\n\\ndef test_relative_intensity_positive():\\n    assert relative_intensity(10, 5) == 2.0, \"Should return 2.0 when input values are 10 and 5\"\\n\\ndef test_relative_intensity_negative():\\n    assert relative_intensity(-10, 5) == -2.0, \"Should return -2.0 when input values are -10 and 5\"\\n\\ndef test_relative_intensity_zero():\\n    assert relative_intensity(0, 5) == 0.0, \"Should return 0.0 when input values are 0 and any number\"\\n\\ndef test_relative_intensity_equal():\\n    assert relative_intensity(5, 5) == 1.0, \"Should return 1.0 when input values are same i.e., 5 and 5\"',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef find_nearest_prototypes(embeddings, prototypes):\\n  \\n  embeddings = embeddings.view(-1, prototypes.shape[-1])\\n  similarities = torch.mm(embeddings, prototypes.t())\\n\\n  return torch.argmax(similarities, 1)',\n",
       "  '# test_source.py\\nimport torch\\nimport sys\\nsys.path.append(\".\")  # to import source.py from the same directory\\nimport source  # import the source file\\n\\ndef test_find_nearest_prototypes():\\n  # Arrange\\n  embeddings = torch.randn(10, 10)\\n  prototypes = torch.randn(10, 10)\\n\\n  # Act\\n  result = source.find_nearest_prototypes(embeddings, prototypes)\\n\\n  # Assert\\n  assert result.shape == torch.Size([10])  # check if the result has the correct shape',\n",
       "  100.0),\n",
       " ('def crop(img, top, left, height, width):\\n    \\n\\n    return img[top:top + height, left:left + width, :]',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')\\nfrom source import crop\\n\\ndef test_crop_function():\\n    img = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\\n    top = 1\\n    left = 1\\n    height = 1\\n    width = 1\\n    actual = crop(img, top, left, height, width)\\n    expected = [[4]]\\n    assert actual == expected, 'The crop function failed'\",\n",
       "  100.0),\n",
       " (\"def does_intersect_rect(p, particles, padding, rect, is_3d = False):\\n  \\n  if len(p) < 4: raise Exception('p = {} must have atleast 4 elements'.format(p))\\n  if len(particles) == 0: raise Exception('particles = {} can not be empty'.format(particles))\\n  if padding < 0.: raise Exception('padding = {} can not be negative'.format(padding))\\n  if len(rect) < 6: raise Exception('rect = {} must have 6 elements'.format(rect))\\n\\n  pr = [p[0] - p[3], p[1] - p[3], p[2], p[0] + p[3], p[1] + p[3], p[2]]\\n  if is_3d:\\n    pr[2] -= p[3]\\n    pr[5] += p[3]\\n\\n  if pr[0] < rect[0] + padding or pr[1] < rect[1] + padding or pr[3] > rect[3] - padding or pr[4] > rect[4] - padding:\\n    if is_3d:\\n      if pr[2] < rect[2] + padding or pr[5] > rect[5] - padding:\\n        return True\\n    else:\\n      return True\\n  return False\",\n",
       "  'import pytest\\nfrom source import does_intersect_rect  # assuming the function is in source.py\\n\\nclass TestDoesIntersectRect:\\n\\n    def test_does_intersect_rect(self):\\n        # Test with valid parameters\\n        p = [1, 1, 1, 1]\\n        particles = [[1, 1, 1], [2, 2, 2]]\\n        padding = 0.1\\n        rect = [0, 0, 0, 1, 1, 1]\\n        assert does_intersect_rect(p, particles, padding, rect) == False\\n\\n        # Test with valid parameters\\n        p = [0, 0, 0, 0]\\n        particles = [[0.5, 0.5, 0.5], [1, 1, 1]]\\n        padding = 0.1\\n        rect = [-0.5, -0.5, -0.5, 0.5, 0.5, 0.5]\\n        assert does_intersect_rect(p, particles, padding, rect) == True\\n\\n        # Test with invalid padding (negative)\\n        p = [1, 1, 1, 1]\\n        particles = [[1, 1, 1], [2, 2, 2]]\\n        padding = -0.1\\n        rect = [0, 0, 0, 1, 1, 1]\\n        with pytest.raises(Exception) as excinfo:\\n            does_intersect_rect(p, particles, padding, rect)\\n        assert \"padding = {} can not be negative\".format(padding) in str(excinfo.value)\\n\\n        # Test with invalid number of elements in p\\n        p = [1, 1]\\n        particles = [[1, 1, 1], [2, 2, 2]]\\n        padding = 0.1\\n        rect = [0, 0, 0, 1, 1, 1]\\n        with pytest.raises(Exception) as excinfo:\\n            does_intersect_rect(p, particles, padding, rect)\\n        assert \"p = {} must have atleast 4 elements\".format(p) in str(excinfo.value)\\n\\n        # Test with invalid number of elements in rect\\n        p = [1, 1, 1, 1]\\n        particles = [[1, 1, 1], [2, 2, 2]]\\n        padding = 0.1\\n        rect = [0, 0, 0, 1, 1]\\n        with pytest.raises(Exception) as excinfo:\\n            does_intersect_rect(p, particles, padding, rect)\\n        assert \"rect = {} must have 6 elements\".format(rect) in str(excinfo.value)\\n\\n        # Test with empty particles\\n        p = [1, 1, 1, 1]\\n        particles = []\\n        padding = 0.1\\n        rect = [0, 0, 0, 1, 1, 1]\\n        with pytest.raises(Exception) as excinfo:\\n            does_intersect_rect(p, particles, padding, rect)\\n        assert \"particles = {} can not be empty\".format(particles) in str(excinfo.value)\\n\\n        # Test with 3D case\\n        p = [1, 1, 1, 1]\\n        particles = [[1, 1, 1], [2, 2, 2]]\\n        padding = 0.1\\n        rect = [0, 0, 0, 1, 1, 1]\\n        assert does_intersect_rect(p, particles, padding, rect, is_3d=True) == False',\n",
       "  67.0),\n",
       " ('def angstrom_aod_at_lambda(aod0, lambda0, alpha=1.14, lambda1=700.0):\\n    r\\n    return aod0 * ((lambda1 / lambda0) ** (-alpha))',\n",
       "  'import pytest\\n\\nfrom source import angstrom_aod_at_lambda\\n\\ndef test_angstrom_aod_at_lambda():\\n    aod0 = 1\\n    lambda0 = 400\\n    lambda1 = 700\\n    assert angstrom_aod_at_lambda(aod0, lambda0, lambda1=lambda1) == 1',\n",
       "  67.0),\n",
       " ('def timestamp_to_float(t):\\n    \\n    return t.s + t.us * 1E-6',\n",
       "  \"import pytest\\nimport source\\n\\nclass TestTimestampToFloat:\\n\\n    def test_timestamp_to_float(self):\\n        time_stamp = source.TimeStamp(s=1, us=2)\\n        result = source.timestamp_to_float(time_stamp)\\n        assert result == 1.000002, 'The timestamp should convert to float correctly'\",\n",
       "  50.0),\n",
       " ('def diff_length_mlfp(fp, nu, threshold, smartinverse=0, maxiter=10**5):\\n    \\n    return None',\n",
       "  \"import sys\\nsys.path.append('.')\\nfrom source import diff_length_mlfp\\n\\ndef test_diff_length_mlfp():\\n    assert diff_length_mlfp() is None\\n    assert diff_length_mlfp('string') is None\\n    assert diff_length_mlfp(12345, 6, 0.5) is None\\n    assert diff_length_mlfp('file_path', 6, 0.5, 1, 10 ** 6) is None\",\n",
       "  50.0),\n",
       " ('def deg2arg(theta):\\n    \\n    return ((theta + 180) % 360) -180',\n",
       "  '# test_source.py\\nimport pytest\\nimport source  # assuming the source code is in a file named source.py in the same directory\\n\\ndef test_deg2arg():\\n    assert source.deg2arg(0) == 0\\n    assert source.deg2arg(90) == 90\\n    assert source.deg2arg(180) == 180\\n    assert source.deg2arg(270) == 270\\n    assert source.deg2arg(360) == 0\\n    assert source.deg2arg(450) == -90\\n    assert source.deg2arg(540) == -180\\n    assert source.deg2arg(720) == 0',\n",
       "  100.0),\n",
       " ('def flatten_frame(df, swap=False):\\n    \\n    new_frame = df.stack()\\n    if swap:\\n        new_frame = new_frame.swaplevel()\\n    #new_frame.index = new_frame.index.to_series()\\n    return new_frame',\n",
       "  'bash\\npytest -v',\n",
       "  0.0),\n",
       " ('def _fit_quality(column, weights, constraint):\\n    \\n    return abs((column * weights).sum() - constraint) / constraint',\n",
       "  '# import the function from source file\\nfrom source import _fit_quality\\n\\n# Test class for _fit_quality function\\nclass TestFitQuality:\\n    def test_zero_weights(self):\\n        # Define test case - column, weights, and constraint\\n        column = [1, 2, 3]\\n        weights = [0, 0, 0]\\n        constraint = 6\\n\\n        # Assertion to check if function returns expected result\\n        assert _fit_quality(column, weights, constraint) == 0.0\\n\\n    def test_positive_weights(self):\\n        # Define test case - column, weights, and constraint\\n        column = [1, 2, 3]\\n        weights = [1, 2, 3]\\n        constraint = 6\\n\\n        # Assertion to check if function returns expected result\\n        assert _fit_quality(column, weights, constraint) == 0.0\\n\\n    def test_negative_weights(self):\\n        # Define test case - column, weights, and constraint\\n        column = [1, 2, 3]\\n        weights = [-1, -2, -3]\\n        constraint = 6\\n\\n        # Assertion to check if function returns expected result\\n        assert _fit_quality(column, weights, constraint) == 0.0\\n\\n    def test_large_weights(self):\\n        # Define test case - column, weights, and constraint\\n        column = [1, 2, 3]\\n        weights = [1000000, 1000000, 1000000]\\n        constraint = 6\\n\\n        # Assertion to check if function returns expected result\\n        assert _fit_quality(column, weights, constraint) == 0.0',\n",
       "  100.0),\n",
       " ('def orthoProj(pointA, pointB, S):\\n    \\n\\n    d = (S[0]**2 + S[1]**2)**.5\\n    BH = ((pointA[0] - pointB[0])*S[0] + (pointA[1] - pointB[1])*S[1])/d\\n    \\n    xH = pointB[0] + BH/d*S[0]\\n    yH = pointB[1] + BH/d*S[1]\\n    \\n    return (xH, yH)',\n",
       "  \"import pytest\\nimport source  # Assuming that the source code is in a file named 'source.py'\\n\\nclass TestOrthoProj:\\n\\n    def test_ortho_proj_same_point(self):\\n        pointA = (1, 1)\\n        pointB = (1, 1)\\n        S = (1, 0)\\n        assert source.orthoProj(pointA, pointB, S) == pointB\\n\\n    def test_ortho_proj_point_on_line(self):\\n        pointA = (1, 1)\\n        pointB = (2, 2)\\n        S = (1, 1)\\n        assert source.orthoProj(pointA, pointB, S) == pointA\\n\\n    def test_ortho_proj_point_not_on_line(self):\\n        pointA = (1, 1)\\n        pointB = (2, 2)\\n        S = (0, 1)\\n        expected = (2, 2)   # or expected result based on the implementation of orthoProj\\n        assert source.orthoProj(pointA, pointB, S) == expected\",\n",
       "  100.0),\n",
       " (\"def particle_track(time_0, particle, velocity, acceleration, ID):\\n    \\n    _particle_track = {'Times': [time_0],\\n                       'Particles_Position': [particle['pos']],\\n                       'Particles_Size': [particle['size']], \\n                       'Particles_Bbox': [(particle['bbox_tl'], particle['bbox_hw'])],\\n                       'Particles_Max_Intensity': [particle['max_intensity']],\\n                       'Particles_Estimated_Position': [particle['pos']],\\n                       'Particles_Estimated_Velocity': [velocity],\\n                       'Particles_Estimated_Acceleration': [acceleration],\\n                       'Projected_Frames': 0,\\n                       'Track_ID': ID}\\n\\n    return _particle_track\",\n",
       "  'import pytest\\nimport source  # assuming source.py is in the same directory\\n\\ndef test_particle_track():\\n    time_0 = 10\\n    particle = {\\'pos\\': [1, 2], \\'size\\': 3, \\'bbox_tl\\': [1, 2], \\'bbox_hw\\': [4, 5], \\'max_intensity\\': 6}\\n    velocity = [0.1, 0.2]\\n    acceleration = [0.3, 0.4]\\n    ID = \\'Test_ID\\'\\n    \\n    result = source.particle_track(time_0, particle, velocity, acceleration, ID)\\n    \\n    assert isinstance(result, dict), \"The function should return a dictionary\"',\n",
       "  100.0),\n",
       " ('def sliding_window(image, step=10, window_size=(20, 20), with_data=True):\\n    \\n    # slide a window across the image\\n    w, h = window_size\\n    W, H = image.shape[:2]\\n    offset_w = (W - w) % step\\n    offset_h = (H - h) % step\\n    \\n    for x in range(0, W - w + offset_w + 1, step):\\n        if x + w > W:\\n            x = W - w\\n        for y in range(0, H - h + offset_h + 1, step):\\n            if y + h > H:\\n                y = H - h\\n            if with_data:\\n                yield image[x:x + w, y:y + h], x, y, w, h\\n            else:\\n                yield x, y, w, h',\n",
       "  'import pytest\\nimport numpy as np\\n\\nfrom source import sliding_window\\n\\ndef test_sliding_window_returns_expected_shapes():\\n    image = np.zeros((100, 100))\\n    for window, x, y, w, h in sliding_window(image, window_size=(5, 5)):\\n        assert isinstance(window, np.ndarray)\\n        assert window.shape == (5, 5)\\n        assert isinstance(x, int)\\n        assert isinstance(y, int)\\n        assert isinstance(w, int)\\n        assert isinstance(h, int)\\n\\ndef test_sliding_window_with_data_returns_expected_shapes():\\n    image = np.zeros((100, 100))\\n    for window, x, y, w, h in sliding_window(image, with_data=True, window_size=(5, 5)):\\n        assert isinstance(window, np.ndarray)\\n        assert window.shape == (5, 5)\\n        assert isinstance(x, int)\\n        assert isinstance(y, int)\\n        assert isinstance(w, int)\\n        assert isinstance(h, int)\\n\\ndef test_sliding_window_offsets():\\n    image = np.zeros((100, 100))\\n    for window, x, y, w, h in sliding_window(image, step=15, window_size=(20, 20)):\\n        assert x % 15 == 0\\n        assert y % 15 == 0\\n\\ndef test_sliding_window_full_image():\\n    image = np.zeros((100, 100))\\n    for window, x, y, w, h in sliding_window(image, step=100, window_size=(100, 100)):\\n        assert x == 0\\n        assert y == 0\\n        assert w == 100\\n        assert h == 100',\n",
       "  93.0),\n",
       " ('def snaps_to_hydroobject(gdf, datamodel, method, tolerance=0.001, dtype=bool):\\n    \\n\\n    branches = datamodel.hydroobject\\n    gdf.snap_to_branch(branches, snap_method=method, maxdist=tolerance)\\n    series = ~gdf.branch_offset.isna()\\n    return series.astype(dtype)',\n",
       "  'import pytest\\nfrom source import snaps_to_hydroobject\\nimport pandas as pd\\n\\ndef test_snaps_to_hydroobject():\\n    # Arrange\\n    gdf = pd.DataFrame()  # consider filling this with some values or use a fixture\\n    datamodel = pd.DataFrame()  # consider filling this with some values or use a fixture\\n    method = \"some_method\"  # replace with a real method\\n\\n    # Act\\n    result = snaps_to_hydroobject(gdf, datamodel, method)\\n\\n    # Assert\\n    assert result.shape == (0, 0)  # or whatever is appropriate for your case',\n",
       "  40.0),\n",
       " ('def to_zero_index_slice(s):  # pylint: disable=invalid-name\\n    \\n    if s.start < 0:\\n        return slice(s.start, s.stop-1, s.step)\\n    return slice(s.start-1, s.stop, s.step)',\n",
       "  \"import pytest\\nimport importlib\\n\\ndef test_to_zero_index_slice():\\n    source = importlib.import_module('source')  # imports the source.py module\\n\\n    # Testing with positive start\\n    assert source.to_zero_index_slice(slice(2, 5, 1)) == slice(1, 4, 1)\\n\\n    # Testing with negative start\\n    assert source.to_zero_index_slice(slice(-2, 5, 1)) == slice(-1, 4, 1)\",\n",
       "  75.0),\n",
       " ('import torch\\n\\ndef get_bert_embeddings_pooler(tokens_tensor, segments_tensors, model):\\n    \\n\\n    # Gradient calculation id disabled\\n    # Model is in inference mode\\n    with torch.no_grad():\\n        outputs = model(tokens_tensor, segments_tensors)\\n        final_embedding = outputs[\"pooler_output\"].squeeze(dim=0)\\n        return  final_embedding.tolist()',\n",
       "  \"import torch\\nimport pytest\\nfrom source import get_bert_embeddings_pooler\\n\\ndef test_get_bert_embeddings_pooler():\\n    tokens_tensor = torch.tensor([[7, 6, 0, 1]])\\n    segments_tensors = torch.tensor([[0, 1, 0, 1]])\\n    model = torch.nn.BertModel.from_pretrained('bert-base-uncased')\\n    result = get_bert_embeddings_pooler(tokens_tensor, segments_tensors, model)\\n    assert isinstance(result, list), 'The function did not return a list'\",\n",
       "  33.0),\n",
       " ('def predict(model, X_test):\\n    \\n\\n    return model.predict(X_test, batch_size=None, verbose=0, steps=None).flatten()',\n",
       "  '# test_source.py\\nimport pytest\\nimport numpy as np\\nfrom source import predict\\n\\ndef test_predict():\\n    \"\"\"\\n    Test for predict function\\n    \"\"\"\\n    # Assuming we have a dummy model and test data\\n    class DummyModel:\\n        def predict(self, X_test, batch_size=None, verbose=0, steps=None):\\n            return np.array([1, 2, 3])\\n\\n    model = DummyModel()\\n    X_test = np.array([4, 5, 6])\\n\\n    expected_output = np.array([1, 2, 3])\\n    assert np.array_equal(predict(model, X_test), expected_output), \"Predict function output does not match expected output\"',\n",
       "  100.0),\n",
       " ('def create_image(deck, background=\\'black\\'):\\n    \\n    from PIL import Image\\n\\n    image_format = deck.key_image_format()\\n\\n    return Image.new(\"RGB\", image_format[\\'size\\'], background)',\n",
       "  'import pytest\\nfrom source import create_image\\nfrom PIL import Image\\n\\ndef test_create_image_RGB():\\n    deck = lambda: None\\n    deck.key_image_format = lambda : {\\'size\\': (500,500)}\\n    result = create_image(deck, \\'white\\')\\n    assert isinstance(result, Image.Image), \"The function did not return an Image Object\"\\n\\ndef test_create_image_size():\\n    deck = lambda: None\\n    deck.key_image_format = lambda : {\\'size\\': (1000,1000)}\\n    result = create_image(deck, \\'white\\')\\n    assert result.size == (1000,1000), \"The Image size is not correct\"\\n\\ndef test_create_image_format():\\n    deck = lambda: None\\n    deck.key_image_format = lambda : {\\'size\\': (500,500)}\\n    result = create_image(deck, \\'white\\')\\n    assert result.mode == \\'RGB\\', \"The Image format is not RGB\"',\n",
       "  100.0),\n",
       " ('def unpack_state(x, nR, nZ):\\n    \\n\\n    cR = x[:nR]\\n    cZ = x[nR:nR+nZ]\\n    cL = x[nR+nZ:]\\n    return cR, cZ, cL',\n",
       "  'import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.dirname(os.path.abspath(__file__)) + \"/../\") # To import source.py\\nfrom source import unpack_state  # Importing the function from source.py\\n\\ndef test_unpack_state():\\n    x = [1, 2, 3, 4, 5, 6, 7, 8, 9]\\n    nR = 3\\n    nZ = 2\\n    cR, cZ, cL = unpack_state(x, nR, nZ)\\n    assert cR == [1, 2, 3], \"The first part of the array should be [1, 2, 3]\"\\n    assert cZ == [4, 5], \"The second part of the array should be [4, 5]\"\\n    assert cL == [6, 7, 8, 9], \"The last part of the array should be [6, 7, 8, 9]\"',\n",
       "  100.0),\n",
       " ('def squared_differences(x, y):\\n    \\n\\n    return (x - y) ** 2',\n",
       "  'from source import *\\nimport pytest\\nimport source\\n\\ndef test_squared_differences():\\n    assert source.squared_differences(5, 3) == 4',\n",
       "  100.0),\n",
       " ('def kmh_to_si(vals):\\n    \\n    return vals * 1000. / 3600.',\n",
       "  'import pytest\\nimport source  # this file should contain the function kmh_to_si\\n\\ndef test_kmh_to_si():\\n    assert source.kmh_to_si(3) == 3000.0 / 3600.0',\n",
       "  100.0),\n",
       " ('def set_voronoi_point_data(i, point, radius_array_data, cell_array, count, points, radius):\\n    \\n    points.InsertNextPoint(point)\\n    cell_array.InsertNextCell(1)\\n    cell_array.InsertCellPoint(count)\\n    value = radius_array_data(i)\\n    radius[count] = value\\n    count += 1\\n\\n    return count',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import set_voronoi_point_data\\n\\ndef test_set_voronoi_point_data():\\n    i = 0\\n    point = [0, 0, 0]\\n    radius_array_data = lambda i: i\\n    cell_array = vtk.vtkCellArray()\\n    points = vtk.vtkPoints()\\n    radius = [0] * 10\\n    count = 0\\n\\n    assert set_voronoi_point_data(i, point, radius_array_data, cell_array, count, points, radius) == 1',\n",
       "  12.0),\n",
       " ('import torch\\n\\ndef shearx_grid(output_size, ulim=(-1, 1), vlim=(-5, 5), out=None, device=None):\\n    \\n    nv, nu = output_size\\n    urange = torch.linspace(ulim[0], ulim[1], nu, device=device)\\n    vrange = torch.linspace(vlim[0], vlim[1], nv, device=device)\\n    vs, us = torch.meshgrid([vrange, urange])\\n    ys = us\\n    xs = us * vs\\n    return torch.stack([xs, ys], 2, out=out)',\n",
       "  '# test_shearx_grid.py\\nimport pytest\\nimport torch\\nfrom source import shearx_grid  # Assuming that the function is defined in source.py\\n\\ndef test_shearx_grid_shape():\\n    output = shearx_grid((10, 10))\\n    assert output.shape == (10, 10, 2), \"The function should return a 3D tensor with shape (N, 2)\"\\n\\ndef test_shearx_grid_values():\\n    output = shearx_grid((1, 1))\\n    expected_output = torch.tensor([[[0., 0.]], [[-5., -5.]]], device=torch.device(\\'cpu\\'))\\n    assert torch.allclose(output, expected_output, atol=1e-5), \"The function should return the correct values\"\\n\\nif __name__ == \"__main__\":\\n    pytest.main()',\n",
       "  100.0),\n",
       " ('def is_linear(scan_range):\\n    \\n\\n    if scan_range[1] - scan_range[0] == scan_range[-1] - scan_range[-2]:\\n        return True\\n    else:\\n        return False',\n",
       "  '# test_source.py\\n\\nimport pytest\\nfrom source import is_linear\\n\\ndef test_is_linear():\\n    assert is_linear([1, 2, 3, 4, 5]) == True\\n\\ndef test_is_linear_false():\\n    assert is_linear([1, 3, 5, 7, 9]) == False',\n",
       "  75.0),\n",
       " ('def _distance_acc(distances, thr=0.5):\\n    \\n    distance_valid = distances != -1\\n    num_distance_valid = distance_valid.sum()\\n    if num_distance_valid > 0:\\n        return (distances[distance_valid] < thr).sum() / num_distance_valid\\n    return -1',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\"..\") # Adds the parent directory into the path to import the module\\nfrom source import _distance_acc\\n\\ndef test_distance_acc():\\n    distances = [0.2, 0.6, -1, 0.8, 0.3]\\n    assert _distance_acc(distances) == 0.5, \"_distance_acc function is not working properly\"\\n\\ndef test_distance_acc_with_threshold():\\n    distances = [0.2, 0.6, 0.8, 0.8, 0.3]\\n    assert _distance_acc(distances, thr=0.7) == 0.5, \"_distance_acc function is not working properly with threshold\"\\n\\ndef test_distance_acc_no_valid_input():\\n    distances = [-1, -1, -1, -1, -1]\\n    assert _distance_acc(distances) == -1, \"_distance_acc function is not returning -1 for all -1 input\"',\n",
       "  50.0),\n",
       " (\"def rgb(red, green, blue):\\n    \\n    assert isinstance(red, int)\\n    assert 0 <= red < 256\\n\\n    assert isinstance(green, int)\\n    assert 0 <= green < 256\\n\\n    assert isinstance(blue, int)\\n    assert 0 <= blue < 256\\n\\n    return 'rgba(%d, %d, %d)' % (red, green, blue)\",\n",
       "  \"import pytest\\nfrom source import rgb\\n\\ndef test_rgb():\\n    assert rgb(10, 20, 30) == 'rgba(10, 20, 30)'\\n    assert rgb(255, 0, 0) == 'rgba(255, 0, 0)'\\n    assert rgb(0, 255, 0) == 'rgba(0, 255, 0)'\\n    assert rgb(0, 0, 255) == 'rgba(0, 0, 255)'\\n    assert rgb(255, 255, 255) == 'rgba(255, 255, 255)'\",\n",
       "  100.0),\n",
       " ('def det(vector1, vector2):\\n    \\n    return vector1.x_ * vector2.y_ - vector1.y_ * vector2.x_',\n",
       "  'import sys\\nsys.path.append(\".\")  # To import source.py from the same directory\\nfrom source import Vector  # Assuming the vector class is defined in source.py\\nimport pytest\\n\\ndef test_det():\\n    vector1 = Vector(2, 3)  # Create instance of Vector with x=2, y=3\\n    vector2 = Vector(4, 5)  # Create instance of Vector with x=4, y=5\\n    assert abs(det(vector1, vector2) - (-3)) < 1e-9  # Use assertion to test if the determinant is correct',\n",
       "  50.0),\n",
       " ('def air_density(T, P):\\n    \\n    R = 28704e-2 # gas constant for air\\n    return P / (T*R)',\n",
       "  'import pytest\\nfrom source import air_density\\n\\ndef test_air_density():\\n    assert air_density(293, 101325) == 1.0023778766810583',\n",
       "  100.0),\n",
       " ('def encode_chromosome(chrom):\\n    \\n    chrom = chrom.upper()\\n    if chrom == \"X\":\\n        return 23\\n    if chrom == \"Y\":\\n        return 24\\n    if chrom == \"XY\" or chrom == \"YX\":\\n        return 25\\n    if chrom == \"M\" or chrom == \"MT\":\\n        return 26\\n\\n    try:\\n        chrom = int(chrom)\\n    except ValueError:\\n        return 0\\n\\n    if chrom < 1 or chrom > 26:\\n        return 0\\n\\n    return chrom',\n",
       "  \"import pytest\\nfrom source import *\\n\\ndef test_encode_chromosome():\\n    assert encode_chromosome('X') == 23\\n    assert encode_chromosome('Y') == 24\\n    assert encode_chromosome('XY') == 25\\n    assert encode_chromosome('YX') == 25\\n    assert encode_chromosome('M') == 26\\n    assert encode_chromosome('MT') == 26\\n    assert encode_chromosome('21') == 21\\n    assert encode_chromosome('0') == 0\\n    assert encode_chromosome('27') == 0\\n    assert encode_chromosome('A') == 0\",\n",
       "  100.0),\n",
       " ('def fit_one_peak_curve(x, y, fit_range=None):\\n    \\n    from lmfit.models import LinearModel, LorentzianModel\\n\\n    peak = LorentzianModel()\\n    background = LinearModel()\\n    model = peak + background\\n    if fit_range is not None:\\n        x1, x2 = fit_range\\n        xf = x[x1:x2]\\n        yf = y[x1:x2]\\n    else:\\n        xf = x\\n        yf = y\\n    model.set_param_hint(\"slope\", value=5)\\n    model.set_param_hint(\"intercept\", value=0)\\n    model.set_param_hint(\"center\", value=0.005)\\n    model.set_param_hint(\"amplitude\", value=0.1)\\n    model.set_param_hint(\"sigma\", value=0.003)\\n    # out=model.fit(yf, x=xf)#, method=\\'nelder\\')\\n    out = model.fit(yf, x=xf, method=\"leastsq\")\\n    cen = out.params[\"center\"].value\\n    cen_std = out.params[\"center\"].stderr\\n    wid = out.params[\"sigma\"].value * 2\\n    wid_std = out.params[\"sigma\"].stderr * 2\\n    return cen, cen_std, wid, wid_std, xf, out',\n",
       "  'import os\\nimport numpy as np\\nfrom lmfit.models import LorentzianModel\\nfrom source import fit_one_peak_curve\\n\\ndef test_fit_one_peak_curve():\\n    # Assuming a source.py file in the same directory has the function fit_one_peak_curve\\n    # Test data\\n    x = np.linspace(0, 100, 1000)\\n    y = np.sin(x) + np.random.normal(0, 0.1, x.shape)\\n\\n    # Without specifying a fit_range, it should use the whole x\\n    cen, cen_std, wid, wid_std, xf, out = fit_one_peak_curve(x, y)\\n    assert np.isclose(cen, 0.5 * np.pi, atol=0.1)\\n\\n    # With a fit_range, it should only fit in that range\\n    cen, cen_std, wid, wid_std, xf, out = fit_one_peak_curve(x, y, fit_range=(200, 800))\\n    assert np.isclose(cen, 0.5 * np.pi, atol=0.1)\\n\\n    # Large deviation from the truth should raise an exception\\n    with pytest.raises(AssertionError):\\n        cen, cen_std, wid, wid_std, xf, out = fit_one_peak_curve(x, y * 2, fit_range=(200, 800))',\n",
       "  0.0),\n",
       " ('def normalize_directory_name(directory_name):\\n    \\n    return directory_name.lower()',\n",
       "  '# test_source.py\\n\\nimport os\\nimport sys\\nsys.path.append(\".\")\\n\\nfrom source import normalize_directory_name\\n\\ndef test_normalize_directory_name():\\n    assert normalize_directory_name(\"TestDirectory\") == \"testdirectory\"',\n",
       "  100.0),\n",
       " ('def get_game_info_helper(game, player):\\n    \\n    agent_moves = len(game.get_legal_moves(player))\\n    opponent_moves = len(game.get_legal_moves(game.get_opponent(player)))\\n    blanks = len(game.get_blank_spaces())\\n    agent_y, agent_x = game.get_player_location(player)\\n    opponent_y, opponent_x = game.get_player_location(game.get_opponent(player))\\n\\n    return agent_moves, opponent_moves, blanks, agent_y, agent_x, opponent_y, opponent_x',\n",
       "  \"import sys\\nsys.path.insert(0, './')\\nfrom game import Game\\nfrom source import get_game_info_helper\\n\\ndef test_get_game_info_helper():\\n    game = Game()\\n    player = 'X'\\n    # case when there is only one legal move\\n    game.make_move(player, 0, 0)\\n    assert get_game_info_helper(game, player) == (1, 0, 0, 0, 0, 0, 0)\\n    \\n    # case when there are multiple legal moves\\n    game.make_move(player, 0, 1)\\n    assert get_game_info_helper(game, player) == (2, 0, 1, 0, 0, 0, 0)\\n    \\n    # case when there are no blank spaces\\n    game.make_move(player, 0, 2)\\n    assert get_game_info_helper(game, player) == (2, 0, 1, 0, 0, 0, 0)\\n    \\n    # case when the player is in the corner\\n    game.make_move(player, 0, 0)\\n    assert get_game_info_helper(game, player) == (2, 0, 1, 0, 0, 0, 0)\\n    \\n    # case when the player is in the center\\n    game.make_move(player, 1, 1)\\n    assert get_game_info_helper(game, player) == (2, 0, 1, 1, 1, 0, 0)\\n    \\n    # case when the player is in the last row\\n    game.make_move(player, 2, 0)\\n    assert get_game_info_helper(game, player) == (2, 0, 1, 1, 1, 0, 0)\",\n",
       "  0.0),\n",
       " (\"def build_keras_model():\\n    \\n    from keras.models import Sequential\\n    from keras.layers import Dense, Dropout, Activation\\n    from keras.layers import Embedding\\n    from keras.layers import LSTM\\n    from keras.layers import Convolution1D, MaxPooling1D\\n    keras_model = Sequential()\\n    keras_model.add(Embedding(20000, 128, input_length=100))\\n    keras_model.add(Dropout(0.25))\\n    keras_model.add(Convolution1D(nb_filter=64,\\n                                  filter_length=5,\\n                                  border_mode='valid',\\n                                  activation='relu',\\n                                  subsample_length=1))\\n    keras_model.add(MaxPooling1D(pool_length=4))\\n    keras_model.add(LSTM(70))\\n    keras_model.add(Dense(1))\\n    keras_model.add(Activation('sigmoid'))\\n    return keras_model\",\n",
       "  '# test_source.py\\n\\nimport pytest\\nfrom source import build_keras_model\\n\\ndef test_build_keras_model():\\n    keras_model = build_keras_model()\\n    assert isinstance(keras_model, Sequential)',\n",
       "  13.0),\n",
       " ('def linear(x):\\n    \\n    return x',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\\nimport source  # Assuming the file is named source.py\\n\\ndef test_linear():\\n    assert source.linear(2) == 2\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef pdist(sample_1, sample_2, norm=2, eps=1e-5):\\n    r\\n    n_1, n_2 = sample_1.size(0), sample_2.size(0)\\n    norm = float(norm)\\n    if norm == 2.:\\n        norms_1 = torch.sum(sample_1**2, dim=1, keepdim=True)\\n        norms_2 = torch.sum(sample_2**2, dim=1, keepdim=True)\\n        norms = (norms_1.expand(n_1, n_2) +\\n                 norms_2.transpose(0, 1).expand(n_1, n_2))\\n        distances_squared = norms - 2 * sample_1.mm(sample_2.t())\\n        return torch.sqrt(eps + torch.abs(distances_squared))\\n    else:\\n        dim = sample_1.size(1)\\n        expanded_1 = sample_1.unsqueeze(1).expand(n_1, n_2, dim)\\n        expanded_2 = sample_2.unsqueeze(0).expand(n_1, n_2, dim)\\n        differences = torch.abs(expanded_1 - expanded_2) ** norm\\n        inner = torch.sum(differences, dim=2, keepdim=False)\\n        return (eps + inner) ** (1. / norm)',\n",
       "  \"import torch\\nimport sys\\nsys.path.insert(0, '../')\\nfrom source import pdist\\n\\ndef test_pdist():\\n    sample_1 = torch.tensor([[1.0, 2.0, 3.0]])\\n    sample_2 = torch.tensor([[4.0, 5.0, 6.0]])\\n    assert torch.allclose(pdist(sample_1, sample_2), torch.tensor([[5.19615242, 11.31370084, 17.47030025]]))\\n\\ntest_pdist()\",\n",
       "  0),\n",
       " ('def area_to_capacity(statistical_roof_model_area_based, power_density_flat, power_density_tilted):\\n    \\n    cap_based = statistical_roof_model_area_based.copy()\\n    flat_roofs = cap_based.index.get_level_values(0) == \"flat\"\\n    tilted_roofs = cap_based.index.get_level_values(0) != \"flat\"\\n    cap_based[flat_roofs] = cap_based[flat_roofs] * power_density_flat\\n    cap_based[tilted_roofs] = cap_based[tilted_roofs] * power_density_tilted\\n    return cap_based / cap_based.sum()',\n",
       "  '# testing_file.py\\n\\nimport pytest\\nimport os\\nimport pandas as pd\\nfrom source import area_to_capacity\\n\\n# Read data\\nfile_path = os.path.join(os.path.dirname(__file__), \\'source.py\\')\\ncap_data = pd.read_csv(file_path)\\n\\n# Define test case\\ndef test_area_to_capacity():\\n    # Assuming the first row of the file has the data for flat_roofs and power_density_flat\\n    # Assuming the second row of the file has the data for tilted_roofs and power_density_tilted\\n    # Assuming the third row of the file has the data for statistical_roof_model_area_based\\n    flat_roofs = cap_data.iloc[0]\\n    power_density_flat = cap_data.iloc[1]\\n    power_density_tilted = cap_data.iloc[2]\\n    statistical_roof_model_area_based = cap_data.iloc[3]\\n\\n    # Run function and get result\\n    result = area_to_capacity(statistical_roof_model_area_based, power_density_flat, power_density_tilted)\\n\\n    # Check if the result is as expected\\n    assert result.equals(cap_data.iloc[4]), \"The function did not return the expected result.\"\\n\\n\\nif __name__ == \"__main__\":\\n    test_area_to_capacity()',\n",
       "  86.0),\n",
       " ('import torch\\n\\ndef pdist(sample_1, sample_2, norm=2, eps=1e-5):\\n    r\\n    n_1, n_2 = sample_1.size(0), sample_2.size(0)\\n    norm = float(norm)\\n    if norm == 2.:\\n        norms_1 = torch.sum(sample_1**2, dim=1, keepdim=True)\\n        norms_2 = torch.sum(sample_2**2, dim=1, keepdim=True)\\n        norms = (norms_1.expand(n_1, n_2) +\\n                 norms_2.transpose(0, 1).expand(n_1, n_2))\\n        distances_squared = norms - 2 * sample_1.mm(sample_2.t())\\n        return torch.sqrt(eps + torch.abs(distances_squared))\\n    else:\\n        dim = sample_1.size(1)\\n        expanded_1 = sample_1.unsqueeze(1).expand(n_1, n_2, dim)\\n        expanded_2 = sample_2.unsqueeze(0).expand(n_1, n_2, dim)\\n        differences = torch.abs(expanded_1 - expanded_2) ** norm\\n        inner = torch.sum(differences, dim=2, keepdim=False)\\n        return (eps + inner) ** (1. / norm)',\n",
       "  \"import torch\\nimport sys\\nsys.path.insert(0, './')\\nfrom source import pdist\\n\\ndef test_pdist():\\n    sample_1 = torch.Tensor([[1.0, 0.0, 2.0], [3.0, 4.0, 0.0]])\\n    sample_2 = torch.Tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\\n    assert torch.allclose(pdist(sample_1, sample_2), torch.Tensor([[1.0000, 2.2360], [2.2360, 5.1961]]))\",\n",
       "  18.0),\n",
       " ('import torch\\n\\ndef rotationMatrix_to_Quaternion(rotation_matrix, eps=1e-6):\\n    \\n    if not torch.is_tensor(rotation_matrix):\\n        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\\n            type(rotation_matrix)))\\n\\n    if len(rotation_matrix.shape) > 3:\\n        raise ValueError(\\n            \"Input size must be a three dimensional tensor. Got {}\".format(\\n                rotation_matrix.shape))\\n    if not rotation_matrix.shape[-2:] == (3, 4):\\n        raise ValueError(\\n            \"Input size must be a N x 3 x 4  tensor. Got {}\".format(\\n                rotation_matrix.shape))\\n\\n    rmat_t = torch.transpose(rotation_matrix, 1, 2)\\n\\n    mask_d2 = rmat_t[:, 2, 2] < eps\\n\\n    mask_d0_d1 = rmat_t[:, 0, 0] > rmat_t[:, 1, 1]\\n    mask_d0_nd1 = rmat_t[:, 0, 0] < -rmat_t[:, 1, 1]\\n\\n    t0 = 1 + rmat_t[:, 0, 0] - rmat_t[:, 1, 1] - rmat_t[:, 2, 2]\\n    q0 = torch.stack([rmat_t[:, 1, 2] - rmat_t[:, 2, 1],\\n                      t0, rmat_t[:, 0, 1] + rmat_t[:, 1, 0],\\n                      rmat_t[:, 2, 0] + rmat_t[:, 0, 2]], -1)\\n    t0_rep = t0.repeat(4, 1).t()\\n\\n    t1 = 1 - rmat_t[:, 0, 0] + rmat_t[:, 1, 1] - rmat_t[:, 2, 2]\\n    q1 = torch.stack([rmat_t[:, 2, 0] - rmat_t[:, 0, 2],\\n                      rmat_t[:, 0, 1] + rmat_t[:, 1, 0],\\n                      t1, rmat_t[:, 1, 2] + rmat_t[:, 2, 1]], -1)\\n    t1_rep = t1.repeat(4, 1).t()\\n\\n    t2 = 1 - rmat_t[:, 0, 0] - rmat_t[:, 1, 1] + rmat_t[:, 2, 2]\\n    q2 = torch.stack([rmat_t[:, 0, 1] - rmat_t[:, 1, 0],\\n                      rmat_t[:, 2, 0] + rmat_t[:, 0, 2],\\n                      rmat_t[:, 1, 2] + rmat_t[:, 2, 1], t2], -1)\\n    t2_rep = t2.repeat(4, 1).t()\\n\\n    t3 = 1 + rmat_t[:, 0, 0] + rmat_t[:, 1, 1] + rmat_t[:, 2, 2]\\n    q3 = torch.stack([t3, rmat_t[:, 1, 2] - rmat_t[:, 2, 1],\\n                      rmat_t[:, 2, 0] - rmat_t[:, 0, 2],\\n                      rmat_t[:, 0, 1] - rmat_t[:, 1, 0]], -1)\\n    t3_rep = t3.repeat(4, 1).t()\\n\\n    mask_c0 = mask_d2 * mask_d0_d1\\n    mask_c1 = mask_d2 * (~mask_d0_d1)\\n    mask_c2 = (~mask_d2) * mask_d0_nd1\\n    mask_c3 = (~mask_d2) * (~mask_d0_nd1)\\n    mask_c0 = mask_c0.view(-1, 1).type_as(q0)\\n    mask_c1 = mask_c1.view(-1, 1).type_as(q1)\\n    mask_c2 = mask_c2.view(-1, 1).type_as(q2)\\n    mask_c3 = mask_c3.view(-1, 1).type_as(q3)\\n\\n    q = q0 * mask_c0 + q1 * mask_c1 + q2 * mask_c2 + q3 * mask_c3\\n    q /= torch.sqrt(t0_rep * mask_c0 + t1_rep * mask_c1 +  # noqa\\n                    t2_rep * mask_c2 + t3_rep * mask_c3)  # noqa\\n    q *= 0.5\\n    return q',\n",
       "  'import pytest\\nimport torch\\n\\nfrom source import rotationMatrix_to_Quaternion\\n\\ndef test_rotationMatrix_to_Quaternion():\\n    # given\\n    rotation_matrix = torch.tensor([[[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]])\\n    eps = 1e-6\\n\\n    # when\\n    result = rotationMatrix_to_Quaternion(rotation_matrix, eps)\\n\\n    # then\\n    assert torch.allclose(result, torch.tensor([[0.5, 0.5, 0.5, 0.5]]))',\n",
       "  17.0),\n",
       " (\"def rgb_to_grayscale(img):\\n    \\n    if img.shape[0] != 3:\\n        raise TypeError('Input Image does not contain 3 Channels')\\n\\n    return (0.2989 * img[0] + 0.5870 * img[1] + 0.1140 * img[2]).to(img.dtype)\",\n",
       "  'def test_rgb_to_grayscale_conversion():\\n    img = np.array([[[255, 0, 0], [0, 255, 0], [0, 0, 255]], \\n                    [[255, 255, 255], [0, 0, 0], [255, 255, 255]], \\n                    [[0, 0, 0], [255, 255, 255], [0, 0, 0]]], dtype=np.uint8)\\n    grayscale_img = rgb_to_grayscale(img)\\n    expected_output = np.array([[[77, 77, 77], [77, 77, 77], [77, 77, 77]],\\n                                [[255, 255, 255], [0, 0, 0], [255, 255, 255]], \\n                                [[77, 77, 77], [255, 255, 255], [77, 77, 77]]], dtype=np.uint8)\\n    assert np.array_equal(grayscale_img, expected_output), \"The conversion to grayscale is not correct\"',\n",
       "  0.0),\n",
       " ('def eletype(eletype):\\n    \\n    elem_id = {\\n        1: (8, 4, 4),\\n        2: (12, 6, 7),\\n        3: (6, 3, 3),\\n        4: (18, 9, 9),\\n        5: (4, 2, 3),\\n        6: (4, 2, 3),\\n        7: (6, 2, 3),\\n        8: (6, 2, 3)}\\n    try:\\n        return elem_id[eletype]\\n    except:\\n        raise ValueError(\"You entered an invalid type of element.\")',\n",
       "  'import pytest\\nfrom source import eletype\\n\\ndef test_eletype():\\n    assert eletype(1) == (8, 4, 4)\\n    assert eletype(2) == (12, 6, 7)\\n    assert eletype(3) == (6, 3, 3)\\n    assert eletype(4) == (18, 9, 9)\\n    assert eletype(5) == (4, 2, 3)\\n    assert eletype(6) == (4, 2, 3)\\n    assert eletype(7) == (6, 2, 3)\\n    assert eletype(8) == (6, 2, 3)\\n\\ndef test_eletype_invalid_input():\\n    with pytest.raises(ValueError):\\n        eletype(9)',\n",
       "  100.0),\n",
       " ('def interval_to_point(predictions):\\n    \\n    return (predictions[:, 0] + predictions[:, 1]) / 2.',\n",
       "  \"import sys\\nsys.path.append('..')\\nimport source\\n\\ndef test_interval_to_point():\\n    predictions = [[0, 1], [2, 3], [4, 5]]\\n    expected_output = [0.5, 2.5, 4.5]\\n    assert source.interval_to_point(predictions) == expected_output\",\n",
       "  100.0),\n",
       " ('def rectCenter(rect):\\n    \\n    (xMin, yMin, xMax, yMax) = rect\\n    return (xMin+xMax)/2, (yMin+yMax)/2',\n",
       "  '# Import the function from your source.py file\\nfrom source import rectCenter\\n\\n# Test case 1: Test with normal values\\ndef test_rectCenter_1():\\n    rect = (1, 2, 3, 4)  # (xMin, yMin, xMax, yMax)\\n    expected_output = (2.5, 3.5)  # expected center point\\n    assert rectCenter(rect) == expected_output\\n\\n# Test case 2: Test with even values\\ndef test_rectCenter_2():\\n    rect = (0, 0, 2, 2)  # (xMin, yMin, xMax, yMax)\\n    expected_output = (1.0, 1.0)  # expected center point\\n    assert rectCenter(rect) == expected_output\\n\\n# Test case 3: Test with zero values\\ndef test_rectCenter_3():\\n    rect = (0, 0, 0, 0)  # (xMin, yMin, xMax, yMax)\\n    expected_output = (0.0, 0.0)  # expected center point\\n    assert rectCenter(rect) == expected_output\\n\\n# Test case 4: Test with large values\\ndef test_rectCenter_4():\\n    rect = (10**18, 10**18, 10**19, 10**19)  # (xMin, yMin, xMax, yMax)\\n    expected_output = ((10**18+10**19)/2, (10**18+10**19)/2)  # expected center point\\n    assert rectCenter(rect) == expected_output\\n\\n# Test case 5: Test with negative values\\ndef test_rectCenter_5():\\n    rect = (-10, -10, -5, -5)  # (xMin, yMin, xMax, yMax)\\n    expected_output = (-7.5, -7.5)  # expected center point\\n    assert rectCenter(rect) == expected_output',\n",
       "  100.0),\n",
       " ('def bisect_right(sorted_collection, item, lo=0, hi=None):\\n    \\n    if hi is None:\\n        hi = len(sorted_collection)\\n\\n    while lo < hi:\\n        mid = (lo + hi) // 2\\n        if sorted_collection[mid] <= item:\\n            lo = mid + 1\\n        else:\\n            hi = mid\\n\\n    return lo',\n",
       "  'import pytest\\nfrom source import bisect_right\\n\\ndef test_bisect_right():\\n    assert bisect_right([1, 2, 3, 4, 5, 6, 7], 6) == 4\\n    assert bisect_right([1, 2, 3, 4, 5, 6, 7], 1) == 0\\n    assert bisect_right([1, 2, 3, 4, 5, 6, 7], 7) == 7\\n    assert bisect_right([1, 2, 3, 4, 5, 6, 7], 0) == 0\\n    assert bisect_right([1, 2, 3, 4, 5, 6, 7], 2.5) == 2',\n",
       "  100.0),\n",
       " ('def get_stock_takeout_volume(stock_concentration, final_volume, concentration):\\n    \\n    dil_factor = stock_concentration / float(concentration)\\n    take_out_volume = final_volume / dil_factor\\n    take_out_volume = round(take_out_volume, 1)\\n    return take_out_volume',\n",
       "  'Python\\nimport pytest\\nfrom source import get_stock_takeout_volume\\n\\ndef test_get_stock_takeout_volume():\\n    assert get_stock_takeout_volume(200, 100, 20) == 50',\n",
       "  0.0),\n",
       " ('import torch\\n\\ndef compute_amplitude(waveforms, lengths=None, amp_type=\"avg\", scale=\"linear\"):\\n    \\n    if len(waveforms.shape) == 1:\\n        waveforms = waveforms.unsqueeze(0)\\n\\n    assert amp_type in [\"avg\", \"peak\"]\\n    assert scale in [\"linear\", \"dB\"]\\n\\n    if amp_type == \"avg\":\\n        if lengths is None:\\n            out = torch.mean(torch.abs(waveforms), dim=1, keepdim=True)\\n        else:\\n            wav_sum = torch.sum(input=torch.abs(waveforms), dim=1, keepdim=True)\\n            out = wav_sum / lengths\\n    elif amp_type == \"peak\":\\n        out = torch.max(torch.abs(waveforms), dim=1, keepdim=True)[0]\\n    else:\\n        raise NotImplementedError\\n\\n    if scale == \"linear\":\\n        return out\\n    elif scale == \"dB\":\\n        return torch.clamp(20 * torch.log10(out), min=-80)  # clamp zeros\\n    else:\\n        raise NotImplementedError',\n",
       "  'import pytest\\nimport torch\\nfrom source import compute_amplitude\\n\\ndef test_compute_amplitude():\\n    # Testing with valid inputs\\n    # case 1: for 1D tensor\\n    waveforms = torch.rand(100, 1)\\n    assert torch.allclose(compute_amplitude(waveforms), torch.abs(torch.mean(waveforms)), atol=1e-6)\\n\\n    # case 2: for 2D tensor\\n    waveforms = torch.rand(10, 100)\\n    assert torch.allclose(compute_amplitude(waveforms), torch.abs(torch.mean(waveforms)), atol=1e-6)\\n\\n    # Testing with \"peak\" amp_type\\n    waveforms = torch.rand(10, 100)\\n    assert torch.allclose(compute_amplitude(waveforms, amp_type=\"peak\"), torch.abs(torch.max(waveforms)), atol=1e-6)\\n\\n    # Testing with \"dB\" scale\\n    waveforms = torch.rand(10, 100)\\n    assert torch.allclose(compute_amplitude(waveforms, scale=\"dB\"), 20 * torch.log10(torch.mean(torch.abs(waveforms))), atol=1e-6)\\n\\n    # Testing with lengths\\n    waveforms = torch.rand(10, 100)\\n    lengths = torch.rand(10)\\n    assert torch.allclose(compute_amplitude(waveforms, lengths=lengths), torch.abs(torch.sum(input=torch.abs(waveforms), dim=1)) / lengths, atol=1e-6)\\n\\n    # Testing with invalid inputs\\n    # case 1: invalid amp_type\\n    waveforms = torch.rand(10, 100)\\n    with pytest.raises(NotImplementedError):\\n        compute_amplitude(waveforms, amp_type=\"invalid\")\\n        \\n    # case 2: invalid scale\\n    waveforms = torch.rand(10, 100)\\n    with pytest.raises(NotImplementedError):\\n        compute_amplitude(waveforms, scale=\"invalid\")',\n",
       "  53.0),\n",
       " ('import torch\\n\\ndef attention_consistency_loss(attention_weights, reattention_weights):\\n    \\n    return torch.sum(\\n        torch.square(torch.subtract(attention_weights, reattention_weights))\\n    )',\n",
       "  'import pytest\\nimport torch\\nfrom source import attention_consistency_loss\\n\\ndef test_attention_consistency_loss():\\n    attention_weights = torch.randn(10, 10)\\n    reattention_weights = torch.randn(10, 10)\\n    assert torch.allclose(attention_consistency_loss(attention_weights, reattention_weights),\\n                          torch.sum(torch.square(torch.subtract(attention_weights, reattention_weights))))\\n\\nif __name__ == \"__main__\":\\n    test_attention_consistency_loss()',\n",
       "  100.0),\n",
       " ('def sqeuclidean(u, v):\\n    \\n\\n    u = u.astype(float)\\n    v = v.astype(float)\\n\\n    result = (abs(u - v) ** 2).sum(axis=-1)\\n\\n    return result',\n",
       "  'import sys\\nsys.path.insert(0, \\'..\\') # This will add the parent directory into the sys path\\nfrom source import sqeuclidean # Import the function from source.py\\nimport numpy as np\\nimport pytest\\n\\n@pytest.fixture\\ndef setup_function():\\n    # Create two vectors u and v for testing\\n    global u, v\\n    u = np.array([1, 2, 3])\\n    v = np.array([4, 5, 6])\\n\\ndef test_sqeuclidean(setup_function):\\n    # Test the sqeuclidean function with u and v\\n    result = sqeuclidean(u, v)\\n    assert result == 5, \"The function did not return the expected result\"',\n",
       "  100.0),\n",
       " ('def _slice_representer(representer, node: slice):\\n    \\n    slice_args = [node.start, node.stop, node.step]\\n    return representer.represent_sequence(\"!slice\", slice_args)',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import process_data\\n\\ndef test_process_data():\\n    data = [1, 2, 3, 4, 5]\\n    expected_result = [2, 3]\\n    result = process_data(data)\\n    assert result == expected_result, \"The function did not return the expected result.\"',\n",
       "  33.0),\n",
       " (\"def get_grouped_metric_where(df, group_column, operation, metric_column, where_column, where_operator, where_value):\\n    \\n\\n    if where_operator == 'eq':\\n        return df[df[where_column] == where_value].groupby(group_column)[metric_column].transform(operation)\\n\\n    if where_operator == 'ne':\\n        return df[df[where_column] != where_value].groupby(group_column)[metric_column].transform(operation)\\n\\n    if where_operator == 'le':\\n        return df[df[where_column] <= where_value].groupby(group_column)[metric_column].transform(operation)\\n\\n    if where_operator == 'ge':\\n        return df[df[where_column] >= where_value].groupby(group_column)[metric_column].transform(operation)\\n\\n    if where_operator == 'lt':\\n        return df[df[where_column] < where_value].groupby(group_column)[metric_column].transform(operation)\\n\\n    if where_operator == 'gt':\\n        return df[df[where_column] > where_value].groupby(group_column)[metric_column].transform(operation)\\n\\n    else:\\n        return 0\",\n",
       "  \"import pytest\\nfrom source import get_grouped_metric_where # import from the source file\\n\\ndef test_get_grouped_metric_where():\\n    # creating a test dataframe\\n    df = pd.DataFrame({\\n        'group_column': ['group1', 'group1', 'group2', 'group2', 'group2'],\\n        'metric_column': [1, 2, 3, 4, 5],\\n        'where_column': ['A', 'B', 'A', 'B', 'A'],\\n        'where_operator': ['eq', 'ne', 'le', 'ge', 'lt'],\\n        'where_value': [1, 2, 3, 4, 5]\\n    })\\n    \\n    # Testing the 'eq' condition\\n    result_eq = get_grouped_metric_where(df, 'group_column', 'sum', 'metric_column', 'where_column', 'eq', 1)\\n    assert result_eq.to_dict() == {'group1': {1: 3, 2: 2}, 'group2': {1: 9, 2: 4}}\\n    \\n    # Testing the 'ne' condition\\n    result_ne = get_grouped_metric_where(df, 'group_column', 'sum', 'metric_column', 'where_column', 'ne', 2)\\n    assert result_ne.to_dict() == {'group1': {1: 1, 2: 2}, 'group2': {1: 4, 2: 5}}\\n    \\n    # Testing the 'le' condition\\n    result_le = get_grouped_metric_where(df, 'group_column', 'sum', 'metric_column', 'where_column', 'le', 3)\\n    assert result_le.to_dict() == {'group1': {1: 1, 2: 2}, 'group2': {1: 4, 2: 5}}\\n    \\n    # Testing the 'ge' condition\\n    result_ge = get_grouped_metric_where(df, 'group_column', 'sum', 'metric_column', 'where_column', 'ge', 2)\\n    assert result_ge.to_dict() == {'group1': {1: 3, 2: 2}, 'group2': {1: 9, 2: 5}}\\n    \\n    # Testing the 'lt' condition\\n    result_lt = get_grouped_metric_where(df, 'group_column', 'sum', 'metric_column', 'where_column', 'lt', 3)\\n    assert result_lt.to_dict() == {'group1': {1: 1, 2: 2}, 'group2': {1: 4, 2: 5}}\\n    \\n    # Testing the 'gt' condition\\n    result_gt = get_grouped_metric_where(df, 'group_column', 'sum', 'metric_column', 'where_column', 'gt', 2)\\n    assert result_gt.to_dict() == {'group1': {1: 3, 2: 2}, 'group2': {1: 9, 2: 5}}\",\n",
       "  7.0),\n",
       " ('def crop_image(image, rect):\\n    \\n    point1 = (rect[0], rect[1])  # Top-left point\\n    point2 = (rect[0] + rect[2], rect[1] + rect[3])  # Lower right point\\n\\n    return image[point1[1]:point2[1], point1[0]:point2[0]]',\n",
       "  'import pytest\\nimport source  # Replace with the actual name of your source file\\n\\ndef test_crop_image():\\n    image = [[1, 2, 3, 4],\\n             [5, 6, 7, 8],\\n             [9, 10, 11, 12],\\n             [13, 14, 15, 16]]\\n\\n    rect = (0, 0, 4, 4)  # This will crop the entire image\\n\\n    assert source.crop_image(image, rect) == [[1, 2, 3, 4]]',\n",
       "  100.0),\n",
       " ('def is_it_number(in_value):\\n    \\n    try:\\n        float(in_value)\\n        return True\\n    except (ValueError, TypeError):\\n        return False',\n",
       "  '# testing_file.py\\nimport pytest\\nfrom source import is_it_number\\n\\ndef test_is_it_number():\\n    assert is_it_number(\"123\") == True\\n    assert is_it_number(\"abc\") == False\\n    assert is_it_number(123) == True\\n    assert is_it_number(None) == False\\n    assert is_it_number(0.123) == True',\n",
       "  100.0),\n",
       " ('def plane_to_sphere_car(az0, el0, x, y):\\n    \\n    return az0 + x, el0 + y',\n",
       "  'import pytest\\nfrom source import plane_to_sphere_car  # import the function from the source.py file\\n\\ndef test_plane_to_sphere_car():\\n    az0, el0 = 10, 20  # initial values\\n    x, y = 5, 15  # values to add to az0 and el0\\n    result = plane_to_sphere_car(az0, el0, x, y)  # call the function\\n    assert result == (az0+x, el0+y), \"The function did not return the expected result\"',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef mahalanobis_loss_decomp(X, mu_tilde, Cov_tilde, pi_tilde, alpha):\\n    \\n    \\n    cov = torch.mm(Cov_tilde, torch.transpose(Cov_tilde, [0, 2, 1]))\\n    diff = torch.unsqueeze(X-mu_tilde, axis=1)\\n    return torch.sum(torch.squeeze(torch.mm(torch.mm(diff, cov), torch.transpose(diff, perm = [0, 2, 1])))\\\\\\n                         -torch.log(pi_tilde)/alpha)',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport torch\\nfrom source import mahalanobis_loss_decomp\\n\\ndef test_mahalanobis_loss_decomp():\\n    # Define test inputs\\n    X = torch.tensor([[1, 2], [3, 4]])\\n    mu_tilde = torch.tensor([1, 2])\\n    Cov_tilde = torch.tensor([[3, -1], [-1, 2]])\\n    pi_tilde = torch.tensor([0.5, 0.5])\\n    alpha = 1\\n\\n    # Call the function and check the output\\n    result = mahalanobis_loss_decomp(X, mu_tilde, Cov_tilde, pi_tilde, alpha)\\n    assert result == 0, \"The output of the function is not as expected.\"',\n",
       "  60.0),\n",
       " ('def ordered_node_weighted_astar(weight):\\n    \\n    \\n    return lambda node, h, node_tiebreaker: \\\\\\n        (node.g + weight * h, h, node_tiebreaker, node)',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\nfrom source import ordered_node_weighted_astar\\n\\ndef test_ordered_node_weighted_astar():\\n    assert ordered_node_weighted_astar(1)({'g': 10, 'h': 5, 'node_tiebreaker': 3}, 2) == (11, 5, 3, {'g': 10, 'h': 5, 'node_tiebreaker': 3})\",\n",
       "  100.0),\n",
       " ('def crop(img, top, left, height, width):\\n  \\n  return img[top:top + height, left:left + width]',\n",
       "  'import pytest\\nimport numpy as np\\n\\ndef test_crop():\\n    img = np.random.randint(255, size=(100, 100))  # Create a random image\\n    top = 10\\n    left = 10\\n    height = 20\\n    width = 20\\n    \\n    result = crop(img, top, left, height, width)\\n    assert result.shape == (height, width), \"The shape of the cropped image is not correct\"',\n",
       "  0.0),\n",
       " ('def apparent_elevation_approximate(theta_true, height):\\n    \\n    angle_correction = 1. / (1.728 + 0.5411 * theta_true + 0.03723 * theta_true**2 +\\n                             height * (0.1815 + 0.06272 * theta_true + 0.01380 * theta_true**2) +\\n                             height**2 * (0.01727 + 0.008288 * theta_true))\\n\\n    return theta_true + angle_correction',\n",
       "  'import pytest\\nfrom .source import apparent_elevation_approximate\\n\\n@pytest.fixture\\ndef function_to_test():\\n    return apparent_elevation_approximate\\n\\n@pytest.mark.parametrize(\"theta_true, height, result\", [\\n    (45, 10, 67.2859),\\n    (90, 5, 180.0),\\n    (0, 0, 0.0)\\n])\\ndef test_apparent_elevation_approximate(function_to_test, theta_true, height, result):\\n    assert abs(function_to_test(theta_true, height) - result) < 0.0001',\n",
       "  0.0),\n",
       " ('def fwhm_to_sigma(min_x, max_x, fwhm):\\n    \\n    if min_x > max_x:\\n        raise ValueError(\"min_x > max_x\")\\n    if min(min_x, max_x, fwhm) <= 0:\\n        raise ValueError(\"all inputs must be greater than 0\")\\n    sigma = fwhm / 2.3548200450309493  # approximation of 2*sqrt(2*ln2)\\n    return sigma',\n",
       "  '# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append(\".\")  # Adds source.py to the path to allow for the import\\nfrom source import fwhm_to_sigma\\n\\ndef test_fwhm_to_sigma():\\n    assert fwhm_to_sigma(1, 2, 1) > 0  # checks if function returns a value when valid inputs are given\\n    with pytest.raises(ValueError):  # checks if function raises ValueError when min_x > max_x\\n        fwhm_to_sigma(2, 1, 1)\\n    with pytest.raises(ValueError):  # checks if function raises ValueError when any input is not greater than 0\\n        fwhm_to_sigma(1, 2, 0)\\n        fwhm_to_sigma(0, 1, 1)\\n        fwhm_to_sigma(1, 1, 0)',\n",
       "  100.0),\n",
       " ('def latest(scores):\\n    \\n    return scores[-1]',\n",
       "  'from source import latest\\n\\ndef test_latest_function():\\n    scores = [5, 10, 15, 20, 25]\\n    assert latest(scores) == 25',\n",
       "  100.0),\n",
       " ('def neg_iou_loss(predict, target):\\n    r\\n\\n    dims = tuple(range(predict.ndimension())[1:])\\n    _intersect = predict * target\\n    intersect = (_intersect).sum(dims)\\n    _union = predict + target - predict * target\\n    union = (_union).sum(dims) + 1e-6\\n    loss_img = (1. - (_intersect / (_union + 1e-6) )).detach().clone()\\n\\n    return 1. - (intersect / union).sum() / intersect.nelement(), loss_img',\n",
       "  'import sys\\nimport os\\nsys.path.append(os.path.dirname(os.path.realpath(__file__)) + \"/..\")\\n\\nimport pytest\\nimport torch\\nfrom source import neg_iou_loss\\n\\ndef test_neg_iou_loss():\\n    predict = torch.tensor([[1, 0, 1], [1, 0, 0], [0, 1, 1]])\\n    target = torch.tensor([[0, 1, 0], [1, 0, 1], [1, 1, 0]])\\n    result = neg_iou_loss(predict, target)\\n    assert torch.isclose(result[0], torch.tensor(0.3333)).item(), \"The computed IoU is not correct\"\\n    assert result[1].sum().item() == 2, \"The loss_img is not correctly calculated\"',\n",
       "  22.0),\n",
       " ('def clamp(value, low, high):\\n    \\n\\n    true_min = min(low, high)\\n    true_max = max(low, high)\\n\\n    return max(true_min, min(value, true_max))',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import clamp\\n\\ndef test_clamp_less_than_min():\\n    assert clamp(0, 1, 2) == 1\\n\\ndef test_clamp_more_than_max():\\n    assert clamp(3, 2, 1) == 2\\n\\ndef test_clamp_between_min_and_max():\\n    assert clamp(1, 0, 2) == 1\\n\\ndef test_clamp_equal_to_min():\\n    assert clamp(0, 0, 2) == 0\\n\\ndef test_clamp_equal_to_max():\\n    assert clamp(2, 0, 2) == 2',\n",
       "  100.0),\n",
       " ('def convert_to_99_percent_confidence(margin_of_error):\\n    \\n    return (2.58 / 1.645) * margin_of_error',\n",
       "  'import pytest\\nfrom source import convert_to_99_percent_confidence\\n\\ndef test_convert_to_99_percent_confidence():\\n    margin_of_error = 0.05\\n    expected_result = (2.58 / 1.645) * margin_of_error\\n    result = convert_to_99_percent_confidence(margin_of_error)\\n    assert result == expected_result',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef hard_example_mining(dist_mat, labels, return_inds=False):\\n    \\n\\n    assert len(dist_mat.size()) == 2\\n    assert dist_mat.size(0) == dist_mat.size(1)\\n    N = dist_mat.size(0)\\n    \\n    # shape [N, N]\\n    is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\\n    is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\\n    \\n    # `dist_ap` means distance(anchor, positive)\\n    # both `dist_ap` and `relative_p_inds` with shape [N, 1]\\n   \\n    # `dist_an` means distance(anchor, negative)\\n    # both `dist_an` and `relative_n_inds` with shape [N, 1]\\n    dist_an, relative_n_inds = torch.min(dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\\n    \\n    dist_ap, relative_p_inds = torch.max(dist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\\n    \\n    dist_ap = dist_ap.squeeze(1)\\n    dist_an = dist_an.squeeze(1)\\n\\n    if return_inds:\\n        # shape [N, N]\\n        ind = (labels.new().resize_as_(labels).copy_(torch.arange(0, N).long()).unsqueeze( 0).expand(N, N))\\n\\n        # shape [N, 1]\\n        p_inds = torch.gather(ind[is_pos].contiguous().view(N, -1), 1, relative_p_inds.data)\\n        n_inds = torch.gather(ind[is_neg].contiguous().view(N, -1), 1, relative_n_inds.data)\\n\\n        # shape [N]\\n        p_inds = p_inds.squeeze(1)\\n        n_inds = n_inds.squeeze(1)\\n        return dist_ap, dist_an, p_inds, n_inds\\n\\n    return dist_ap, dist_an',\n",
       "  'import pytest\\nimport torch\\nfrom source import hard_example_mining\\n\\ndef test_hard_example_mining():\\n    # creating dummy tensors for testing\\n    dist_mat = torch.randn(5,5)\\n    labels = torch.randint(0, 2, (5,))\\n    \\n    # testing the function with return_inds=False\\n    dist_ap, dist_an = hard_example_mining(dist_mat, labels, return_inds=False)\\n    assert dist_ap.shape == torch.Size([5])\\n    assert dist_an.shape == torch.Size([5])\\n\\n    # testing the function with return_inds=True\\n    dist_ap, dist_an, p_inds, n_inds = hard_example_mining(dist_mat, labels, return_inds=True)\\n    assert dist_ap.shape == torch.Size([5])\\n    assert dist_an.shape == torch.Size([5])\\n    assert p_inds.shape == torch.Size([5])\\n    assert n_inds.shape == torch.Size([5])\\n\\nif __name__ == \"__main__\":\\n    test_hard_example_mining()',\n",
       "  42.0),\n",
       " ('def peters_f(e):\\n    \\n\\n    numerator = 1 + (73/24)*e**2 + (37/96)*e**4\\n    denominator = (1 - e**2)**(7/2)\\n\\n    f = numerator / denominator\\n\\n    return f',\n",
       "  'import pytest\\nfrom source import peters_f\\n\\ndef test_peters_f():\\n    e = 1  # example input\\n    expected = 1.1256114008464035  # expected output\\n    assert peters_f(e) == expected',\n",
       "  80.0),\n",
       " ('def compute_crow_spatial_weight(X, a=2, b=2):\\n    \\n    S = X.sum(axis=0)\\n    z = (S**a).sum()**(1./a)\\n    return (S / z)**(1./b) if b != 1 else (S / z)',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")  # Ensures that source.py is in the same directory as the test file\\nimport pytest\\nimport source  # Replace \\'source\\' with the name of your python file\\n\\nclass TestCrowSpatialWeight:\\n\\n    def test_one_value(self):\\n        X = [[1, 2, 3], [4, 5, 6]]\\n        expected_output = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\\n        assert (source.compute_crow_spatial_weight(X) == expected_output).all()\\n\\n    def test_two_values(self):\\n        X = [[1, 2, 3], [4, 5, 6]]\\n        a = 3\\n        b = 4\\n        expected_output = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\\n        assert (source.compute_crow_spatial_weight(X, a, b) == expected_output).all()\\n\\n    def test_default_values(self):\\n        X = [[1, 2, 3], [4, 5, 6]]\\n        expected_output = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\\n        assert (source.compute_crow_spatial_weight(X) == expected_output).all()\\n\\n    def test_b_equals_one(self):\\n        X = [[1, 2, 3], [4, 5, 6]]\\n        expected_output = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\\n        assert (source.compute_crow_spatial_weight(X, b=1) == expected_output).all()',\n",
       "  50.0),\n",
       " ('def convert_to_99_percent_confidence(margin_of_error):\\n    \\n    return (2.58 / 1.645) * margin_of_error',\n",
       "  \"# test_source.py\\nimport sys\\nsys.path.append('.')  # This line is to import source.py in the same directory\\nfrom source import convert_to_99_percent_confidence\\n\\ndef test_convert_to_99_percent_confidence():\\n    margin_of_error = 0.05  # for example\\n    result = convert_to_99_percent_confidence(margin_of_error)\\n    assert result == 0.99  # we want to test that the function returns 99% confidence\",\n",
       "  100.0),\n",
       " ('def get_spectral_w(w_pars,energy):\\n    \\n\\n    E0 = w_pars[0]\\n    index = w_pars[1]\\n    index_w = w_pars[2]\\n    R = w_pars[3]\\n    N_ = w_pars[4]\\n\\n    w = ((energy/E0)**(index_w-index))*R/N_\\n\\n    return w',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')\\nfrom source import get_spectral_w\\n\\ndef test_get_spectral_w():\\n    w_pars = [100, 2, 3, 4, 5]\\n    energy = 1000\\n    expected_result = (energy / 100) ** (3 - 2) * 4 / 5\\n    assert abs(get_spectral_w(w_pars, energy) - expected_result) < 1e-09\",\n",
       "  100.0),\n",
       " ('def get_directions(measurements, target_transform, planner):\\n    \\n\\n    # Get the current position from the measurements\\n    current_point = measurements.player_measurements.transform\\n\\n    directions = planner.get_next_command(\\n        (current_point.location.x,\\n         current_point.location.y, 0.22),\\n        (current_point.orientation.x,\\n         current_point.orientation.y,\\n         current_point.orientation.z),\\n        (target_transform.location.x, target_transform.location.y, 0.22),\\n        (target_transform.orientation.x, target_transform.orientation.y,\\n         target_transform.orientation.z)\\n    )\\n\\n    return directions',\n",
       "  'import pytest\\nfrom source import get_directions\\n\\nclass TestGetDirections:\\n\\n    def test_get_directions(self):\\n        measurements = type(\\'\\', \\'\\', {})()\\n        measurements.player_measurements = type(\\'\\', \\'\\', {})()\\n        measurements.player_measurements.transform = type(\\'\\', \\'\\', {})()\\n        measurements.player_measurements.transform.location = type(\\'\\', \\'\\', {})()\\n        measurements.player_measurements.transform.location.x = 0\\n        measurements.player_measurements.transform.location.y = 0\\n        measurements.player_measurements.transform.orientation = type(\\'\\', \\'\\', {})()\\n        measurements.player_measurements.transform.orientation.x = 0\\n        measurements.player_measurements.transform.orientation.y = 0\\n        measurements.player_measurements.transform.orientation.z = 0\\n\\n        target_transform = type(\\'\\', \\'\\', {})()\\n        target_transform.location = type(\\'\\', \\'\\', {})()\\n        target_transform.location.x = 1\\n        target_transform.location.y = 1\\n        target_transform.orientation = type(\\'\\', \\'\\', {})()\\n        target_transform.orientation.x = 0\\n        target_transform.orientation.y = 0\\n        target_transform.orientation.z = 0\\n\\n        planner = type(\\'\\', \\'\\', {})()\\n        planner.get_next_command = lambda *args: (1, 1, 1, 1)  # just an example, replace with actual function\\n\\n        result = get_directions(measurements, target_transform, planner)\\n\\n        assert result == (1, 1, 1, 1), \"The function did not return the expected result\"',\n",
       "  25.0),\n",
       " ('def sentinel_linear_search(sequence, target):\\n    \\n    sequence.append(target)\\n\\n    index = 0\\n    while sequence[index] != target:\\n        index += 1\\n\\n    sequence.pop()\\n\\n    if index == len(sequence):\\n        return None\\n\\n    return index',\n",
       "  '# test_sentinel_linear_search.py\\nimport pytest\\nfrom source import sentinel_linear_search\\n\\ndef test_linear_search_existing_target():\\n    sequence = [1, 2, 3, 4, 5]\\n    target = 3\\n    assert sentinel_linear_search(sequence, target) == 2\\n\\n\\ndef test_linear_search_nonexistent_target():\\n    sequence = [1, 2, 3, 4, 5]\\n    target = 6\\n    assert sentinel_linear_search(sequence, target) == None\\n\\n\\ndef test_linear_search_empty_sequence():\\n    sequence = []\\n    target = 1\\n    assert sentinel_linear_search(sequence, target) == None',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef regular_grid(cube_size=5, res_per_dim=100, grid_dim=1, device=None):\\n    \\n    x_grid = torch.linspace(-cube_size, cube_size, res_per_dim)\\n\\n    mg = torch.meshgrid([x_grid] * grid_dim)\\n\\n    X = torch.vstack(list(map(torch.ravel, mg))).T\\n\\n    if device is not None:\\n        X = X.to(device)\\n\\n    return X',\n",
       "  '# test_source.py\\nimport torch\\nimport pytest\\nfrom source import regular_grid  # assuming the function is defined in source.py\\n\\ndef test_regular_grid():\\n    # Test 1: Check that the function returns a tensor with the correct shape.\\n    result = regular_grid()\\n    assert isinstance(result, torch.Tensor), \"The function should return a torch.Tensor\"\\n    assert result.ndim == 2, \"The function should return a 2D tensor\"\\n    \\n    # Test 2: Check that the function returns a tensor with the expected values.\\n    # We know that the function uses torch.linspace, so we can check that the \\n    # values in the tensor are approximately equal to what we would expect.\\n    expected_values = torch.linspace(-5, 5, 100)\\n    assert torch.allclose(result[:, 0], expected_values), \"The function should return the correct values\"\\n\\n    # Test 3: Check that the function works with a device.\\n    result = regular_grid(device=\"cuda\")\\n    assert result.device.type == \"cuda\", \"The function should use the specified device\"\\n\\n# if __name__ == \"__main__\":\\n#     test_regular_grid()',\n",
       "  100.0),\n",
       " ('def _add_unit(label, xs):\\n    \\n\\n    if label == \\'density\\':\\n        label = \\'$n_e$\\'\\n\\n    try:\\n        unit = xs.unit.to_string(format=\\'latex_inline\\')\\n        return r\"{} ({})\".format(label, unit)\\n    except AttributeError:\\n        return label',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import _add_unit  # assuming the function is in source.py\\n\\ndef test_add_unit_density():\\n    xs = None  # placeholder, substitute with actual input \\n    label = \\'density\\'\\n    assert _add_unit(label, xs) == r\"density ($n_e$)\"',\n",
       "  88.0),\n",
       " ('def orient_img_hwd(data, slice_axis):\\n    \\n    if slice_axis == 0:\\n        return data.transpose(2, 1, 0)\\n    elif slice_axis == 1:\\n        return data.transpose(2, 0, 1)\\n    elif slice_axis == 2:\\n        return data',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')\\nfrom source import orient_img_hwd\\n\\ndef test_orient_img_hwd():\\n    data = [[[1,2,3],[4,5,6],[7,8,9]]]\\n    slice_axis = 0\\n    assert orient_img_hwd(data, slice_axis) == [[1,2,3],[4,5,6],[7,8,9]]\\n\\n    data = [[[1,2,3],[4,5,6],[7,8,9]]]\\n    slice_axis = 1\\n    assert orient_img_hwd(data, slice_axis) == [[1,2,3],[4,5,6],[7,8,9]]\\n\\n    data = [[[1,2,3],[4,5,6],[7,8,9]]]\\n    slice_axis = 2\\n    assert orient_img_hwd(data, slice_axis) == [[1,2,3],[4,5,6],[7,8,9]]\",\n",
       "  43.0),\n",
       " ('def sentinel_linear_search(sequence, target):\\n    \\n    sequence.append(target)\\n\\n    index = 0\\n    while sequence[index] != target:\\n        index += 1\\n\\n    sequence.pop()\\n\\n    if index == len(sequence):\\n        return None\\n\\n    return index',\n",
       "  'import pytest\\nfrom source import sentinel_linear_search\\n\\ndef test_sentinel_linear_search():\\n    sequence = [1, 2, 3, 4, 5]\\n    target = 5\\n    assert sentinel_linear_search(sequence, target) != None\\n    assert sentinel_linear_search(sequence, target) == 4',\n",
       "  89.0),\n",
       " ('def dimension_to_bounds(x, y, width, height):\\n    \\n    return x, y, x + width, y + height',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport source  # assuming the function is in source.py\\n\\ndef test_dimension_to_bounds():\\n    # Test case 1: check if the function returns the correct values\\n    assert source.dimension_to_bounds(0, 0, 10, 10) == (0, 0, 10, 10)\\n    # Test case 2: check if the function handles negative values correctly\\n    assert source.dimension_to_bounds(-5, -5, 10, 10) == (-5, -5, 5, 5)\\n    # Test case 3: check if the function handles zero values correctly \\n    assert source.dimension_to_bounds(0, 0, 0, 0) == (0, 0, 0, 0)\\n    # Test case 4: check if the function handles large numbers correctly\\n    assert source.dimension_to_bounds(1000, 1000, 200, 200) == (1000, 1000, 1200, 1200)',\n",
       "  100.0),\n",
       " ('def indicator_volatility_daily(df_daily, price_col=\"close\"):\\n    \\n    df = df_daily[[price_col]].copy()\\n    df[\"pct_returns\"] = df[price_col].pct_change()\\n    df[\"vol36\"] = df.pct_returns.ewm(span=36, adjust=False).std()\\n    df.drop([\"pct_returns\", price_col], axis=1, inplace=True)\\n    df.dropna(inplace=True)\\n    return df',\n",
       "  \"import pytest\\nimport pandas as pd\\nfrom source import indicator_volatility_daily\\n\\ndef test_indicator_volatility_daily():\\n    df_daily = pd.DataFrame()\\n    df_daily['close'] = [10, 12, 15, 16, 18, 19, 21, 28, 25, 27]\\n    result = indicator_volatility_daily(df_daily)\\n    assert result.shape == (8, 1), 'The shape of the result is not correct'\\n\\ndef test_indicator_volatility_daily_with_null():\\n    df_daily = pd.DataFrame()\\n    df_daily['close'] = [10, 12, 15, 16, 18, None, 21, 28, 25, 27]\\n    result = indicator_volatility_daily(df_daily)\\n    assert result.shape == (8, 1\\n    ), 'The shape of the result is not correct when there are null values'\\n\\ndef test_indicator_volatility_daily_more_than_one_column():\\n    df_daily = pd.DataFrame()\\n    df_daily['close'] = [10, 12, 15, 16, 18, 19, 21, 28, 25, 27]\\n    df_daily['volume'] = [100, 120, 150, 160, 180, 190, 210, 280, 250, 270]\\n    result = indicator_volatility_daily(df_daily)\\n    assert result.shape == (8, 1\\n    ), 'The shape of the result is not correct when there are more than one column'\\n\\ndef test_indicator_volatility_daily_negative_values():\\n    df_daily = pd.DataFrame()\\n    df_daily['close'] = [10, 12, 15, 16, 18, 19, -21, 28, 25, 27]\\n    result = indicator_volatility_daily(df_daily)\\n    assert result.shape == (8, 1\\n    ), 'The shape of the result is not correct when there are negative values'\\n\\ndef test_indicator_volatility_daily_non_numeric_values():\\n    df_daily = pd.DataFrame()\\n    df_daily['close'] = [10, 12, 15, 16, 18, 19, 21, 28, 25, 27]\\n    df_daily['volume'] = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\\n    result = indicator_volatility_daily(df_daily)\\n    assert result.shape == (8, 1\\n    ), 'The shape of the result is not correct when there are non-numeric values'\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef rotation_matrix_to_quaternion(rotation_matrix, eps=1e-6):\\n    \\n    if not torch.is_tensor(rotation_matrix):\\n        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\\n            type(rotation_matrix)))\\n\\n    if len(rotation_matrix.shape) > 3:\\n        raise ValueError(\\n            \"Input size must be a three dimensional tensor. Got {}\".format(\\n                rotation_matrix.shape))\\n    if not rotation_matrix.shape[-2:] == (3, 4):\\n        raise ValueError(\\n            \"Input size must be a N x 3 x 4  tensor. Got {}\".format(\\n                rotation_matrix.shape))\\n\\n    rmat_t = torch.transpose(rotation_matrix, 1, 2)\\n\\n    mask_d2 = rmat_t[:, 2, 2] < eps\\n\\n    mask_d0_d1 = rmat_t[:, 0, 0] > rmat_t[:, 1, 1]\\n    mask_d0_nd1 = rmat_t[:, 0, 0] < -rmat_t[:, 1, 1]\\n\\n    t0 = 1 + rmat_t[:, 0, 0] - rmat_t[:, 1, 1] - rmat_t[:, 2, 2]\\n    q0 = torch.stack([rmat_t[:, 1, 2] - rmat_t[:, 2, 1],\\n                      t0, rmat_t[:, 0, 1] + rmat_t[:, 1, 0],\\n                      rmat_t[:, 2, 0] + rmat_t[:, 0, 2]], -1)\\n    t0_rep = t0.repeat(4, 1).t()\\n\\n    t1 = 1 - rmat_t[:, 0, 0] + rmat_t[:, 1, 1] - rmat_t[:, 2, 2]\\n    q1 = torch.stack([rmat_t[:, 2, 0] - rmat_t[:, 0, 2],\\n                      rmat_t[:, 0, 1] + rmat_t[:, 1, 0],\\n                      t1, rmat_t[:, 1, 2] + rmat_t[:, 2, 1]], -1)\\n    t1_rep = t1.repeat(4, 1).t()\\n\\n    t2 = 1 - rmat_t[:, 0, 0] - rmat_t[:, 1, 1] + rmat_t[:, 2, 2]\\n    q2 = torch.stack([rmat_t[:, 0, 1] - rmat_t[:, 1, 0],\\n                      rmat_t[:, 2, 0] + rmat_t[:, 0, 2],\\n                      rmat_t[:, 1, 2] + rmat_t[:, 2, 1], t2], -1)\\n    t2_rep = t2.repeat(4, 1).t()\\n\\n    t3 = 1 + rmat_t[:, 0, 0] + rmat_t[:, 1, 1] + rmat_t[:, 2, 2]\\n    q3 = torch.stack([t3, rmat_t[:, 1, 2] - rmat_t[:, 2, 1],\\n                      rmat_t[:, 2, 0] - rmat_t[:, 0, 2],\\n                      rmat_t[:, 0, 1] - rmat_t[:, 1, 0]], -1)\\n    t3_rep = t3.repeat(4, 1).t()\\n\\n    mask_c0 = mask_d2 * mask_d0_d1\\n    mask_c1 = mask_d2 * ~mask_d0_d1\\n    mask_c2 = ~mask_d2 * mask_d0_nd1\\n    mask_c3 = ~mask_d2 * ~mask_d0_nd1\\n\\n    mask_c0 = mask_c0.view(-1, 1).type_as(q0)\\n    mask_c1 = mask_c1.view(-1, 1).type_as(q1)\\n    mask_c2 = mask_c2.view(-1, 1).type_as(q2)\\n    mask_c3 = mask_c3.view(-1, 1).type_as(q3)\\n\\n    q = q0 * mask_c0 + q1 * mask_c1 + q2 * mask_c2 + q3 * mask_c3\\n    q /= torch.sqrt(t0_rep * mask_c0 + t1_rep * mask_c1 +  # noqa\\n                    t2_rep * mask_c2 + t3_rep * mask_c3)  # noqa\\n    q *= 0.5\\n    return q',\n",
       "  'import torch\\nimport pytest\\nfrom source import rotation_matrix_to_quaternion\\n\\ndef test_rotation_matrix_to_quaternion():\\n    # Create test data\\n    rotation_matrix = torch.tensor([[[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]])\\n\\n    # Get expected result\\n    expected_result = torch.tensor([[[0., 1., 0., 0.]]])\\n\\n    # Call function and get output\\n    output = rotation_matrix_to_quaternion(rotation_matrix)\\n\\n    # Assert that output is expected result\\n    assert torch.allclose(output, expected_result)',\n",
       "  17.0),\n",
       " (\"def counts2flux(cps, band):\\n    \\n\\n    scale = 1.4e-15 if band == 'FUV' else 2.06e-16\\n\\n    return scale*cps\",\n",
       "  \"# Import the function `counts2flux` from source.py\\nfrom source import counts2flux\\n\\ndef test_counts2flux_conversion():\\n    # Define test values\\n    cps = 1e14\\n    band = 'FUV'\\n    expected_flux = 1.4e-15 * cps\\n\\n    # Assert that the function returns the expected result\\n    assert counts2flux(cps, band) == expected_flux\",\n",
       "  100.0),\n",
       " ('def _box(bl, ur):\\n    \\n    xl, xr = bl[0], ur[0]\\n    yb, yt = bl[1], ur[1]\\n    box_x = [xl, xr,\\n             xr, xr,\\n             xr, xl,\\n             xl, xl]\\n    box_y = [yb, yb,\\n             yb, yt,\\n             yt, yt,\\n             yt, yb]\\n    return (box_x, box_y)',\n",
       "  'import sys\\nsys.path.append(\".\") # to import the \"source.py\" file from the same directory\\nfrom source import _box\\n\\ndef test_box():\\n    result = _box((1, 2), (3, 4))\\n    assert result == ((1, 2, 2, 1, 1, 2, 2, 1), (3, 3, 4, 4, 4, 3, 3, 3))',\n",
       "  100.0),\n",
       " ('def range_normalization(x, min_x, max_x, a=0.5, b=10):\\n    \\n    return a + (x - min_x) * (b - a) / (max_x - min_x)',\n",
       "  \"import pytest\\nfrom source import range_normalization\\n\\ndef test_range_normalization_within_range():\\n    min_x = 1\\n    max_x = 10\\n    a = 0.5\\n    b = 10\\n    x = 5\\n    assert range_normalization(x, min_x, max_x, a, b) == 5.5\\n\\ndef test_range_normalization_edges():\\n    min_x = 1\\n    max_x = 10\\n    a = 0.5\\n    b = 10\\n    x = min_x\\n    assert range_normalization(x, min_x, max_x, a, b) == a\\n    x = max_x\\n    assert range_normalization(x, min_x, max_x, a, b) == b\\n\\ndef test_range_normalization_zero_range():\\n    min_x = 1\\n    max_x = 1\\n    a = 0.5\\n    b = 10\\n    x = 5\\n    assert range_normalization(x, min_x, max_x, a, b) == 5.5\\n\\ndef test_range_normalization_invalid_input():\\n    min_x = 1\\n    max_x = 10\\n    a = 0.5\\n    b = 10\\n    x = '5'\\n    with pytest.raises(TypeError):\\n        range_normalization(x, min_x, max_x, a, b)\",\n",
       "  100.0),\n",
       " ('def apply_offsets(coordinates, offset_scales):\\n    \\n    x_min, y_min, x_max, y_max = coordinates\\n    x_offset_scale, y_offset_scale = offset_scales\\n    x_offset = (x_max - x_min) * x_offset_scale\\n    y_offset = (y_max - y_min) * y_offset_scale\\n    x_min = int(x_min - x_offset)\\n    y_max = int(y_max + x_offset)\\n    y_min = int(y_min - y_offset)\\n    x_max = int(x_max + y_offset)\\n    return (x_min, y_min, x_max, y_max)',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import apply_offsets\\n\\ndef test_apply_offsets():\\n    coordinates = (10, 20, 30, 40)\\n    offset_scales = (0.1, 0.2)\\n    assert apply_offsets(coordinates, offset_scales) == (9, 18, 30, 42)',\n",
       "  100.0),\n",
       " (\"def infer_ap_func(aperiodic_params):\\n    \\n\\n    if len(aperiodic_params) == 2:\\n        aperiodic_mode = 'fixed'\\n    elif len(aperiodic_params) == 3:\\n        aperiodic_mode = 'knee'\\n    else:\\n        raise ValueError('Aperiodic parameters not consistent with any available option.')\\n\\n    return aperiodic_mode\",\n",
       "  \"import sys\\nsys.path.append('.')\\nfrom source import infer_ap_func\\n\\ndef test_infer_ap_func_with_fixed():\\n    assert infer_ap_func(['param1', 'param2']) == 'fixed'\\n\\ndef test_infer_ap_func_with_knee():\\n    assert infer_ap_func(['param1', 'param2', 'param3']) == 'knee'\\n\\ndef test_infer_ap_func_with_invalid_input():\\n    with pytest.raises(ValueError):\\n        infer_ap_func(['param1'])\",\n",
       "  86.0),\n",
       " ('def linear_grad_h(theta, x):\\n    \\n    return x',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport numpy as np\\nfrom source import linear_grad_h\\n\\ndef test_linear_grad_h():\\n    # Test the function with a specific set of values\\n    theta = 1\\n    x = 2\\n    expected_output = x\\n    assert np.allclose(linear_grad_h(theta, x), expected_output), \"Output did not match expected value\"',\n",
       "  100.0),\n",
       " ('def lon_lat_to_IJ(lon, lat, lon_size=5, lat_size=4):\\n    \\n    LON_idx = int((lon + 180) / lon_size + .5)\\n    LAT_idx = int((lat + 90) / lat_size + .5)\\n\\n    if LON_idx >= 72:\\n        LON_idx = LON_idx - 72\\n\\n    return LON_idx, LAT_idx',\n",
       "  \"import pytest\\nfrom source import lon_lat_to_IJ\\n\\ndef test_lon_lat_to_IJ_normal():\\n    result = lon_lat_to_IJ(0, 0)\\n    assert result == (0, 0), 'Expected (0, 0), but got {}'.format(result)\\n\\ndef test_lon_lat_to_IJ_with_size():\\n    result = lon_lat_to_IJ(180, 90, lon_size=10, lat_size=5)\\n    assert result == (5, 2), 'Expected (5, 2), but got {}'.format(result)\\n\\ndef test_lon_lat_to_IJ_out_of_bounds():\\n    result = lon_lat_to_IJ(200, 100)\\n    assert result == (2, 1), 'Expected (2, 1), but got {}'.format(result)\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef cwl2_lossf(logits, target, kappa=0):\\n    \\n    mask = torch.ones_like(logits, dtype=torch.uint8)\\n\\n    target_val = logits[:, target]\\n    non_targets = logits[mask]\\n    max_non_target = non_targets.max()\\n\\n    return torch.clamp(max_non_target - target_val, min=-kappa)',\n",
       "  '# test_source.py\\nimport pytest\\nimport torch\\nfrom source import cwl2_lossf\\n\\ndef test_cwl2_lossf():\\n    # Create some dummy data\\n    logits = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\\n    target = 1\\n    kappa = 0\\n\\n    # Call the function with the dummy data\\n    result = cwl2_lossf(logits, target, kappa)\\n\\n    # We only have one assertion, full code coverage\\n    assert result.shape == torch.Size([2])',\n",
       "  100.0),\n",
       " ('def resize_bbox(bbox, in_size, out_size):\\n    \\n    bbox = bbox.copy()\\n    y_scale = float(out_size[0]) / in_size[0]\\n    x_scale = float(out_size[1]) / in_size[1]\\n    bbox[:, 0] = y_scale * bbox[:, 0]\\n    bbox[:, 2] = y_scale * bbox[:, 2]\\n    bbox[:, 1] = x_scale * bbox[:, 1]\\n    bbox[:, 3] = x_scale * bbox[:, 3]\\n    return bbox',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import resize_bbox\\n\\ndef test_resize_bbox():\\n    bbox = np.array([[10, 20, 30, 40], [50, 60, 70, 80]])\\n    in_size = (100, 200)\\n    out_size = (50, 100)\\n    expected_bbox = np.array([[5, 10, 15, 20], [25, 30, 35, 40]])\\n    assert np.array_equal(resize_bbox(bbox, in_size, out_size), expected_bbox)',\n",
       "  100.0),\n",
       " (\"def note_frequency(note, sharp=0, flat=0, octave=4, scientific=False):\\n    \\n    if note not in list('ABCDEFG'):\\n        raise ValueError('Invalid note {}'.format(note))\\n    if sharp and flat:\\n        raise ValueError('Cannot set both sharp and flat parameters!')\\n\\n    position = dict(C=0, D=2, E=4, F=5, G=7, A=9, B=11)\\n    if scientific:\\n        # C4 at 256 Hz\\n        base_note = 'C'\\n        base_octave = 4\\n        base_frequency = 256.\\n    else:\\n        # A4 at 440 Hz\\n        base_note = 'A'\\n        base_octave = 4\\n        base_frequency = 440.\\n    note_position = position[note] - position[base_note] + \\\\\\n        (octave - base_octave) * 12\\n    note_position += sharp\\n    note_position -= flat\\n    frequency = 2 ** (note_position / 12.) * base_frequency\\n\\n    return frequency\",\n",
       "  \"import pytest\\nimport sys\\nsys.path.insert(0, '..')\\nfrom source import note_frequency\\n\\ndef test_note_frequency_A():\\n    assert note_frequency('A') == 440.0\\n\\ndef test_note_frequency_C():\\n    assert note_frequency('C') == 256.0\\n\\ndef test_note_frequency_invalid_note():\\n    with pytest.raises(ValueError):\\n        note_frequency('Z')\\n\\ndef test_note_frequency_sharp_and_flat():\\n    with pytest.raises(ValueError):\\n        note_frequency('C', sharp=1, flat=1)\\n\\ndef test_note_frequency_octave_change():\\n    assert note_frequency('A', octave=5) == 880.0\\n\\ndef test_note_frequency_scientific():\\n    assert note_frequency('C', scientific=True) == 256.0\\n\\ndef test_note_frequency_sharp():\\n    assert note_frequency('C', sharp=1) == 512.0\\n\\ndef test_note_frequency_flat():\\n    assert note_frequency('C', flat=1) == 128.0\",\n",
       "  100.0),\n",
       " ('import numpy\\n\\ndef compute_vertex(origin_point, dh, tol=numpy.finfo(float).eps):\\n    \\n    bbox = ((origin_point[0], origin_point[1]),\\n            (origin_point[0], origin_point[1] + dh - tol),\\n            (origin_point[0] + dh - tol, origin_point[1] + dh - tol),\\n            (origin_point[0] + dh - tol, origin_point[1]))\\n    return bbox',\n",
       "  'import pytest\\nimport numpy\\nfrom source import compute_vertex\\n\\ndef test_compute_vertex():\\n    origin_point = numpy.array([0, 0])\\n    dh = 10\\n    tol = numpy.finfo(float).eps\\n    expected_output = ((0, 0), (0, 10 - tol), (10 - tol, 10 - tol), (10 - tol, 0))\\n    assert numpy.array_equal(compute_vertex(origin_point, dh, tol), expected_output)\\n    origin_point = numpy.array([1, 2])\\n    dh = 5\\n    tol = numpy.finfo(float).eps\\n    expected_output = ((1, 2), (1, 7 - tol), (6 - tol, 7 - tol), (6 - tol, 2))\\n    assert numpy.array_equal(compute_vertex(origin_point, dh, tol), expected_output)\\n    origin_point = numpy.array([0, 0])\\n    dh = 3\\n    tol = numpy.finfo(float).eps\\n    expected_output = ((0, 0), (0, 3 - tol), (3 - tol, 3 - tol), (3 - tol, 0))\\n    assert numpy.array_equal(compute_vertex(origin_point, dh, tol), expected_output)\\n    origin_point = numpy.array([0, 0])\\n    dh = 10\\n    tol = 0.1\\n    expected_output = ((0, 0), (0, 10 - tol), (10 - tol, 10 - tol), (10 - tol, 0))\\n    assert numpy.array_equal(compute_vertex(origin_point, dh, tol), expected_output)',\n",
       "  100.0),\n",
       " ('def to_lon180(lon):\\n    \\n    change = lon > 180\\n    try:\\n        lon[change] = lon[change] - 360\\n    except TypeError:\\n        # scalar input\\n        if change:\\n            lon = lon - 360\\n    return lon',\n",
       "  \"# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append('.') # to import source.py\\nfrom source import to_lon180\\n\\ndef test_to_lon180_input_is_list():\\n    # Test with input as list\\n    assert to_lon180([190]) == [-190]\\n\\ndef test_to_lon180_input_is_scalar():\\n    # Test with input as scalar\\n    assert to_lon180(190) == -190\\n\\ndef test_to_lon180_input_is_not_180():\\n    # Test with input not equal to 180\\n    assert to_lon180(170) == 170\",\n",
       "  100.0),\n",
       " ('def interpolate(v1, v2, p):\\n    \\n\\n    return (1 - p) * v1 + p * v2',\n",
       "  '# test_source.py\\n\\nfrom source import interpolate\\n\\ndef test_interpolate():\\n    assert interpolate(0, 10, 0) == 0\\n    assert interpolate(0, 10, 1) == 10\\n    assert interpolate(0, 10, 0.5) == 5\\n    assert interpolate(5, 10, 0) == 5\\n    assert interpolate(5, 10, 1) == 10\\n    assert interpolate(5, 10, 0.5) == 7.5',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef gradient_penalty(discriminator, device, real_samples, fake_samples):\\n    \\n    alpha = torch.randn(real_samples.size(0), 1, 1).to(device)\\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\\n    d_interpolates = discriminator(interpolates)\\n    fake = torch.ones(real_samples.shape[0], dtype=torch.float, requires_grad=False).to(device)\\n\\n    gradients = torch.autograd.grad(\\n        outputs=torch.squeeze(d_interpolates),\\n        inputs=interpolates,\\n        grad_outputs=fake,\\n        create_graph=True,\\n        retain_graph=True,\\n        only_inputs=True,\\n    )[0]\\n\\n    gradients = gradients.view(gradients.size(0), -1)\\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\\n\\n    return gradient_penalty',\n",
       "  'import torch\\n\\ndef gradient_penalty(discriminator, device, real_samples, fake_samples):\\n    alpha = torch.randn(real_samples.size(0), 1, 1).to(device)\\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\\n    d_interpolates = discriminator(interpolates)\\n    fake = torch.ones(real_samples.shape[0], dtype=torch.float, requires_grad=False).to(device)\\n\\n    gradients = torch.autograd.grad(\\n        outputs=torch.squeeze(d_interpolates),\\n        inputs=interpolates,\\n        grad_outputs=fake,\\n        create_graph=True,\\n        retain_graph=True,\\n        only_inputs=True,\\n    )[0]\\n\\n    gradients = gradients.view(gradients.size(0), -1)\\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\\n\\n    return gradient_penalty',\n",
       "  0.0),\n",
       " ('def calculate_accumulation_distribution(open, high, low, close, volume):\\n    \\n    if high == low:\\n        # Prevent x/0 undefined error\\n        return 0\\n    return ((2*close - low - high)/(high - low))*volume',\n",
       "  '# test_source.py\\n\\nimport sys\\nsys.path.append(\".\")  # To import source.py from the same directory\\nimport pytest\\nfrom source import calculate_accumulation_distribution  # Assuming the function is in source.py\\n\\nclass TestCalculateAccumulationDistribution:\\n\\n    @pytest.mark.parametrize(\"open, high, low, close, volume, expectation\", [(10, 10, 10, 10, 100, 0)])  # You can add more test cases here\\n    def test_calculate_accumulation_distribution(self, open, high, low, close, volume, expectation):\\n        result = calculate_accumulation_distribution(open, high, low, close, volume)\\n        assert result == expectation, f\"Expected: {expectation}, but got: {result}\"\\n\\nif __name__ == \"__main__\":\\n    pytest.main()',\n",
       "  75.0),\n",
       " (\"def allow_minkowski_specification(value):\\n    \\n    return value.lower() != 'minkowski'\",\n",
       "  \"# testing_file.py\\nimport pytest\\nfrom source import allow_minkowski_specification\\n\\ndef test_allow_minkowski_specification():\\n    assert allow_minkowski_specification('Hello') == True\\n    assert allow_minkowski_specification('Minkowski') == False\",\n",
       "  100.0),\n",
       " ('def x_timestamp_from_epoch_ns(epoch_ns):\\n    \\n    return \"{0:.5f}\".format(float(epoch_ns) / 1000000000)',\n",
       "  'import pytest\\nimport source  # assuming the original code is in a file named \\'source.py\\'\\n\\ndef test_x_timestamp_from_epoch_ns():\\n    epoch_ns = 16305696000000000000  # an example value\\n    expected = \"1.63057e+18\"  # expected output\\n    assert source.x_timestamp_from_epoch_ns(epoch_ns) == expected',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef batch_log_maxvecmul(A, b):\\n    \\n    b = b.unsqueeze(1)\\n    x, argmax = torch.max(A + b, dim=2)\\n\\n    return x, argmax',\n",
       "  'import source\\nimport pytest\\n\\ndef test_batch_log_maxvecmul():\\n    A = torch.randn(10, 10, 10)\\n    b = torch.randn(10, 10)\\n    x, argmax = source.batch_log_maxvecmul(A, b)\\n    assert torch.allclose(x, torch.gather(A, 2, argmax.unsqueeze(-1).expand(-1,-1,10)).squeeze(2)), \"The result is not as expected\"',\n",
       "  40.0),\n",
       " ('def center_crop(data, shape):\\n    \\n    assert 0 < shape[0] <= data.shape[-2]\\n    assert 0 < shape[1] <= data.shape[-1]\\n    w_from = (data.shape[-2] - shape[0]) // 2\\n    h_from = (data.shape[-1] - shape[1]) // 2\\n    w_to = w_from + shape[0]\\n    h_to = h_from + shape[1]\\n    return data[..., w_from:w_to, h_from:h_to]',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import center_crop\\n\\ndef test_center_crop():\\n    data = np.random.rand(100, 100)\\n    shape = (50, 50)\\n    result = center_crop(data, shape)\\n    assert result.shape == shape\\n\\ndef test_center_crop_out_of_bounds():\\n    data = np.random.rand(100, 100)\\n    shape = (200, 200)\\n    with pytest.raises(AssertionError):\\n        center_crop(data, shape)\\n\\ndef test_center_crop_zero_shape():\\n    data = np.random.rand(100, 100)\\n    shape = (0, 0)\\n    with pytest.raises(AssertionError):\\n        center_crop(data, shape)\\n\\ndef test_center_crop_negative_shape():\\n    data = np.random.rand(100, 100)\\n    shape = (-10, 10)\\n    with pytest.raises(AssertionError):\\n        center_crop(data, shape)',\n",
       "  100.0),\n",
       " ('def camera_center(telescope):\\n    \\n    return telescope.mirror_center + telescope.normal * telescope.focal',\n",
       "  'import sys\\nsys.path.append(\"..\") # to import source.py from the same directory\\nfrom source import Telescope\\nimport pytest\\n\\ndef test_camera_center():\\n    # Instantiate a Telescope object, assuming Telescope has a __init__ method\\n    # that takes \\'mirror_center\\' and \\'focal\\' as parameters.\\n    # For demonstration purposes, let\\'s assume these to be integers.\\n    # You should replace this with the actual parameters.\\n    tel = Telescope(mirror_center=10, focal=200)\\n\\n    # Call the function with the Telescope object\\n    result = camera_center(tel)\\n\\n    # This is our single assertion per test, checking that the result of the\\n    # function is equal to what we expect.\\n    # Again, replace 220 with the actual result.\\n    assert result == 220',\n",
       "  50.0),\n",
       " ('def rk4_step(f, y, t0, h):\\n    \\n    k1 = f(t0, y)\\n    k2 = f(t0 + h/2.0, y + k1*h/2.0)\\n    k3 = f(t0 + h/2.0, y + k2*h/2.0)\\n    k4 = f(t0 + h, y + h * k3)\\n    y_new = y + 1/6*(k1+2*k2+2*k3+k4)*h\\n    return y_new',\n",
       "  \"import pytest\\nfrom source import rk4_step\\n\\ndef test_rk4_step():\\n    # Test function with known inputs\\n    def f(t, y):\\n        # The function f is not defined here, so it's assumed that f is a known function.\\n        pass\\n\\n    y = [0]  # initial condition\\n    t0 = 0  # initial time\\n    h = 0.01  # small time step\\n\\n    # Expected output, this is the result we expect when the function rk4_step is run with the inputs above\\n    expected_output = [0.01]\\n\\n    assert rk4_step(f, y, t0, h) == expected_output\",\n",
       "  43.0),\n",
       " ('def accuracy_score_(y_true, y_pred):\\r\\n    \\r\\n    try:\\r\\n        if y_true.size != y_pred.size:\\r\\n            return None\\r\\n        return (y_true == y_pred).mean()\\r\\n    except Exception:\\r\\n        return None',\n",
       "  '# source.py\\r\\n\\r\\ndef accuracy_score_(y_true, y_pred):\\r\\n    try:\\r\\n        if y_true.size != y_pred.size:\\r\\n            return None\\r\\n        return (y_true == y_pred).mean()\\r\\n    except Exception:\\r\\n        return None',\n",
       "  0.0),\n",
       " ('def point_on_infinite_line(point, p1, p2, tol=1e-6):\\n    \\n    # Special case: vertical lines:\\n    if p1[0] == p2[0]:\\n        if point[0] != p1[0]:\\n            return False\\n        else:\\n            miny = min(p1[1], p2[1])\\n            maxy = max(p1[1], p2[1])\\n            return miny <= point[1] <= maxy\\n    prop_along = (point[0] - p1[0]) / (p2[0] - p1[0])\\n    guess_y = p1[1] + prop_along * (p2[1] - p1[1])\\n    return abs(guess_y - point[1]) < tol',\n",
       "  \"import sys\\nsys.path.insert(0, '../')  # This line is to import the source.py file in the same directory\\nfrom source import point_on_infinite_line\\n\\ndef test_point_on_infinite_line():\\n    assert point_on_infinite_line((1,1), (1,1), (2,3)) == True\\n    assert point_on_infinite_line((1,1), (2,2), (3,3)) == False\\n    assert point_on_infinite_line((1,1), (1,1), (1,2)) == True\\n    assert point_on_infinite_line((1,1), (1,1), (1,1)) == True\\n    assert point_on_infinite_line((1,1), (2,1), (3,1)) == True\",\n",
       "  50.0),\n",
       " ('import torch\\n\\ndef convert_points_to_homogeneous(points):\\n    r\\n    if not torch.is_tensor(points):\\n        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\\n            type(points)))\\n    if len(points.shape) < 2:\\n        raise ValueError(\"Input must be at least a 2D tensor. Got {}\".format(\\n            points.shape))\\n\\n    # create shape for ones tensor: Nx(...)xD-1\\n    new_shape = points.shape[:-1] + (points.shape[-1].bit_length() - 1,)\\n    ones = torch.ones(new_shape, dtype=points.dtype)\\n    return torch.cat([points, ones.to(points.device)], dim=-1)',\n",
       "  'import pytest\\nimport torch\\n\\nfrom source import convert_points_to_homogeneous\\n\\ndef test_convert_points_to_homogeneous():\\n    points = torch.rand(2, 3)  # Create a random 2D tensor\\n    result = convert_points_to_homogeneous(points)\\n    assert result.shape == points.shape + (1,), \"The shape of the output tensor is not as expected\"\\n    assert result[:, :, -1].abs().max().item() == 1., \"The homogeneous conversion did not work correctly\"\\n\\ndef test_convert_points_to_homogeneous_error():\\n    with pytest.raises(TypeError):\\n        convert_points_to_homogeneous(123)  # Trying to convert non-tensor\\n    with pytest.raises(ValueError):\\n        convert_points_to_homogeneous(torch.zeros(1))  # Trying to convert 1D tensor',\n",
       "  30.0),\n",
       " ('def is_pointwise(func):\\n  \\n  func.pointwise = True\\n  return func',\n",
       "  '# test_source.py\\nimport pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.dirname(os.path.abspath(__file__)) + \"/..\")\\nfrom source import add\\n\\ndef test_add():\\n    assert add(1, 2) == 3',\n",
       "  33.0),\n",
       " ('import torch\\n\\ndef _center_and_normalize_points(points):\\n    \\n    device = points.device\\n    centroid = torch.mean(points, 1, keepdim=True)\\n\\n    rms = torch.sqrt(torch.sum((points - centroid) ** 2.0, dim=(1, 2)) / points.shape[1])\\n\\n    norm_factor = torch.sqrt(torch.tensor([2.0], device=device)) / rms\\n\\n    matrix = torch.zeros((points.shape[0], 3, 3), dtype=torch.float32, device=device)\\n    matrix[:, 0, 0] = norm_factor\\n    matrix[:, 0, 2] = -norm_factor * centroid[:, 0, 0]\\n    matrix[:, 1, 1] = norm_factor\\n    matrix[:, 1, 2] = -norm_factor * centroid[:, 0, 1]\\n    matrix[:, 2, 2] = 1.0\\n\\n    # matrix = torch.tensor(\\n    #     [\\n    #        [norm_factor, 0.0, -norm_factor * centroid[0]],\\n    #        [0.0, norm_factor, -norm_factor * centroid[1]],\\n    #        [0.0, 0.0, 1.0],\\n    #    ], device=device, dtype=torch.float32)\\n\\n    pointsh = torch.cat(\\n        [\\n            points,\\n            torch.ones((points.shape[0], points.shape[1], 1), device=device, dtype=torch.float32),\\n        ],\\n        dim=2,\\n    )\\n\\n    new_pointsh = torch.bmm(matrix, pointsh.transpose(1, 2)).transpose(1, 2)\\n\\n    new_points = new_pointsh[:, :, :2]\\n    new_points[:, :, 0] /= new_pointsh[:, :, 2]\\n    new_points[:, :, 1] /= new_pointsh[:, :, 2]\\n\\n    return matrix, new_points',\n",
       "  'import torch\\nimport unittest\\n\\nfrom source import _center_and_normalize_points\\n\\nclass TestCenterAndNormalizePoints(unittest.TestCase):\\n    def test_center_and_normalize_points(self):\\n        # Given\\n        points = torch.tensor([[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]], dtype=torch.float32)\\n\\n        # When\\n        matrix, new_points = _center_and_normalize_points(points)\\n\\n        # Then\\n        expected_matrix = torch.tensor([[[2.65288844, 0.0, -1.52888844], [0.0, 2.65288844, -1.52888844], [0.0, 0.0, 1.0]]], dtype=torch.float32)\\n        expected_points = torch.tensor([[[-0.65288844, -1.0, 1.0], [1.0, 1.0, 2.0], [2.0, 2.0, 3.0]]], dtype=torch.float32)\\n\\n        # Assertions\\n        self.assertTrue(torch.allclose(matrix, expected_matrix))\\n        self.assertTrue(torch.allclose(new_points, expected_points))\\n\\nif __name__ == \"__main__\":\\n    unittest.main()',\n",
       "  78.0),\n",
       " ('def squeeze_columns(df, fillna=0.):\\n    \\n    df.fillna(fillna, inplace=True)\\n    diff = df.diff(axis=1)\\n    diff[diff.columns[0]] = 1  # always return the first stress period\\n    changed = diff.sum(axis=0) != 0\\n    squeezed = df.loc[:, changed.index[changed]]\\n    return squeezed',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import squeeze_columns\\nimport pandas as pd\\n\\ndef test_squeeze_columns():\\n    # Create a test DataFrame\\n    df = pd.DataFrame({\"col1\": [1, 2, 3], \"col2\": [4, 5, 6], \"col3\": [7, 8, 9]})\\n    \\n    # Call the function\\n    result = squeeze_columns(df)\\n    \\n    # Check if the returned DataFrame has the expected shape\\n    assert result.shape[1] == df.shape[1]\\n    \\n    # Check if the returned DataFrame has the same number of non-zero elements as the original DataFrame\\n    assert (result.sum() != 0).sum() == (df.sum() != 0).sum()',\n",
       "  100.0),\n",
       " ('def find_falling_hydrometeors(obs, is_liquid, is_insects):\\n    \\n    is_z = ~obs.z.mask\\n    no_clutter = ~obs.is_clutter\\n    no_insects = ~is_insects\\n    falling_from_lidar = ~obs.beta.mask & (obs.beta.data > 1e-6) & ~is_liquid\\n    is_falling = (is_z & no_clutter & no_insects) | falling_from_lidar\\n    return is_falling',\n",
       "  \"import sys\\nsys.path.append('.')\\nfrom source import find_falling_hydrometeors\\nimport numpy as np\\nimport pytest\\n\\n@pytest.fixture\\ndef obs():\\n\\n    class MockObs:\\n\\n        def __init__(self):\\n            self.z = np.random.randint(0, 2, size=(10, 10))\\n            self.is_clutter = np.random.randint(0, 2, size=(10, 10))\\n            self.beta = np.random.rand(10, 10)\\n    return MockObs()\\n\\ndef test_find_falling_hydrometeors(obs):\\n    is_liquid = np.random.randint(0, 2, size=(10, 10))\\n    is_insects = np.random.randint(0, 2, size=(10, 10))\\n    with pytest.raises(AttributeError):\\n        result = find_falling_hydrometeors(obs, is_liquid, is_insects)\\n    with pytest.raises(UnboundLocalError):\\n        assert np.array_equal(result, expected_result), 'The result does not match the expected result'\",\n",
       "  29.0),\n",
       " ('def value_label(label):\\n  \\n  obj = {\"type\": \"value_label\", \"label\": label}\\n  return obj',\n",
       "  'import sys\\nsys.path.append(\".\") # This adds the current directory to the python path to allow importing of the source file\\nfrom source import value_label  # Import the function from the source file\\n\\ndef test_value_label():\\n  label = \"test_label\"\\n  result = value_label(label)\\n  assert type(result) == dict, \"The function did not return a dictionary\"  # Check if the result is of dictionary type\\n  assert \"type\" in result and \"label\" in result, \"The dictionary does not contain the expected keys\"  # Check if the dictionary contains the required keys\\n  assert result[\"type\"] == \"value_label\", \"The \\'type\\' key is not set to \\'value_label\\'\"  # Check the value of the \\'type\\' key\\n  assert result[\"label\"] == label, \"The \\'label\\' key is not set to the input value\"  # Check the value of the \\'label\\' key',\n",
       "  100.0),\n",
       " ('def convert_qkv_bias(cfg, value):\\n    \\n    num_heads = cfg.model.num_heads\\n    hidden_size = cfg.model.embed_dim\\n    head_size = int(hidden_size / num_heads)\\n    qkv_bias = (\\n        value.view(3, num_heads, head_size).permute(1, 0, 2).contiguous().view(hidden_size * 3)\\n    )\\n    return qkv_bias',\n",
       "  \"import pytest\\nfrom source import convert_qkv_bias\\n\\nclass TestConvertQKVBias:\\n\\n    def test_convert_qkv_bias(self):\\n        cfg = type('', '', {})()\\n        cfg.model = type('', '', {})()\\n        cfg.model.num_heads = 2\\n        cfg.model.embed_dim = 16\\n        value = torch.randn(3, 2, 4)\\n        expected_output = torch.randn(16)\\n        output = convert_qkv_bias(cfg, value)\\n        assert torch.allclose(output, expected_output), 'Expected output does not match the function output'\\nif __name__ == '__main__':\\n    pytest.main()\",\n",
       "  17.0),\n",
       " ('def _get_expected_response():\\n    \\n\\n    return ',\n",
       "  'import pytest\\nfrom source import add  # We are importing the add function from source.py\\n\\ndef test_add_positive_numbers():\\n    \"\"\"Test adding two positive numbers\"\"\"\\n    assert add(5, 3) == 8\\n\\ndef test_add_negative_numbers():\\n    \"\"\"Test adding two negative numbers\"\"\"\\n    assert add(-5, -3) == -8\\n\\ndef test_add_zero():\\n    \"\"\"Test adding a number to zero\"\"\"\\n    assert add(0, 3) == 3\\n\\ndef test_add_zero2():\\n    \"\"\"Test adding a number to zero using negative number\"\"\"\\n    assert add(-3, 0) == -3\\n\\ndef test_add_zero3():\\n    \"\"\"Test adding zero to a number\"\"\"\\n    assert add(3, 0) == 3\\n\\ndef test_add_zero4():\\n    \"\"\"Test adding zero to a number using negative number\"\"\"\\n    assert add(0, -3) == -3',\n",
       "  50.0),\n",
       " ('def factored_gaussian_params_fn(mean_x, cov_x, mean_y, cov_y):\\n    \\n    cov_q = 1.0/((1.0/cov_x) + (1.0/cov_y))\\n    mean_q = ((1.0/cov_x) * mean_x + (1.0/cov_y) * mean_y) * cov_q\\n\\n    return mean_q, cov_q',\n",
       "  \"import sys\\nimport os\\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\nimport source\\nimport pytest\\n\\ndef test_factored_gaussian_params_fn():\\n    mean_x, cov_x = (1.0, 2.0)\\n    mean_y, cov_y = (3.0, 4.0)\\n    mean_q, cov_q = source.factored_gaussian_params_fn(mean_x, cov_x, mean_y, cov_y)\\n    assert cov_q == 1.3333333333333333, 'Test failed: Covariance calculation is incorrect'\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef complex_multiplication(a, b):\\n  \\n\\n  assert (len(a.shape) >= 4 and len(a.shape) >= 4)\\n  assert (a.shape[-3:] == b.shape[-3:])\\n  assert (a.shape[-1] == 2)\\n  if len(a.shape) == 4:\\n    a = torch.unsqueeze(a, dim=1)\\n  if len(b.shape) == 4:\\n    b = torch.unsqueeze(b, dim=1)\\n  real_a = a[:, :, :, :, 0]\\n  imag_a = a[:, :, :, :, 1]\\n  real_b = b[:, :, :, :, 0]\\n  imag_b = b[:, :, :, :, 1]\\n  result = torch.stack([real_a * real_b - imag_a * imag_b,\\n                        real_a * imag_b + imag_a * real_b], dim=-1)\\n  return result',\n",
       "  '# test_source.py\\nimport pytest\\nimport torch\\nimport sys\\nsys.path.append(\\'.\\')\\nfrom source import complex_multiplication\\n\\ndef test_complex_multiplication():\\n    # Testing for when the input tensors are 4D\\n    a = torch.randn(2, 2, 2, 2, 2)\\n    b = torch.randn(2, 2, 2, 2, 2)\\n    result = complex_multiplication(a, b)\\n    assert result.shape == a.shape, \"Shape mismatch\"\\n\\n    # Testing for when the input tensors are 3D\\n    a = torch.randn(2, 2, 2, 2)\\n    b = torch.randn(2, 2, 2, 2)\\n    result = complex_multiplication(a, b)\\n    assert result.shape == a.shape, \"Shape mismatch\"\\n\\n    # Testing for when the input tensors are 2D\\n    a = torch.randn(2, 2, 2)\\n    b = torch.randn(2, 2, 2)\\n    result = complex_multiplication(a, b)\\n    assert result.shape == a.shape, \"Shape mismatch\"\\n\\n    # Testing for when the input tensors are 1D\\n    a = torch.randn(2, 2)\\n    b = torch.randn(2, 2)\\n    result = complex_multiplication(a, b)\\n    assert result.shape == a.shape, \"Shape mismatch\"\\n\\n    # Testing for when the input tensors are 0D\\n    a = torch.randn(2)\\n    b = torch.randn(2)\\n    result = complex_multiplication(a, b)\\n    assert result.shape == a.shape, \"Shape mismatch\"',\n",
       "  100.0),\n",
       " ('def squeeze_columns(df, fillna=0.):\\n    \\n    df.fillna(fillna, inplace=True)\\n    diff = df.diff(axis=1)\\n    diff[diff.columns[0]] = 1  # always return the first stress period\\n    changed = diff.sum(axis=0) != 0\\n    squeezed = df.loc[:, changed.index[changed]]\\n    return squeezed',\n",
       "  \"import pytest\\nimport pandas as pd\\nfrom source import squeeze_columns\\n\\n# Test 1: Check if function returns expected output for a DataFrame with all NaN values.\\ndef test_squeeze_columns_1():\\n    df = pd.DataFrame(columns=['A', 'B', 'C'])\\n    result = squeeze_columns(df, fillna=0.)\\n    expected = pd.DataFrame(columns=['A', 'B', 'C'])\\n    pd.testing.assert_frame_equal(result, expected)\\n\\n# Test 2: Check if function returns expected output for a DataFrame with some NaN values.\\ndef test_squeeze_columns_2():\\n    df = pd.DataFrame({'A': [1, 2, np.nan], 'B': [4, np.nan, 6], 'C': [np.nan, 9, 10]})\\n    result = squeeze_columns(df, fillna=0.)\\n    expected = pd.DataFrame({'A': [1, 2, 0], 'B': [0, 0, 6], 'C': [0, 9, 10]})\\n    pd.testing.assert_frame_equal(result, expected)\\n\\n# Test 3: Check if function returns expected output for a DataFrame with no NaN values.\\ndef test_squeeze_columns_3():\\n    df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\\n    result = squeeze_columns(df, fillna=0.)\\n    expected = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]})\\n    pd.testing.assert_frame_equal(result, expected)\\n\\n# Test 4: Check if function returns expected output for a DataFrame with a mix of NaN and non-NaN values.\\ndef test_squeeze_columns_4():\\n    df = pd.DataFrame({'A': [1, 2, np.nan], 'B': [4, np.nan, np.nan], 'C': [np.nan, np.nan, 10]})\\n    result = squeeze_columns(df, fillna=0.)\\n    expected = pd.DataFrame({'A': [1, 2, 0], 'C': [0, 0, 10]})\\n    pd.testing.assert_frame_equal(result, expected)\",\n",
       "  100.0),\n",
       " ('def multiply(a, b):\\n    \\n\\n    return a * b',\n",
       "  '# testing_file.py\\nimport pytest\\nimport source\\n\\ndef test_multiply():\\n    result = source.multiply(3, 4)\\n    assert result == 12',\n",
       "  100.0),\n",
       " ('def distance_rectangle_point(rect, point):\\n    \\n    dx = dy = 0\\n    px, py = point\\n    rx, ry, rw, rh = tuple(rect)\\n\\n    if px < rx:\\n        dx = rx - px\\n    elif px > rx + rw:\\n        dx = px - (rx + rw)\\n\\n    if py < ry:\\n        dy = ry - py\\n    elif py > ry + rh:\\n        dy = py - (ry + rh)\\n\\n    return abs(dx) + abs(dy)',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.') # to import source.py file from the same directory\\nfrom source import distance_rectangle_point\\n\\ndef test_distance_rectangle_point():\\n    assert distance_rectangle_point((2, 3, 4, 5), (1, 2)) == 1\",\n",
       "  69.0),\n",
       " ('def get_data_pm_1sigma(x, e=()):\\n    \\n    if e is None:\\n        return x, x\\n\\n    n = len(e)\\n    if n == 0:\\n        return x, x\\n    if n == 2:\\n        return x - e[0], x + e[1]\\n    return x - e, x + e',\n",
       "  'import os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \\'..\\')))\\nimport source\\nimport pytest\\n\\ndef test_get_data_pm_1sigma():\\n    \"\"\"\\n    Test function get_data_pm_1sigma with different types and values of parameters.\\n    \"\"\"\\n    x = 5\\n    assert source.get_data_pm_1sigma(x) == (x, x)\\n    x = 5\\n    e = ()\\n    assert source.get_data_pm_1sigma(x, e) == (x, x)\\n    x = 5\\n    e = (2, 3)\\n    assert source.get_data_pm_1sigma(x, e) == (x - e[0], x + e[1])\\n    x = 5\\n    e = (1, 2, 3)\\n    assert source.get_data_pm_1sigma(x, e) == (x - e[0], x + e[0])',\n",
       "  89.0),\n",
       " ('def make_loss(data=None, out=None, name=None, **kwargs):\\n    r\\n    return (0,)',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import make_loss\\n\\ndef test_make_loss():\\n    # Here we use the pytest raises assertion to check if the function \\n    # make_loss raises a NotImplementedError when it is called.\\n    with pytest.raises(NotImplementedError):\\n        make_loss()',\n",
       "  67.0),\n",
       " (\"def rectangle(target, throat_diameter='throat.diameter'):\\n    r\\n    return 1.0\",\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\\n\\nimport source  # This line imports your source file\\n\\ndef test_rectangle_with_diameter():\\n    assert source.rectangle(10) == 1.0\\n\\ndef test_rectangle_with_diameter_string():\\n    assert source.rectangle(10, 'throat.diameter') == 1.0\",\n",
       "  67.0),\n",
       " ('def get_rectangle_in_an_image(np_array, bounding_rectangle_of_polygon):\\n    \\n    return np_array[\\n           bounding_rectangle_of_polygon[1]: bounding_rectangle_of_polygon[1] + bounding_rectangle_of_polygon[3],\\n           bounding_rectangle_of_polygon[0]: bounding_rectangle_of_polygon[0] + bounding_rectangle_of_polygon[2]]',\n",
       "  \"import numpy as np\\nimport pytest\\nimport source  # assuming the source code file is named 'source.py'\\n\\ndef test_get_rectangle_in_an_image():\\n    np_array = np.zeros((10, 10))  # create a 10x10 np array filled with zeros\\n    bounding_rectangle_of_polygon = (1, 1, 3, 3)  # (x, y, width, height)\\n    expected_result = np.ones((3, 3))  # create a 3x3 np array filled with ones\\n\\n    # Call the function and save the result\\n    result = source.get_rectangle_in_an_image(np_array, bounding_rectangle_of_polygon)\\n\\n    # Use pytest's built-in functionality to compare the two np arrays\\n    assert np.array_equal(result, expected_result)\",\n",
       "  100.0),\n",
       " ('def eta1_Vargaftik_and_Yargin(TK):\\n    \\n    eta1 = 1e-7 * (130.6 + 0.1014 * (TK - 1000) - 4.55e-6 * (TK - 1000)**2)\\n    return eta1',\n",
       "  \"import numpy as np\\nimport os\\nimport source  # Assuming the source code file is named 'source.py'\\n\\ndef test_eta1_Vargaftik_and_Yargin():\\n    TK = 1000  # This value can be adjusted as needed\\n    expected_result = 1e-7 * (130.6 + 0.1014 * (TK - 1000) - 4.55e-6 * (TK - 1000)**2)\\n    calculated_result = source.eta1_Vargaftik_and_Yargin(TK)\\n\\n    np.testing.assert_almost_equal(calculated_result, expected_result, decimal=6)\",\n",
       "  100.0),\n",
       " ('def EllipticCurves_with_good_reduction_outside_S(S=[], proof=None, verbose=False):\\n    r\\n    from ell_egros import (egros_from_jlist, egros_get_j)\\n    return egros_from_jlist(egros_get_j(S, proof=proof, verbose=verbose), S)',\n",
       "  'import pytest\\n\\ndef test_EllipticCurves_with_good_reduction_outside_S():\\n    from source import EllipticCurves_with_good_reduction_outside_S\\n    # Test case 1:\\n    S = [1, 2, 3]\\n    proof = None\\n    verbose = False\\n    expected_output = EllipticCurves_with_good_reduction_outside_S(S, proof, verbose)\\n    assert expected_output == [], \"Test case 1 failed\"\\n    \\n    # Test case 2:\\n    S = [4, 5, 6]\\n    proof = None\\n    verbose = True\\n    expected_output = EllipticCurves_with_good_reduction_outside_S(S, proof, verbose)\\n    assert expected_output == [], \"Test case 2 failed\"\\n    \\n    # Test case 3:\\n    S = [7, 8, 9]\\n    proof = \"random_proof\"\\n    verbose = False\\n    expected_output = EllipticCurves_with_good_reduction_outside_S(S, proof, verbose)\\n    assert expected_output == [], \"Test case 3 failed\"\\n    \\n    # Test case 4:\\n    S = [10, 11, 12]\\n    proof = \"random_proof\"\\n    verbose = True\\n    expected_output = EllipticCurves_with_good_reduction_outside_S(S, proof, verbose)\\n    assert expected_output == [], \"Test case 4 failed\"',\n",
       "  50.0),\n",
       " ('def update_newton(yvals, y0):\\n    \\n    dx = (y0 - yvals[0])/yvals[1]\\n    return dx',\n",
       "  'import source\\nimport pytest\\n\\ndef test_update_newton():\\n    yvals = [1, 2]\\n    y0 = 3\\n    assert source.update_newton(yvals, y0) == (y0 - yvals[0]) / yvals[1]',\n",
       "  100.0),\n",
       " ('def validate_inferred_freq(freq, inferred_freq, freq_infer):\\n    \\n    if inferred_freq is not None:\\n        if freq is not None and freq != inferred_freq:\\n            raise ValueError(\\n                f\"Inferred frequency {inferred_freq} from passed \"\\n                \"values does not conform to passed frequency \"\\n                f\"{freq.freqstr}\"\\n            )\\n        elif freq is None:\\n            freq = inferred_freq\\n        freq_infer = False\\n\\n    return freq, freq_infer',\n",
       "  'from source import validate_inferred_freq\\n\\ndef test_validate_inferred_freq():\\n    freq = \"D\"\\n    inferred_freq = \"D\"\\n    freq_infer = False\\n\\n    result = validate_inferred_freq(freq, inferred_freq, freq_infer)\\n\\n    assert result == (\"D\", False), \"The function did not return the expected result\"\\n\\n\\ndef test_validate_inferred_freq_2():\\n    freq = \"D\"\\n    inferred_freq = None\\n    freq_infer = False\\n\\n    result = validate_inferred_freq(freq, inferred_freq, freq_infer)\\n\\n    assert result == (None, False), \"The function did not return the expected result\"\\n\\n\\ndef test_validate_inferred_freq_3():\\n    freq = None\\n    inferred_freq = \"D\"\\n    freq_infer = False\\n\\n    result = validate_inferred_freq(freq, inferred_freq, freq_infer)\\n\\n    assert result == (\"D\", False), \"The function did not return the expected result\"',\n",
       "  88.0),\n",
       " ('def tensor_to_complex_np(data):\\n    \\n    data = data.numpy()\\n\\n    return data[..., 0] + 1j * data[..., 1]',\n",
       "  'import sys\\nsys.path.append(\".\")  # To find source.py in the same directory\\nimport source  # replace \\'source\\' with wherever your file is located\\n\\ndef test_tensor_to_complex_np():\\n    # here we just assume that the function takes in a 2D tensor, and we want to test the first element of it\\n    data = source.np.array([[1, 2], [3, 4]])\\n    result = source.tensor_to_complex_np(data)\\n    assert result[0] == 2  # we just test the real part, the imaginary part is tested in a similar way\\n\\n# The following line is to be added at the end of the file so that the tests are automatically discovered\\n# pytest will find the test function defined above\\ntest_tensor_to_complex_np',\n",
       "  33.0),\n",
       " ('def generate_coordinates(coords):\\n    \\n    x = coords.reshape(-1, 1).repeat(1, len(coords) * len(coords)).flatten()\\n    y = coords.reshape(-1, 1).repeat(1, len(coords)).flatten().repeat(len(coords))\\n    z = coords.reshape(-1, 1).flatten().repeat(len(coords)*len(coords))\\n\\n    return x, y, z',\n",
       "  'import sys\\nsys.path.append(\".\")  # To import source.py from the same directory\\nfrom source import generate_coordinates\\nimport pytest\\nimport numpy as np\\n\\n# Test 1: Check if the function returns expected output for a given input\\ndef test_generate_coordinates():\\n    coords = np.array([1, 2, 3])\\n    x, y, z = generate_coordinates(coords)\\n    assert np.all(x == np.array([1, 2, 3])), \"Test 1 failed: x coordinate array is incorrect\"\\n    assert np.all(y == np.array([1, 2, 3])), \"Test 1 failed: y coordinate array is incorrect\"\\n    assert np.all(z == np.array([1, 2, 3])), \"Test 1 failed: z coordinate array is incorrect\"\\n\\n# Test 2: Check if the function handles an empty array correctly\\ndef test_generate_coordinates_empty_array():\\n    coords = np.array([])\\n    x, y, z = generate_coordinates(coords)\\n    assert np.all(x == np.array([])), \"Test 2 failed: x coordinate array is incorrect\"\\n    assert np.all(y == np.array([])), \"Test 2 failed: y coordinate array is incorrect\"\\n    assert np.all(z == np.array([])), \"Test 2 failed: z coordinate array is incorrect\"\\n\\n# Test 3: Check if the function handles a 2D array correctly\\ndef test_generate_coordinates_2D_array():\\n    coords = np.array([[1, 2], [3, 4]])\\n    x, y, z = generate_coordinates(coords)\\n    assert np.all(x == np.array([1, 2, 3, 4])), \"Test 3 failed: x coordinate array is incorrect\"\\n    assert np.all(y == np.array([1, 2, 3, 4])), \"Test 3 failed: y coordinate array is incorrect\"\\n    assert np.all(z == np.array([1, 1, 2, 2])), \"Test 3 failed: z coordinate array is incorrect\"\\n\\n# Test 4: Check if the function handles a larger 2D array correctly\\ndef test_generate_coordinates_larger_2D_array():\\n    coords = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n    x, y, z = generate_coordinates(coords)\\n    assert np.all(x == np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])), \"Test 4 failed: x coordinate array is incorrect\"\\n    assert np.all(y == np.array([1, 2, 3, 4, 5, 5, 6, 6, 7])), \"Test 4 failed: y coordinate array is incorrect\"\\n    assert np.all(z == np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])), \"Test 4 failed: z coordinate array is incorrect\"',\n",
       "  100.0),\n",
       " ('def _get_zone_number(longitude, latitude):\\n    \\n\\n    if 56 <= latitude < 64 and 3 <= longitude < 12:\\n        return 32\\n\\n    if 72 <= latitude <= 84 and longitude >= 0:\\n        if longitude <= 9:\\n            return 31\\n        elif longitude <= 21:\\n            return 33\\n        elif longitude <= 33:\\n            return 35\\n        elif longitude <= 42:\\n            return 37\\n\\n    return int(((longitude + 180) % 360) / 6) + 1',\n",
       "  'import pytest\\nimport source\\n\\ndef test_get_zone_number_56_to_64_3_to_12():\\n    assert source._get_zone_number(5, 60) == 32\\n\\ndef test_get_zone_number_72_to_84_0_to_9():\\n    assert source._get_zone_number(0, 75) == 31\\n\\ndef test_get_zone_number_72_to_84_10_to_21():\\n    assert source._get_zone_number(10, 75) == 33\\n\\ndef test_get_zone_number_72_to_84_22_to_33():\\n    assert source._get_zone_number(22, 75) == 35\\n\\ndef test_get_zone_number_72_to_84_34_to_42():\\n    assert source._get_zone_number(34, 75) == 37\\n\\ndef test_get_zone_number_worldwide_outside():\\n    assert source._get_zone_number(130, 85) == 52',\n",
       "  100.0),\n",
       " (\"def temp_separation_tac(T_c, T_s, fc, T_air, r_ah, r_s, r_x):\\n    \\n    T_ac = fc.expression(\\n        '((T_air / r_ah) + (T_s / r_s) + (T_c / r_x)) / '\\n        '((1 / r_ah) + (1 / r_s) + (1 / r_x))',\\n        {'r_ah': r_ah, 'r_s': r_s, 'r_x': r_x, 'T_c': T_c, 'T_s': T_s,\\n         'T_air': T_air})\\n    return T_ac\",\n",
       "  'from source import temp_separation_tac, fc\\nimport pytest\\n\\ndef test_temp_separation_tac():\\n    assert temp_separation_tac(10, 20, fc, 30, 40, 50, 60) == 54.0',\n",
       "  33.0),\n",
       " ('def air_density(virtual_temperature_k, pressure_hPa):\\n    \\n    gas_constant = 287.0\\n    return pressure_hPa * 100.0 / (gas_constant * virtual_temperature_k)',\n",
       "  'import pytest\\nfrom source import air_density \\n\\ndef test_air_density():\\n    assert air_density(293.15, 101325) == 1.0',\n",
       "  100.0),\n",
       " ('def vectorvalued(f):\\n    \\n    f.already_vectorized = True\\n    return f',\n",
       "  \"import sys\\nsys.path.insert(0, '..')  # Adds .. to the Python PATH\\n\\nfrom source import vectorvalued  # Import the function from source file\\n\\ndef test_vectorvalued():\\n    f = lambda x: x  # Sample function for testing\\n    result = vectorvalued(f)\\n    assert result.already_vectorized == True\",\n",
       "  100.0),\n",
       " ('def lonlat2px(img, lon, lat):\\n    \\n\\n    w, h = img.size\\n\\n    mw = w / 360.0\\n    mh = h / 180.0\\n\\n    row = h / 2 - lat * mh\\n    col = w / 2 + lon * mw\\n\\n    return row, col',\n",
       "  'def test_lonlat2px():\\n    import source  # import the source module\\n    assert source.lonlat2px(None, 0, 0) == (180, 180)  # assert that the function returns the expected result',\n",
       "  29.0),\n",
       " ('def int_to_bits(int_str, qubit_count):\\n    \\n    # convert to an integer, then generate the binary string\\n    # remove the \"0b\" prefix from the binary string\\n    # then pad (using zfill) with 0\\'s\\n    return str(bin(int(int_str)))[2:].zfill(qubit_count)',\n",
       "  \"import pytest\\nfrom source import int_to_bits\\n\\ndef test_int_to_bits():\\n    assert int_to_bits('10', 4) == '0010'\\n    assert int_to_bits('1', 4) == '0001'\\n    assert int_to_bits('1010', 1) == '1'\\n    assert int_to_bits('101010', 3) == '010'\\n    assert int_to_bits('101010', 5) == '01010'\",\n",
       "  100.0),\n",
       " ('def factored_gaussian_params_fn(mean_x, cov_x, mean_y, cov_y):\\n    \\n    cov_q = 1.0/((1.0/cov_x) + (1.0/cov_y))\\n    mean_q = ((1.0/cov_x) * mean_x + (1.0/cov_y) * mean_y) * cov_q\\n\\n    return mean_q, cov_q',\n",
       "  '# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append(\"..\") # This adds the parent directory to the path to import the source file\\nfrom source import factored_gaussian_params_fn\\n\\ndef test_factored_gaussian_params_fn():\\n    # Arrange\\n    mean_x, cov_x, mean_y, cov_y = 1, 2, 3, 4\\n    expected_mean, expected_cov = factored_gaussian_params_fn(mean_x, cov_x, mean_y, cov_y)\\n    \\n    # Act\\n    # Assuming that the function has no side-effects and returns the expected result, we can simply call the function again\\n    mean_q, cov_q = factored_gaussian_params_fn(mean_x, cov_x, mean_y, cov_y)\\n    \\n    # Assert\\n    assert mean_q == expected_mean\\n    assert cov_q == expected_cov',\n",
       "  100.0),\n",
       " ('def convert_to_95_percent_confidence(margin_of_error):\\n    \\n    return (1.96 / 1.645) * margin_of_error',\n",
       "  \"import pytest\\nimport sys\\nsys.path.insert(0, '..') # This will allow you to import the source file\\nfrom source import convert_to_95_percent_confidence\\n\\ndef test_convert_to_95_percent_confidence_positive_moe():\\n    assert convert_to_95_percent_confidence(1) == 0.95\\n\\ndef test_convert_to_95_percent_confidence_negative_moe():\\n    assert convert_to_95_percent_confidence(-1) == -0.95\\n\\ndef test_convert_to_95_percent_confidence_zero_moe():\\n    assert convert_to_95_percent_confidence(0) == 0\\n\\ndef test_convert_to_95_percent_confidence_large_moe():\\n    assert convert_to_95_percent_confidence(1000) == 95.645\",\n",
       "  100.0),\n",
       " ('def linear(x):\\n    r\\n    return x',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '../'))\\nfrom source import linear\\n\\ndef test_linear():\\n    assert linear(1) == 1\\n    assert linear(-1) == -1\\n    assert linear(0) == 0\\n    assert linear(10) == 10\\n    assert linear(-10) == -10\\n    assert linear(1.5) == 1.5\\n    assert linear(-1.5) == -1.5\\n    assert linear(0.5) == 0.5\\n    assert linear(5) == 5\\n    assert linear(-5) == -5\\n    assert linear(10.5) == 10.5\\n    assert linear(-10.5) == -10.5\",\n",
       "  67.0),\n",
       " (\"def windowing(x, w, axis=None):\\n    \\n    if axis is None:\\n        return x * w\\n\\n    if type(axis) is not int:\\n        raise TypeError('The axis should be a integer!')\\n\\n    d = x.dim()\\n    shape = [1] * d\\n    shape[axis] = len(w)\\n\\n    w = w.view(shape)\\n    return x * w\",\n",
       "  \"import pytest\\nimport numpy as np\\nfrom source import windowing \\n\\ndef test_windowing():\\n    # Test with 1D array\\n    x = np.array([1, 2, 3, 4])\\n    w = np.array([0.1, 0.2, 0.3])\\n    assert np.array_equal(windowing(x, w), np.array([1.1, 2.2, 3.3]))\\n\\n    # Test with 2D array\\n    x = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\\n    w = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\\n    assert np.array_equal(windowing(x, w), np.array([[1.1, 2.2, 3.3], [5.4, 6.5, 7.6]]))\\n\\n    # Test with 3D array\\n    x = np.array([[[1, 2, 3, 4], [5, 6, 7, 8]], [[1, 2, 3, 4], [5, 6, 7, 8]]])\\n    w = np.array([[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8, 0.9], [1.0, 1.1, 1.2]]])\\n    assert np.array_equal(windowing(x, w), np.array([[[1.1, 2.2, 3.3], [5.4, 6.5, 7.6]], [[1.7, 2.8, 3.9], [5.0, 6.1, 7.2]]]))\\n\\n    # Test with invalid axis\\n    x = np.array([1, 2, 3, 4])\\n    w = np.array([0.1, 0.2, 0.3])\\n    with pytest.raises(TypeError):\\n        windowing(x, w, axis='a')\",\n",
       "  30.0),\n",
       " ('import torch\\n\\ndef group_pixels(center_points, offsets):\\n    \\n    height, width = offsets.size()[1:]\\n\\n    # Generates a coordinate map, where each location is the coordinate of\\n    # that location.\\n    y_coord, x_coord = torch.meshgrid(\\n        torch.arange(height, dtype=offsets.dtype, device=offsets.device),\\n        torch.arange(width, dtype=offsets.dtype, device=offsets.device),\\n    )\\n    coord = torch.cat((y_coord.unsqueeze(0), x_coord.unsqueeze(0)), dim=0)\\n\\n    center_loc = coord + offsets\\n    center_loc = center_loc.flatten(1).T.unsqueeze_(0)  # [1, H*W, 2]\\n    center_points = center_points.unsqueeze(1)  # [K, 1, 2]\\n\\n    # Distance: [K, H*W].\\n    distance = torch.norm(center_points - center_loc, dim=-1)\\n\\n    # Finds center with minimum distance at each location, offset by 1, to\\n    # reserve id=0 for stuff.\\n    instance_id = torch.argmin(distance, dim=0).reshape((1, height, width)) + 1\\n    return instance_id',\n",
       "  '',\n",
       "  0.0),\n",
       " ('def comp_mass_magnets(self):\\n    \\n\\n    M = 0\\n    # magnet_0 and magnet_1 can have different materials\\n    if self.magnet_0:\\n        M += (\\n            self.H2\\n            * self.W4\\n            * self.magnet_0.Lmag\\n            * self.magnet_0.mat_type.mechanics.rho\\n        )\\n    if self.magnet_1:\\n        M += (\\n            self.H2\\n            * self.W4\\n            * self.magnet_1.Lmag\\n            * self.magnet_1.mat_type.mechanics.rho\\n        )\\n    return M',\n",
       "  'import pytest\\nfrom source import comp_mass_magnets\\n\\nclass TestCompMassMagnets:\\n    def test_comp_mass_magnets(self):\\n        assert comp_mass_magnets() == 0',\n",
       "  14.0),\n",
       " ('def get_HAMP_label(frequency, units=True, show_center=True):\\n    \\n    if 118.75 <= frequency < (118.75 + 25):\\n        label = \"± %.1f\" % (frequency - 118.75)\\n        if show_center:\\n            label = \"118.75 \" + label\\n    elif 183.31 <= frequency < (183.31 + 25):\\n        label = \"± %.1f\" % (frequency - 183.31)\\n        if show_center:\\n            label = \"183.31 \" + label\\n    else:\\n        label = \"%.2f\" % frequency\\n    if units:\\n        if \"±\" in label:\\n            label = \"(%s) GHz\" % label\\n        else:\\n            label = \"%s GHz\" % label\\n\\n    return label',\n",
       "  'import pytest\\nimport os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \\'..\\')))\\n\\nfrom source import get_HAMP_label\\n\\ndef test_get_HAMP_label():\\n    assert get_HAMP_label(118.75) == \"118.75 ± 0.0\"\\n    assert get_HAMP_label(134.25) == \"134.25 ± 0.1\"\\n    assert get_HAMP_label(183.31) == \"183.31 ± 0.0\"\\n    assert get_HAMP_label(120.75) == \"120.75 ± 0.1\"\\n    assert get_HAMP_label(118.75, units=False) == \"± 0.0\"\\n    assert get_HAMP_label(183.31, units=False) == \"± 0.0\"\\n    assert get_HAMP_label(134.25, units=False) == \"± 0.1\"\\n    assert get_HAMP_label(120.75, units=False) == \"± 0.1\"\\n    assert get_HAMP_label(150, show_center=False) == \"150 ± 0.1\"\\n    assert get_HAMP_label(183.31, show_center=False) == \"183.31 ± 0.0\"\\n    assert get_HAMP_label(120.75, show_center=False) == \"120.75 ± 0.1\"',\n",
       "  60.0),\n",
       " ('import torch\\n\\ndef from_mag_and_phase(mag, phase, dim=-2):\\n    \\n    return torch.cat([mag*torch.cos(phase), mag*torch.sin(phase)], dim=dim)',\n",
       "  'import pytest\\nimport torch\\nfrom source import from_mag_and_phase\\n\\nclass TestFromMagAndPhase:\\n\\n    @pytest.fixture\\n    def mag(self):\\n        return torch.tensor([1.0, 2.0, 3.0])\\n\\n    @pytest.fixture\\n    def phase(self):\\n        return torch.tensor([0.1, 0.2, 0.3])\\n\\n    def test_from_mag_and_phase(self, mag, phase):\\n        output = from_mag_and_phase(mag, phase)\\n        expected_output = torch.cat([mag * torch.cos(phase), mag * torch.sin(phase)], dim=-2)\\n        assert torch.allclose(output, expected_output)',\n",
       "  100.0),\n",
       " ('def metalicity_jk_v_band(period, phi31_v):\\n    \\n    return -5.038 - 5.394 * period + 1.345 * phi31_v',\n",
       "  'import pytest\\nfrom source import metalicity_jk_v_band\\n\\ndef test_metalicity_jk_v_band():\\n    with pytest.raises(IndexError):\\n        assert round(metalicity_jk_v_band(1, 0.5), 2) == -0.81',\n",
       "  100.0),\n",
       " ('def convert_to_95_percent_confidence(margin_of_error):\\n    \\n    return (1.96 / 1.645) * margin_of_error',\n",
       "  'import sys\\nsys.path.append(\".\") # This is to import the \\'source\\' file in the same directory\\nimport source \\nimport pytest\\n\\ndef test_convert_to_95_percent_confidence():\\n    assert source.convert_to_95_percent_confidence(1) == pytest.approx(1.96 / 1.645, 0.05)',\n",
       "  100.0),\n",
       " ('def hz2khz(frequency_in_hz):\\n    \\n    return frequency_in_hz / 1000',\n",
       "  '# test_source.py\\nimport source  # imports the source.py file\\nimport pytest\\n\\ndef test_hz2khz():\\n    # checking if the function returns zero when given zero\\n    assert source.hz2khz(0) == 0\\n\\n    # checking if the function returns the expected value\\n    assert source.hz2khz(1000) == 1\\n\\n    # checking if the function returns the expected value\\n    assert source.hz2khz(2000) == 2\\n\\n    # checking if the function returns the expected value\\n    assert source.hz2khz(3000) == 3',\n",
       "  100.0),\n",
       " ('def make_loss(data=None, out=None, name=None, **kwargs):\\n    r\\n    return (0,)',\n",
       "  '# source.py\\ndef make_loss(data=None, out=None, name=None, **kwargs):\\n    # Implementation of the function\\n    pass',\n",
       "  0.0),\n",
       " ('def svm_model_derivative(r3d_kpc, n0, r_c, beta, r_s=1.0, gamma=3.0, epsilon=0.0, alpha=0.0):\\n    \\n    \\n    t1 = n0 / (r3d_kpc*(r_c**2 + r3d_kpc**2))\\n    t2 = (r3d_kpc / r_c)**(-alpha/2.0)\\n    t3 = (1 + (r3d_kpc / r_c)**2)**(-3.0*beta/2.0)\\n    t4 = (1 + (r3d_kpc/r_s)**gamma)**(-epsilon/2.0/gamma - 1.0)\\n    t51 = -alpha * (r_c**2 + r3d_kpc**2) * ((r3d_kpc/r_s)**gamma + 1) / 2.0\\n    t52 = -3*beta*r3d_kpc**2*((r3d_kpc/r_s)**gamma + 1)\\n    t53 = -epsilon*(r_c**2 + r3d_kpc**2) / 2.0 * (r3d_kpc/r_s)**gamma\\n\\n    return t1*t2*t3*t4*(t51+t52+t53)',\n",
       "  'def test_svm_model_derivative():\\n    import source\\n    assert source.svm_model_derivative(1, 1, 1, 1) == 1',\n",
       "  100.0),\n",
       " (\"def design_thick_spherical_transform_lens(n, w, f):\\n    \\n    # Derivation p68 Dane's Fathom logbook #2\\n    roc = f*(n - 1) + w*(n - 1)\\n    d = (f - w)*n*roc/(f*(n - 1))\\n    return roc, d\",\n",
       "  'import pytest\\nimport source\\n\\ndef test_design_thick_spherical_transform_lens():\\n    n = 1.61\\n    w = 1.41\\n    f = 2.4\\n    roc, d = source.design_thick_spherical_transform_lens(n, w, f)\\n    assert roc == 2.3241000000000005\\n    assert d == 2.5303162500000003',\n",
       "  100.0),\n",
       " ('def train_pred_step(X, y, X_pred, model, metric):\\n    \\n    y_pred_one = model.predict_one(X)\\n    metric = metric.update(y, y_pred_one)\\n    model = model.fit_one(X, y)\\n    y_pred_1h = model.predict_one(X_pred)\\n    return y_pred_one, y_pred_1h, metric, model',\n",
       "  'import sys\\nsys.path.append(\".\")\\nfrom source import train_pred_step\\n\\ndef test_train_pred_step():\\n    X = []  # Assuming some input data\\n    y = []  # Assuming some target data\\n    X_pred = []  # Assuming some prediction data\\n    model = None  # Assuming some model\\n    metric = None  # Assuming some metric\\n\\n    y_pred_one, y_pred_1h, metric, model = train_pred_step(X, y, X_pred, model, metric)\\n\\n    assert y_pred_one is not None, \"Failed to predict y_pred_one\"\\n    assert y_pred_1h is not None, \"Failed to predict y_pred_1h\"\\n    assert metric is not None, \"Failed to update metric\"\\n    assert model is not None, \"Failed to fit model\"',\n",
       "  33.0),\n",
       " ('def dotdot(adj, vec1, vec2):\\n    \\n    return ((((adj).dot(vec1)).dot(vec2)))',\n",
       "  'import pytest\\nfrom source import dotdot\\nimport numpy as np\\n\\ndef test_dotdot():\\n    adj = np.array([[1,0,0],[0,1,0],[0,0,1]])\\n    vec1 = np.array([1,2,3])\\n    vec2 = np.array([4,5,6])\\n\\n    # Assertion\\n    assert np.allclose(dotdot(adj, vec1, vec2), 32)',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef quat2mat(quat):\\n    \\n    norm_quat = torch.cat([quat[:,:1].detach()*0 + 1, quat], dim=1)\\n    norm_quat = norm_quat/norm_quat.norm(p=2, dim=1, keepdim=True)\\n    w, x, y, z = norm_quat[:,0], norm_quat[:,1], norm_quat[:,2], norm_quat[:,3]\\n\\n    B = quat.size(0)\\n\\n    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\\n    wx, wy, wz = w*x, w*y, w*z\\n    xy, xz, yz = x*y, x*z, y*z\\n\\n    rotMat = torch.stack([w2 + x2 - y2 - z2, 2*xy - 2*wz, 2*wy + 2*xz,\\n                          2*wz + 2*xy, w2 - x2 + y2 - z2, 2*yz - 2*wx,\\n                          2*xz - 2*wy, 2*wx + 2*yz, w2 - x2 - y2 + z2], dim=1).view(B, 3, 3)\\n    return rotMat',\n",
       "  \"import pytest\\n\\n# Import the source code that you want to test\\nfrom source import quat2mat\\n\\n# Function to test quat2mat\\ndef test_quat2mat():\\n    # Here, we should unittest the function quat2mat\\n    # Let's test it with a simple input\\n    quat = torch.tensor([[[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]], dtype=torch.float32)\\n    result = quat2mat(quat)\\n    # The expected output can be defined here\\n    expected_output = torch.tensor([[[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]], dtype=torch.float32)\\n    # We use pytest's built-in functionality to assert that the two tensors are close\\n    assert torch.allclose(result, expected_output)\",\n",
       "  18.0),\n",
       " ('def get_grads(act_fn, x):\\n    \\n    x = x.clone().requires_grad_()  # Mark the input as tensor for which we want to store gradients\\n    out = act_fn(x)\\n    out.sum().backward()  # Summing results in an equal gradient flow to each element in x\\n    return x.grad  # Accessing the gradients of x by \"x.grad\"',\n",
       "  'import pytest\\nfrom pathlib import Path\\nfrom source import get_grads\\n\\ndef test_get_grads():\\n    act_fn = lambda x: x\\n    x = torch.tensor([1.0], requires_grad=True)\\n\\n    grad = get_grads(act_fn, x)\\n\\n    assert torch.allclose(grad, torch.tensor([1.0]))',\n",
       "  20.0),\n",
       " ('def tophat(vol, strel, block_size=[256, 256, 256]):\\n    \\n    return vol - open(vol, strel, block_size)',\n",
       "  'import pytest\\nfrom pathlib import Path\\nimport sys\\n\\nsys.path.insert(0, str(Path(__file__).parent.parent.parent))\\n\\nfrom source import tophat\\n\\ndef test_tophat():\\n    vol = \"path_to_your_volume_file\"\\n    strel = \"path_to_your_strel_file\"\\n    block_size = [256, 256, 256]\\n    \\n    assert tophat(vol, strel, block_size) == expected_output',\n",
       "  25.0),\n",
       " (\"import numpy\\n\\ndef expectation(sparse_operator, state):\\n    \\n    # Handle density matrix.\\n    if state.shape == sparse_operator.shape:\\n        product = state * sparse_operator\\n        expectation = numpy.sum(product.diagonal())\\n\\n    elif state.shape == (sparse_operator.shape[0], 1):\\n        # Handle state vector.\\n        expectation = state.getH() * sparse_operator * state\\n        expectation = expectation[0, 0]\\n\\n    else:\\n        # Handle exception.\\n        raise ValueError('Input state has invalid format.')\\n\\n    # Return.\\n    return expectation\",\n",
       "  '# test_expectation.py\\nimport pytest\\nimport numpy\\nfrom source import expectation  # replace with actual path if needed\\n\\ndef test_expectation():\\n    # Test case 1: Dense matrix input\\n    state = numpy.array([[1, 0], [0, 1]])\\n    sparse_operator = numpy.array([[1, 2], [3, 4]])\\n    expected_output = numpy.sum(numpy.diag(state * sparse_operator))\\n    assert numpy.isclose(expectation(state, sparse_operator), expected_output)\\n\\n    # Test case 2: Single-qubit state vector input\\n    state = numpy.array([[1], [0]])\\n    sparse_operator = numpy.array([[1, 2], [3, 4]])\\n    expected_output = (state.conj() * sparse_operator * state)[0, 0]\\n    assert numpy.isclose(expectation(state, sparse_operator), expected_output)\\n\\n    # Test case 3: Exception case\\n    state = numpy.array([[1, 2], [3, 4]])\\n    sparse_operator = numpy.array([[1, 2], [3, 4]])\\n    with pytest.raises(ValueError):\\n        expectation(state, sparse_operator)',\n",
       "  80.0),\n",
       " ('def round_scalar(scalar):\\n    \\n    return int(round(scalar))',\n",
       "  '# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append(\".\")\\n\\nfrom source import round_scalar\\n\\ndef test_round_scalar():\\n    assert round_scalar(5.5) == 6, \"The rounded value of 5.5 should be 6\"\\n    assert round_scalar(3.9) == 4, \"The rounded value of 3.9 should be 4\"\\n    assert round_scalar(-2.5) == -2, \"The rounded value of -2.5 should be -2\"\\n    assert round_scalar(-5.5) == -5, \"The rounded value of -5.5 should be -5\"\\n    assert round_scalar(0) == 0, \"The rounded value of 0 should be 0\"',\n",
       "  100.0),\n",
       " (\"def _param_station_data(ds, param, date):\\n    \\n    if isinstance(param, str):\\n        param_as_list = [param]\\n    else:\\n        param_as_list = param\\n\\n    ds_param = ds[param_as_list + ['x', 'y', 'alt']].sel(time=date).dropna(dim='station')\\n    data = ds_param[param]\\n    xs = ds_param['x'].values\\n    ys = ds_param['y'].values\\n    zs = ds_param['alt'].values\\n\\n    if isinstance(param, str):\\n        data_vals = data.values\\n    else:\\n        data_vals = data.to_array().values\\n\\n    return data_vals, xs, ys, zs\",\n",
       "  '# test_source.py\\nimport pytest\\nimport xarray as xr\\nimport numpy as np\\n\\n@pytest.fixture\\ndef dummy_data():\\n    # Creating a dummy xarray dataset\\n    dims = [\\'x\\', \\'y\\', \\'time\\']\\n    coords = {\\'x\\': np.arange(10), \\'y\\': np.arange(10), \\'time\\': np.arange(\\'2021-01-01T00\\', \\'2021-01-02T00\\', dtype=\\'datetime64[s]\\')}\\n    data_vars = {\\'param\\': (dims, np.random.rand(10,10,2)), \\'x\\': ([\\'x\\'], np.arange(10)), \\'y\\': ([\\'y\\'], np.arange(10)), \\'alt\\': ([\\'time\\'], np.arange(2))}\\n    ds = xr.Dataset(data_vars, coords=coords)\\n\\n    return ds\\n\\ndef test_param_station_data(dummy_data):\\n    # Importing the source.py file which has the function to be tested\\n    from source import _param_station_data\\n\\n    # Testing the function with strings and a datetime object\\n    data, xs, ys, zs = _param_station_data(dummy_data, \\'param\\', \\'2021-01-01T00:00:00\\')\\n    assert isinstance(data, np.ndarray), \"The function didn\\'t return a numpy array for the data.\"\\n    assert np.all(np.isin(xs, dummy_data[\\'x\\'].values)), \"The xs variable doesn\\'t contain all x values from the dataset.\"\\n    assert np.all(np.isin(ys, dummy_data[\\'y\\'].values)), \"The ys variable doesn\\'t contain all y values from the dataset.\"\\n    assert np.all(np.isin(zs, dummy_data[\\'alt\\'].values)), \"The zs variable doesn\\'t contain all altitude values from the dataset.\"\\n\\n    # Testing the function with a list of params\\n    data, xs, ys, zs = _param_station_data(dummy_data, [\\'param1\\', \\'param2\\'], \\'2021-01-01T00:00:00\\')\\n    assert isinstance(data, list) and len(data) == 2, \"The function didn\\'t return a list of numpy arrays for the data.\"',\n",
       "  0.0),\n",
       " ('def nxz(PAxz,PBxz,Npulse,P_times_Dj):\\n    \\n    return PAxz*PBxz*Npulse*P_times_Dj',\n",
       "  '# -*- coding: utf-8 -*-\\n\\nimport pytest\\nfrom source import nxz\\n\\ndef test_nxz():\\n    assert nxz(1, 2, 3, 4) == 24',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef quat_distance(target_quats, pred_quats, eps=1e-6, sign_invariant=True):\\n    \\n    quat_dot = torch.sum(target_quats * pred_quats, -1, keepdim=True)\\n    if sign_invariant:\\n        quat_dot = torch.abs(quat_dot) - eps\\n    else:\\n        quat_dot = quat_dot - eps * torch.sign(quat_dot)\\n    return 2 * torch.acos(quat_dot)',\n",
       "  'from source import *\\nimport torch\\nimport numpy as np\\nimport source\\n\\nclass TestQuatDistance:\\n\\n    def test_quat_distance(self):\\n        target_quats = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\\n        pred_quats = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\\n        result = source.quat_distance(target_quats, pred_quats)\\n        np.testing.assert_almost_equal(result.numpy(), np.zeros((4, 4)))\\n\\n    def test_quat_distance_sign_invariant(self):\\n        target_quats = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, -1.0]])\\n        pred_quats = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, -1.0]])\\n        result = source.quat_distance(target_quats, pred_quats, sign_invariant=True)\\n        np.testing.assert_almost_equal(result.numpy(), np.zeros((4, 4)))\\n\\n    def test_quat_distance_eps(self):\\n        target_quats = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\\n        pred_quats = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\\n        result = source.quat_distance(target_quats, pred_quats, eps=0.1)\\n        np.testing.assert_almost_equal(result.numpy(), np.zeros((4, 4)))',\n",
       "  86.0),\n",
       " ('def encode_value_in_name(value, name, max_name_len=20):\\n    \\n    if len(name) > max_name_len:\\n        msg = (\"Data information named \\'{0}\\' is too long\"\\n               \" (max {1} characters.)\").format(name, max_name_len)\\n        raise ValueError(msg)\\n    encoded_name = name if value <= 1 else name + \\'\\\\x00\\' + str(value)\\n    if len(encoded_name) > max_name_len:\\n        msg = (\"Data information named \\'{0}\\' is too long (need to be less\"\\n               \" than {1} characters when storing more than one value\"\\n               \" for a given data information.\"\\n               ).format(name, max_name_len - (len(str(value)) + 1))\\n        raise ValueError(msg)\\n    # Fill to the end with zeros\\n    return encoded_name.ljust(max_name_len, \\'\\\\x00\\').encode(\\'latin1\\')',\n",
       "  'import pytest\\nfrom source import encode_value_in_name\\n\\ndef test_encode_value_in_name():\\n    # Test with a value and name that doesn\\'t cause any exceptions\\n    assert encode_value_in_name(1, \"John\") == b\\'John\\\\x001\\'\\n\\n    # Test with a long name\\n    with pytest.raises(ValueError):\\n        encode_value_in_name(1, \"VeryVeryVeryVeryVeryVeryVeryVeryVeryVeryVeryVeryVe\")\\n\\n    # Test with a name that\\'s exactly at the limit\\n    assert encode_value_in_name(1, \"John\"*20) == b\\'John\\' + b\\'\\\\x001\\'\\n\\n    # Test with a name that\\'s over the limit\\n    with pytest.raises(ValueError):\\n        encode_value_in_name(1, \"John\"*20 + \"J\")\\n\\n    # Test with a value that\\'s 0\\n    assert encode_value_in_name(0, \"John\") == b\\'John\\\\x000\\'\\n\\n    # Test with more than one data information\\n    assert encode_value_in_name(1, \"John\", max_name_len=30) == b\\'John\\\\x001\\\\x00\\'\\n    with pytest.raises(ValueError):\\n        encode_value_in_name(1, \"John\", max_name_len=15)',\n",
       "  56.0),\n",
       " ('import torch\\n\\ndef transform_pointcloud(pointcloud: torch.Tensor, transform: torch.Tensor):\\n    r\\n    if not torch.is_tensor(pointcloud):\\n        raise TypeError(\\n            \"pointcloud should be tensor, but was %r instead\" % type(pointcloud)\\n        )\\n\\n    if not torch.is_tensor(transform):\\n        raise TypeError(\\n            \"transform should be tensor, but was %r instead\" % type(transform)\\n        )\\n\\n    if not pointcloud.ndim == 2:\\n        raise ValueError(\\n            \"pointcloud should have ndim of 2, but had {} instead.\".format(\\n                pointcloud.ndim\\n            )\\n        )\\n    if not pointcloud.shape[1] == 3:\\n        raise ValueError(\\n            \"pointcloud.shape[1] should be 3 (x, y, z), but was {} instead.\".format(\\n                pointcloud.shape[1]\\n            )\\n        )\\n    if not transform.shape[-2:] == (4, 4):\\n        raise ValueError(\\n            \"transform should be of shape (4, 4), but was {} instead.\".format(\\n                transform.shape\\n            )\\n        )\\n\\n    # Rotation matrix\\n    rmat = transform[:3, :3]\\n    # Translation vector\\n    tvec = transform[:3, 3]\\n\\n    # Transpose the pointcloud (to enable broadcast of rotation to each point)\\n    transposed_pointcloud = torch.transpose(pointcloud, 0, 1)\\n    # Rotate and translate cloud\\n    transformed_pointcloud = torch.matmul(rmat, transposed_pointcloud) + tvec.unsqueeze(\\n        1\\n    )\\n    # Transpose the transformed cloud to original dimensions\\n    transformed_pointcloud = torch.transpose(transformed_pointcloud, 0, 1)\\n\\n    return transformed_pointcloud',\n",
       "  'import torch\\nimport pytest\\nfrom source import transform_pointcloud\\n\\ndef test_transform_pointcloud():\\n    # Creating a test tensor\\n    pointcloud = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\\n    # Creating a simple rotation matrix and translation\\n    transform = torch.tensor([[2.0, 0.0, 0.0, 0.0], [0.0, 2.0, 0.0, 0.0], [0.0, 0.0, 2.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\\n    # Call the function and get the result\\n    res = transform_pointcloud(pointcloud, transform)\\n    # This is the single assertion in the test, \\n    # which checks that the transformed pointcloud has the expected shape\\n    assert res.shape == pointcloud.shape',\n",
       "  16.0),\n",
       " ('def natural_bounds(rank, nb_ingredients):\\n    \\n    if rank == 1:\\n        return 100 / nb_ingredients, 100.0\\n    else:\\n        return .0, 100 / rank',\n",
       "  '# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append(\".\") # To import source.py from the same directory\\nimport source \\n\\ndef test_natural_bounds():\\n    assert source.natural_bounds(1, 100) == (100.0, 100.0)\\n    assert source.natural_bounds(2, 100) == (0.0, 100.0)\\n    assert source.natural_bounds(3, 100) == (0.0, 75.0)\\n    assert source.natural_bounds(100, 100) == (0.0, 0.0)',\n",
       "  75.0),\n",
       " ('def get_nonzero_either_mask(vector_a, vector_b):\\n    \\n    return (vector_a > 0) & (vector_b > 0)',\n",
       "  \"# test_source.py\\nimport pytest\\nimport sys\\nsys.path.insert(0, '..') # This will add the parent directory in the path, allowing the import of 'source.py'\\nfrom source import get_nonzero_either_mask\\n\\ndef test_get_nonzero_either_mask():\\n    vector_a = [1, 0, 2]\\n    vector_b = [3, 0, -2]\\n    assert get_nonzero_either_mask(vector_a, vector_b) == [True, False, True]\\n\\ndef test_get_nonzero_either_mask_empty():\\n    vector_a = []\\n    vector_b = []\\n    assert get_nonzero_either_mask(vector_a, vector_b) == []\\n\\ndef test_get_nonzero_either_mask_single():\\n    vector_a = [1]\\n    vector_b = [-1]\\n    assert get_nonzero_either_mask(vector_a, vector_b) == [True]\\n\\ndef test_get_nonzero_either_mask_zero():\\n    vector_a = [0, 2]\\n    vector_b = [0, -2]\\n    assert get_nonzero_either_mask(vector_a, vector_b) == [False, True]\\n\\ndef test_get_nonzero_either_mask_mixed():\\n    vector_a = [0, 1]\\n    vector_b = [2, 0]\\n    assert get_nonzero_either_mask(vector_a, vector_b) == [False, True]\",\n",
       "  100.0),\n",
       " (\"def ndvi(red, nir):\\n    \\n    if red.shape != nir.shape:\\n        raise AttributeError('No equal shape')\\n\\n    # vegetation index components\\n    x = nir - red\\n    y = nir + red\\n\\n    # avoid division by zero error\\n    y[y == 0.0] = 1.0\\n\\n    # NDVI (NIR - RED) / (NIR + RED)\\n    img = x / y\\n\\n    img[y == 1.0] = 0.0\\n\\n    # clip range to normal difference\\n    img[img < -1.0] = -1.0\\n    img[img > 1.0] = 1.0\\n\\n    return img\",\n",
       "  'import pytest\\nimport numpy as np\\nfrom numpy.testing import assert_array_almost_equal\\nfrom source import ndvi    # import the ndvi function from source.py\\n\\ndef test_ndvi():\\n    # test data with shapes\\n    red = np.array([[1, 2, 3], [4, 5, 6]])\\n    nir = np.array([[7, 8, 9], [10, 11, 12]])\\n\\n    # expected output\\n    expected = np.array([[ -0.5, -0.33333333, 0.0 ],\\n                         [ 0.5, 0.33333333, 0.0 ]])\\n\\n    assert_array_almost_equal(ndvi(red, nir), expected)',\n",
       "  91.0),\n",
       " ('import torch\\n\\ndef quat2mat(quat):\\n    \\n    norm_quat = torch.cat([quat[:,:1].detach()*0 + 1, quat], dim=1)\\n    norm_quat = norm_quat/norm_quat.norm(p=2, dim=1, keepdim=True)\\n    w, x, y, z = norm_quat[:,0], norm_quat[:,1], norm_quat[:,2], norm_quat[:,3]\\n\\n    B = quat.size(0)\\n\\n    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\\n    wx, wy, wz = w*x, w*y, w*z\\n    xy, xz, yz = x*y, x*z, y*z\\n\\n    rotMat = torch.stack([w2 + x2 - y2 - z2, 2*xy - 2*wz, 2*wy + 2*xz,\\n                          2*wz + 2*xy, w2 - x2 + y2 - z2, 2*yz - 2*wx,\\n                          2*xz - 2*wy, 2*wx + 2*yz, w2 - x2 - y2 + z2], dim=1).view(B, 3, 3)\\n    return rotMat',\n",
       "  '# test_source.py\\nimport torch\\nimport pytest\\nfrom source import quat2mat\\n\\ndef test_quat2mat():\\n    quat = torch.randn(10, 4)  # Creates a tensor of size 10x4 with random values\\n    result = quat2mat(quat)\\n    assert torch.allclose(result, result, atol=1e-6), \"The function quat2mat did not produce the expected output\"',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef hard_example_mining(distance_matrix, pos_idxs, neg_idxs):\\n    \\n    assert len(distance_matrix.size()) == 2  # noqa: S101\\n\\n    # `dist_ap` means distance(anchor, positive)\\n    # both `dist_ap` and `relative_p_inds` with shape [N]\\n    dist_ap, _ = torch.max(distance_matrix * pos_idxs, dim=1)\\n    # `dist_an` means distance(anchor, negative)\\n    # both `dist_an` and `relative_n_inds` with shape [N]\\n    dist_an, _ = torch.min(distance_matrix * neg_idxs + pos_idxs * 99999999.0, dim=1)\\n\\n    return dist_ap, dist_an',\n",
       "  'import pytest\\nimport torch\\nfrom source import hard_example_mining\\n\\ndef test_hard_example_mining():\\n    distance_matrix = torch.rand([10, 10])\\n    pos_idxs = torch.rand([10])\\n    neg_idxs = torch.rand([10])\\n\\n    dist_ap, dist_an = hard_example_mining(distance_matrix, pos_idxs, neg_idxs)\\n    \\n    # Test that the shape of dist_ap and dist_an are correct\\n    assert dist_ap.shape == pos_idxs.shape\\n    assert dist_an.shape == neg_idxs.shape\\n\\n    # Test that len(distance_matrix.size()) == 2 holds\\n    assert len(distance_matrix.size()) == 2',\n",
       "  100.0),\n",
       " ('def matmul(mat, rhs):\\n    \\n    return mat.matmul(rhs)',\n",
       "  '# test_source.py\\nimport pytest\\nfrom pathlib import Path\\nimport sys\\n\\n# add the directory containing source.py to the sys path to import it\\nsys.path.append(str(Path(__file__).parent))\\n\\nimport source\\n\\ndef test_matmul():\\n    # arrange\\n    mat = [[1, 2], [3, 4]]\\n    rhs = [5, 6]\\n    # act\\n    result = source.matmul(mat, rhs)\\n    # assert\\n    assert result == [11, 16]',\n",
       "  100.0),\n",
       " ('def get_xi_from_xi_ext(xi_ext, fit_intercept):\\n    \\n    dim = len(xi_ext) // 2\\n    xi = xi_ext[:dim] - xi_ext[dim:]\\n    if fit_intercept:\\n        xi_0 = xi[0]\\n        xi = xi[1:]\\n    else:\\n        xi_0 = 0\\n    return xi_0, xi',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\\nfrom source import get_xi_from_xi_ext\\n\\ndef test_get_xi_from_xi_ext():\\nxi_ext = [1, 2, 3, 4, 5, 6]\\nfit_intercept = True\\n    xi_0, xi = get_xi_from_xi_ext(xi_ext, fit_intercept)\\n    assert xi_0 == 1, 'Failure in get_xi_from_xi_ext() with input parameters xi_ext = [1, 2, 3, 4, 5, 6] and fit_intercept = True'\\n\\ndef test_get_xi_from_xi_ext_no_intercept():\\n    xi_ext = [1, 2, 3, 4, 5, 6]\\n    fit_intercept = False\\n    xi_0, xi = get_xi_from_xi_ext(xi_ext, fit_intercept)\\n    assert xi_0 == 0, 'Failure in get_xi_from_xi_ext() with input parameters xi_ext = [1, 2, 3, 4, 5, 6] and fit_intercept = False'\\n\\ndef test_get_xi_from_xi_ext_exception():\\n    xi_ext = []\\n    fit_intercept = True\\n    with pytest.raises(Exception) as excinfo:\\n        get_xi_from_xi_ext(xi_ext, fit_intercept)\\n    assert str(excinfo.value) == 'xi_ext should have at least 2 elements', 'Exception not raised in get_xi_from_xi_ext() with input parameters xi_ext = [] and fit_intercept = True'\",\n",
       "  0.0),\n",
       " ('def ncartesian(L):\\n    \\n\\n    return int((L + 1) * (L + 2) / 2)',\n",
       "  'import sys\\nsys.path.append(\".\")\\nimport source  # Assuming the source code is in the same directory\\n\\ndef test_ncartesian():\\n    assert source.ncartesian(5) == 14',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef quat2mat(quat):\\n    \\n    norm_quat = torch.cat([quat[:,:1].detach()*0 + 1, quat], dim=1)\\n    norm_quat = norm_quat/norm_quat.norm(p=2, dim=1, keepdim=True)\\n    w, x, y, z = norm_quat[:,0], norm_quat[:,1], norm_quat[:,2], norm_quat[:,3]\\n\\n    B = quat.size(0)\\n\\n    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\\n    wx, wy, wz = w*x, w*y, w*z\\n    xy, xz, yz = x*y, x*z, y*z\\n\\n    rotMat = torch.stack([w2 + x2 - y2 - z2, 2*xy - 2*wz, 2*wy + 2*xz,\\n                          2*wz + 2*xy, w2 - x2 + y2 - z2, 2*yz - 2*wx,\\n                          2*xz - 2*wy, 2*wx + 2*yz, w2 - x2 - y2 + z2], dim=1).view(B, 3, 3)\\n    return rotMat',\n",
       "  '# test_source.py\\nimport pytest\\nimport torch\\nfrom source import quat2mat\\n\\ndef test_quat2mat():\\n    quat = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\\n    expected_output = torch.tensor([[[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]],\\n                                    [[1.0, 0.0, 0.0], [0.0, -1.0, 0.0], [0.0, 0.0, -1.0]],\\n                                    [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, -1.0]],\\n                                    [[1.0, 0.0, 0.0], [0.0, -1.0, 0.0], [0.0, 0.0, 1.0]]])\\n    output = quat2mat(quat)\\n    assert torch.allclose(output, expected_output), \"The function quat2mat did not produce the expected output.\"\\n\\nif __name__ == \"__main__\":\\n    test_quat2mat()',\n",
       "  100.0),\n",
       " (\"def spatpix_frame_to_ref(spatpix, frame='dms', subarray='SUBSTRIP256', oversample=1):\\n    \\n\\n    if (frame == 'nat') & (subarray == 'SUBSTRIP256'):\\n        spatpix_ref = spatpix\\n    elif (frame == 'dms') & (subarray == 'SUBSTRIP256'):\\n        spatpix_ref = 255*oversample - spatpix\\n    elif (frame == 'sim') & (subarray == 'SUBSTRIP256'):\\n        spatpix_ref = spatpix\\n    elif (frame == 'nat') & (subarray == 'SUBSTRIP96'):\\n        spatpix_ref = spatpix + 150*oversample\\n    elif (frame == 'dms') & (subarray == 'SUBSTRIP96'):\\n        spatpix_ref = 245*oversample - spatpix\\n    elif (frame == 'sim') & (subarray == 'SUBSTRIP96'):\\n        spatpix_ref = spatpix + 150*oversample\\n    else:\\n        raise ValueError('Unknown coordinate frame or subarray: {} {}'.format(frame, subarray))\\n\\n    return spatpix_ref\",\n",
       "  'import pytest\\nfrom source import spatpix_frame_to_ref\\n\\ndef test_spatpix_frame_to_ref():\\n    assert spatpix_frame_to_ref(300, \\'dms\\', \\'SUBSTRIP256\\', 1) == 255, \\'Test failed for frame=\"dms\", subarray=\"SUBSTRIP256\", oversample=1\\'\\n    assert spatpix_frame_to_ref(300, \\'nat\\', \\'SUBSTRIP256\\', 1) == 300, \\'Test failed for frame=\"nat\", subarray=\"SUBSTRIP256\", oversample=1\\'\\n    assert spatpix_frame_to_ref(300, \\'sim\\', \\'SUBSTRIP256\\', 1) == 300, \\'Test failed for frame=\"sim\", subarray=\"SUBSTRIP256\", oversample=1\\'\\n    assert spatpix_frame_to_ref(300, \\'dms\\', \\'SUBSTRIP96\\', 1) == 245, \\'Test failed for frame=\"dms\", subarray=\"SUBSTRIP96\", oversample=1\\'\\n    assert spatpix_frame_to_ref(300, \\'nat\\', \\'SUBSTRIP96\\', 1) == 300+150, \\'Test failed for frame=\"nat\", subarray=\"SUBSTRIP96\", oversample=1\\'\\n    assert spatpix_frame_to_ref(300, \\'sim\\', \\'SUBSTRIP96\\', 1) == 300+150, \\'Test failed for frame=\"sim\", subarray=\"SUBSTRIP96\", oversample=1\\'\\n\\n    with pytest.raises(ValueError):\\n        spatpix_frame_to_ref(300, \\'bsdf\\', \\'SUBSTRIP96\\', 1)',\n",
       "  33.0),\n",
       " ('def set_limits(output, plot_limits=[None, None], legend_limits=[None, None]):\\n    \\n    if all(plot_limits) and plot_limits[0] > plot_limits[1]:\\n        plot_limits.reverse()\\n    if all(legend_limits) and legend_limits[0] > legend_limits[1]:\\n        legend_limits.reverse()\\n\\n    if plot_limits[0] is None:\\n        plot_limits[0] = min(output)\\n    if plot_limits[1] is None:\\n        plot_limits[1] = max(output)\\n    if legend_limits[0] is None:\\n        legend_limits[0] = min(output)\\n    if legend_limits[1] is None:\\n        legend_limits[1] = max(output)\\n\\n    return plot_limits, legend_limits',\n",
       "  'import pytest\\nfrom source import set_limits  # Importing from the source.py file\\n\\ndef test_set_limits():\\n    output = [10, 20, 30, 40, 50]\\n    plot_limits = [None, None]\\n    legend_limits = [None, None]\\n    \\n    # Testing the function with default values\\n    assert set_limits(output) == ([10, 50], [10, 50])\\n\\n    # Testing the function with custom values\\n    plot_limits = [1, 2]\\n    legend_limits = [3, 4]\\n    assert set_limits(output, plot_limits, legend_limits) == (plot_limits, legend_limits)\\n\\n    # Testing the function with None in plot_limits\\n    plot_limits = [None, 2]\\n    assert set_limits(output, plot_limits) == ([10, 2], [10, 2])\\n\\n    # Testing the function with None in legend_limits\\n    legend_limits = [3, None]\\n    assert set_limits(output, plot_limits=None, legend_limits=legend_limits) == ([10, 2], [3, 50])\\n\\n    # Testing the function with all values None\\n    plot_limits = [None, None]\\n    legend_limits = [None, None]\\n    assert set_limits(output, plot_limits, legend_limits) == ([10, 50], [10, 50])\\n\\n    # Testing the function with mixed None and custom values\\n    plot_limits = [1, 2]\\n    legend_limits = [None, 4]\\n    assert set_limits(output, plot_limits, legend_limits) == ([1, 2], [10, 4])',\n",
       "  86.0),\n",
       " (\"def normalize_position(image, box):\\n    \\n    ymin, xmin, ymax, xmax = box\\n    im_width, im_height = image.shape[:2]\\n\\n    assert xmin <= xmax, 'xmin is greater than xmax'\\n    assert ymin <= ymax, 'ymin is greater than ymax'\\n    assert xmin <= im_width, 'xmin is greater than image width'\\n    assert xmax <= im_width, 'xmax is greater than image width'\\n    assert ymin <= im_height, 'ymin is greater than image height'\\n    assert ymax <= im_height, 'ymax is greater than image height'\\n    assert xmin >= 0, 'xmin < 0'\\n    assert ymin >= 0, 'ymin < 0'\\n\\n    x_center = im_width / 2\\n    y_center = im_height / 2\\n    x = (xmin - (x_center)) / x_center\\n    y = (ymin - (y_center)) / y_center\\n    width = (xmax - xmin) / x_center\\n    height = (ymax - ymin) / y_center\\n    z = 0.0\\n    depth = 0.0\\n    return x, y, z, width, height, depth\",\n",
       "  'import sys\\nsys.path.append(\".\") # Adds the current directory to the system path\\nimport source \\n\\ndef test_normalize_position():\\n    image = (100, 100) # example dimensions\\n    box = (10, 20, 30, 40) # example box values\\n    x, y, z, width, height, depth = source.normalize_position(image, box)\\n\\n    assert x == (box[0] - (image[0] / 2)) / image[0], \"x value is not correctly calculated\"\\n    assert y == (box[1] - (image[1] / 2)) / image[1], \"y value is not correctly calculated\"\\n    assert z == 0.0, \"z value is not correctly calculated\"\\n    assert width == (box[2] - box[0]) / image[0], \"width value is not correctly calculated\"\\n    assert height == (box[3] - box[1]) / image[1], \"height value is not correctly calculated\"\\n    assert depth == 0.0, \"depth value is not correctly calculated\"',\n",
       "  15.0),\n",
       " (\"def getModel():\\n    \\n    from keras.models import Sequential\\n    from keras.layers import Flatten, Dense, Lambda, Activation, Dropout\\n    from keras.layers.convolutional import Convolution2D, Cropping2D\\n    from keras.layers.pooling import AveragePooling2D\\n    from keras.layers.normalization import BatchNormalization\\n    from keras.layers.noise import GaussianNoise\\n\\n    model = Sequential()\\n    model.add(Lambda(lambda x: x/255.0 - 0.5 , input_shape=(66,200,3)))\\n    model.add(Convolution2D(3,1,1))\\n    model.add(Convolution2D(24,5,5,subsample=(2,2), activation='relu'))\\n    model.add(BatchNormalization())\\n    model.add(Convolution2D(36,5,5,subsample=(2,2), activation='relu'))\\n    model.add(BatchNormalization())\\n    model.add(Convolution2D(48,5,5,subsample=(2,2), activation='relu'))\\n    model.add(BatchNormalization())\\n    model.add(Convolution2D(64,3,3, activation='relu'))\\n    model.add(BatchNormalization())\\n    model.add(Convolution2D(64,3,3, activation='relu'))\\n    model.add(BatchNormalization())\\n    model.add(Flatten())\\n    model.add(Dropout(0.5))\\n    model.add(Dense(100, activation='relu'))\\n    model.add(Dense(50, activation='relu'))\\n    model.add(Dense(10, activation='relu'))\\n    model.add(Dense(1))\\n\\n    return model\",\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport sys\\nsys.path.append(\\'.\\')  # Adds the current directory to the Python path\\nimport source  # No need to use the full path to the file\\n\\ndef test_getModel():\\n    model = source.getModel()\\n    assert isinstance(model, source.Sequential), \"The function did not return an instance of Sequential\"',\n",
       "  7.0),\n",
       " ('import numpy\\n\\ndef _hobby(points, angles=None, curl_start=1, curl_end=1, t_in=1, t_out=1, cycle=False):\\n    \\n    z = points[:, 0] + 1j * points[:, 1]\\n    n = z.size\\n    if numpy.isscalar(t_in):\\n        t_in = t_in * numpy.ones(n)\\n    else:\\n        t_in = numpy.array(t_in)\\n    if numpy.isscalar(t_out):\\n        t_out = t_out * numpy.ones(n)\\n    else:\\n        t_out = numpy.array(t_out)\\n    if angles is None:\\n        angles = [None] * n\\n    rotate = 0\\n    if cycle and any(a is not None for a in angles):\\n        while angles[rotate] is None:\\n            rotate += 1\\n        angles = [angles[(rotate + j) % n] for j in range(n + 1)]\\n        z = numpy.hstack((numpy.roll(z, -rotate), z[rotate : rotate + 1]))\\n        t_in = numpy.hstack((numpy.roll(t_in, -rotate), t_in[rotate : rotate + 1]))\\n        t_out = numpy.hstack((numpy.roll(t_out, -rotate), t_out[rotate : rotate + 1]))\\n        cycle = False\\n    if cycle:\\n        # Closed curve\\n        v = numpy.roll(z, -1) - z\\n        d = numpy.abs(v)\\n        delta = numpy.angle(v)\\n        psi = (delta - numpy.roll(delta, 1) + numpy.pi) % (2 * numpy.pi) - numpy.pi\\n        coef = numpy.zeros(2 * n)\\n        coef[:n] = -psi\\n        m = numpy.zeros((2 * n, 2 * n))\\n        i = numpy.arange(n)\\n        i1 = (i + 1) % n\\n        i2 = (i + 2) % n\\n        ni = n + i\\n        m[i, i] = 1\\n        m[i, n + (i - 1) % n] = 1\\n        # A_i\\n        m[ni, i] = d[i1] * t_in[i2] * t_in[i1] ** 2\\n        # B_{i+1}\\n        m[ni, i1] = -d[i] * t_out[i] * t_out[i1] ** 2 * (1 - 3 * t_in[i2])\\n        # C_{i+1}\\n        m[ni, ni] = d[i1] * t_in[i2] * t_in[i1] ** 2 * (1 - 3 * t_out[i])\\n        # D_{i+2}\\n        m[ni, n + i1] = -d[i] * t_out[i] * t_out[i1] ** 2\\n        sol = numpy.linalg.solve(m, coef)\\n        theta = sol[:n]\\n        phi = sol[n:]\\n        w = numpy.exp(1j * (theta + delta))\\n        a = 2 ** 0.5\\n        b = 1.0 / 16\\n        c = (3 - 5 ** 0.5) / 2\\n        sintheta = numpy.sin(theta)\\n        costheta = numpy.cos(theta)\\n        sinphi = numpy.sin(phi)\\n        cosphi = numpy.cos(phi)\\n        alpha = (\\n            a * (sintheta - b * sinphi) * (sinphi - b * sintheta) * (costheta - cosphi)\\n        )\\n        cta = z + w * d * ((2 + alpha) / (1 + (1 - c) * costheta + c * cosphi)) / (\\n            3 * t_out\\n        )\\n        ctb = numpy.roll(z, -1) - numpy.roll(w, -1) * d * (\\n            (2 - alpha) / (1 + (1 - c) * cosphi + c * costheta)\\n        ) / (3 * numpy.roll(t_in, -1))\\n    else:\\n        # Open curve(s)\\n        n = z.size - 1\\n        v = z[1:] - z[:-1]\\n        d = numpy.abs(v)\\n        delta = numpy.angle(v)\\n        psi = (delta[1:] - delta[:-1] + numpy.pi) % (2 * numpy.pi) - numpy.pi\\n        theta = numpy.empty(n)\\n        phi = numpy.empty(n)\\n        i = 0\\n        if angles[0] is not None:\\n            theta[0] = angles[0] - delta[0]\\n        while i < n:\\n            j = i + 1\\n            while j < n + 1 and angles[j] is None:\\n                j += 1\\n            if j == n + 1:\\n                j -= 1\\n            else:\\n                phi[j - 1] = delta[j - 1] - angles[j]\\n                if j < n:\\n                    theta[j] = angles[j] - delta[j]\\n            # Solve open curve z_i, ..., z_j\\n            nn = j - i\\n            coef = numpy.zeros(2 * nn)\\n            coef[1:nn] = -psi[i : j - 1]\\n            m = numpy.zeros((2 * nn, 2 * nn))\\n            if nn > 1:\\n                ii = numpy.arange(nn - 1)  # [0 .. nn-2]\\n                i0 = i + ii  # [i .. j-1]\\n                i1 = 1 + i0  # [i+1 .. j]\\n                i2 = 2 + i0  # [i+2 .. j+1]\\n                ni = nn + ii  # [nn .. 2*nn-2]\\n                ii1 = 1 + ii  # [1 .. nn-1]\\n                m[ii1, ii1] = 1\\n                m[ii1, ni] = 1\\n                # A_ii\\n                m[ni, ii] = d[i1] * t_in[i2] * t_in[i1] ** 2\\n                # B_{ii+1}\\n                m[ni, ii1] = -d[i0] * t_out[i0] * t_out[i1] ** 2 * (1 - 3 * t_in[i2])\\n                # C_{ii+1}\\n                m[ni, ni] = d[i1] * t_in[i2] * t_in[i1] ** 2 * (1 - 3 * t_out[i0])\\n                # D_{ii+2}\\n                m[ni, ni + 1] = -d[i0] * t_out[i0] * t_out[i1] ** 2\\n            if angles[i] is None:\\n                to3 = t_out[0] ** 3\\n                cti3 = curl_start * t_in[1] ** 3\\n                # B_0\\n                m[0, 0] = to3 * (1 - 3 * t_in[1]) - cti3\\n                # D_1\\n                m[0, nn] = to3 - cti3 * (1 - 3 * t_out[0])\\n            else:\\n                coef[0] = theta[i]\\n                m[0, 0] = 1\\n                m[0, nn] = 0\\n            if angles[j] is None:\\n                ti3 = t_in[n] ** 3\\n                cto3 = curl_end * t_out[n - 1] ** 3\\n                # A_{nn-1}\\n                m[2 * nn - 1, nn - 1] = ti3 - cto3 * (1 - 3 * t_in[n])\\n                # C_nn\\n                m[2 * nn - 1, 2 * nn - 1] = ti3 * (1 - 3 * t_out[n - 1]) - cto3\\n            else:\\n                coef[2 * nn - 1] = phi[j - 1]\\n                m[2 * nn - 1, nn - 1] = 0\\n                m[2 * nn - 1, 2 * nn - 1] = 1\\n            if nn > 1 or angles[i] is None or angles[j] is None:\\n                # print(\"range:\", i, j)\\n                # print(\"A =\", m)\\n                # print(\"b =\", coef)\\n                sol = numpy.linalg.solve(m, coef)\\n                # print(\"x =\", sol)\\n                theta[i:j] = sol[:nn]\\n                phi[i:j] = sol[nn:]\\n            i = j\\n        w = numpy.hstack(\\n            (numpy.exp(1j * (delta + theta)), numpy.exp(1j * (delta[-1:] - phi[-1:])))\\n        )\\n        a = 2 ** 0.5\\n        b = 1.0 / 16\\n        c = (3 - 5 ** 0.5) / 2\\n        sintheta = numpy.sin(theta)\\n        costheta = numpy.cos(theta)\\n        sinphi = numpy.sin(phi)\\n        cosphi = numpy.cos(phi)\\n        alpha = (\\n            a * (sintheta - b * sinphi) * (sinphi - b * sintheta) * (costheta - cosphi)\\n        )\\n        cta = z[:-1] + w[:-1] * d * (\\n            (2 + alpha) / (1 + (1 - c) * costheta + c * cosphi)\\n        ) / (3 * t_out[:-1])\\n        ctb = z[1:] - w[1:] * d * (\\n            (2 - alpha) / (1 + (1 - c) * cosphi + c * costheta)\\n        ) / (3 * t_in[1:])\\n        if rotate > 0:\\n            cta = numpy.roll(cta, rotate)\\n            ctb = numpy.roll(ctb, rotate)\\n    return (\\n        numpy.vstack((cta.real, cta.imag)).transpose(),\\n        numpy.vstack((ctb.real, ctb.imag)).transpose(),\\n    )',\n",
       "  'import numpy\\n\\ndef test_hobby():\\n    points = numpy.array([[1, 2], [3, 4], [5, 6]])\\n    angles = [numpy.pi/4, numpy.pi/2, numpy.pi]\\n    t_in = [1, 2, 3]\\n    t_out = [4, 5, 6]\\n    cycle = True\\n    expected_output = numpy.array([[...], [...], [...]])  # add expected output here\\n    actual_output = _hobby(points, angles, 1, 1, t_in, t_out, cycle)\\n    assert numpy.array_equal(expected_output, actual_output), \"The output does not match the expected output.\"',\n",
       "  0.0),\n",
       " ('import numpy\\n\\ndef _hobby(points, angles=None, curl_start=1, curl_end=1, t_in=1, t_out=1, cycle=False):\\n    \\n    z = points[:, 0] + 1j * points[:, 1]\\n    n = z.size\\n    if numpy.isscalar(t_in):\\n        t_in = t_in * numpy.ones(n)\\n    else:\\n        t_in = numpy.array(t_in)\\n    if numpy.isscalar(t_out):\\n        t_out = t_out * numpy.ones(n)\\n    else:\\n        t_out = numpy.array(t_out)\\n    if angles is None:\\n        angles = [None] * n\\n    rotate = 0\\n    if cycle and any(a is not None for a in angles):\\n        while angles[rotate] is None:\\n            rotate += 1\\n        angles = [angles[(rotate + j) % n] for j in range(n + 1)]\\n        z = numpy.hstack((numpy.roll(z, -rotate), z[rotate : rotate + 1]))\\n        t_in = numpy.hstack((numpy.roll(t_in, -rotate), t_in[rotate : rotate + 1]))\\n        t_out = numpy.hstack((numpy.roll(t_out, -rotate), t_out[rotate : rotate + 1]))\\n        cycle = False\\n    if cycle:\\n        # Closed curve\\n        v = numpy.roll(z, -1) - z\\n        d = numpy.abs(v)\\n        delta = numpy.angle(v)\\n        psi = (delta - numpy.roll(delta, 1) + numpy.pi) % (2 * numpy.pi) - numpy.pi\\n        coef = numpy.zeros(2 * n)\\n        coef[:n] = -psi\\n        m = numpy.zeros((2 * n, 2 * n))\\n        i = numpy.arange(n)\\n        i1 = (i + 1) % n\\n        i2 = (i + 2) % n\\n        ni = n + i\\n        m[i, i] = 1\\n        m[i, n + (i - 1) % n] = 1\\n        # A_i\\n        m[ni, i] = d[i1] * t_in[i2] * t_in[i1] ** 2\\n        # B_{i+1}\\n        m[ni, i1] = -d[i] * t_out[i] * t_out[i1] ** 2 * (1 - 3 * t_in[i2])\\n        # C_{i+1}\\n        m[ni, ni] = d[i1] * t_in[i2] * t_in[i1] ** 2 * (1 - 3 * t_out[i])\\n        # D_{i+2}\\n        m[ni, n + i1] = -d[i] * t_out[i] * t_out[i1] ** 2\\n        sol = numpy.linalg.solve(m, coef)\\n        theta = sol[:n]\\n        phi = sol[n:]\\n        w = numpy.exp(1j * (theta + delta))\\n        a = 2 ** 0.5\\n        b = 1.0 / 16\\n        c = (3 - 5 ** 0.5) / 2\\n        sintheta = numpy.sin(theta)\\n        costheta = numpy.cos(theta)\\n        sinphi = numpy.sin(phi)\\n        cosphi = numpy.cos(phi)\\n        alpha = (\\n            a * (sintheta - b * sinphi) * (sinphi - b * sintheta) * (costheta - cosphi)\\n        )\\n        cta = z + w * d * ((2 + alpha) / (1 + (1 - c) * costheta + c * cosphi)) / (\\n            3 * t_out\\n        )\\n        ctb = numpy.roll(z, -1) - numpy.roll(w, -1) * d * (\\n            (2 - alpha) / (1 + (1 - c) * cosphi + c * costheta)\\n        ) / (3 * numpy.roll(t_in, -1))\\n    else:\\n        # Open curve(s)\\n        n = z.size - 1\\n        v = z[1:] - z[:-1]\\n        d = numpy.abs(v)\\n        delta = numpy.angle(v)\\n        psi = (delta[1:] - delta[:-1] + numpy.pi) % (2 * numpy.pi) - numpy.pi\\n        theta = numpy.empty(n)\\n        phi = numpy.empty(n)\\n        i = 0\\n        if angles[0] is not None:\\n            theta[0] = angles[0] - delta[0]\\n        while i < n:\\n            j = i + 1\\n            while j < n + 1 and angles[j] is None:\\n                j += 1\\n            if j == n + 1:\\n                j -= 1\\n            else:\\n                phi[j - 1] = delta[j - 1] - angles[j]\\n                if j < n:\\n                    theta[j] = angles[j] - delta[j]\\n            # Solve open curve z_i, ..., z_j\\n            nn = j - i\\n            coef = numpy.zeros(2 * nn)\\n            coef[1:nn] = -psi[i : j - 1]\\n            m = numpy.zeros((2 * nn, 2 * nn))\\n            if nn > 1:\\n                ii = numpy.arange(nn - 1)  # [0 .. nn-2]\\n                i0 = i + ii  # [i .. j-1]\\n                i1 = 1 + i0  # [i+1 .. j]\\n                i2 = 2 + i0  # [i+2 .. j+1]\\n                ni = nn + ii  # [nn .. 2*nn-2]\\n                ii1 = 1 + ii  # [1 .. nn-1]\\n                m[ii1, ii1] = 1\\n                m[ii1, ni] = 1\\n                # A_ii\\n                m[ni, ii] = d[i1] * t_in[i2] * t_in[i1] ** 2\\n                # B_{ii+1}\\n                m[ni, ii1] = -d[i0] * t_out[i0] * t_out[i1] ** 2 * (1 - 3 * t_in[i2])\\n                # C_{ii+1}\\n                m[ni, ni] = d[i1] * t_in[i2] * t_in[i1] ** 2 * (1 - 3 * t_out[i0])\\n                # D_{ii+2}\\n                m[ni, ni + 1] = -d[i0] * t_out[i0] * t_out[i1] ** 2\\n            if angles[i] is None:\\n                to3 = t_out[0] ** 3\\n                cti3 = curl_start * t_in[1] ** 3\\n                # B_0\\n                m[0, 0] = to3 * (1 - 3 * t_in[1]) - cti3\\n                # D_1\\n                m[0, nn] = to3 - cti3 * (1 - 3 * t_out[0])\\n            else:\\n                coef[0] = theta[i]\\n                m[0, 0] = 1\\n                m[0, nn] = 0\\n            if angles[j] is None:\\n                ti3 = t_in[n] ** 3\\n                cto3 = curl_end * t_out[n - 1] ** 3\\n                # A_{nn-1}\\n                m[2 * nn - 1, nn - 1] = ti3 - cto3 * (1 - 3 * t_in[n])\\n                # C_nn\\n                m[2 * nn - 1, 2 * nn - 1] = ti3 * (1 - 3 * t_out[n - 1]) - cto3\\n            else:\\n                coef[2 * nn - 1] = phi[j - 1]\\n                m[2 * nn - 1, nn - 1] = 0\\n                m[2 * nn - 1, 2 * nn - 1] = 1\\n            if nn > 1 or angles[i] is None or angles[j] is None:\\n                sol = numpy.linalg.solve(m, coef)\\n                theta[i:j] = sol[:nn]\\n                phi[i:j] = sol[nn:]\\n            i = j\\n        w = numpy.hstack(\\n            (numpy.exp(1j * (delta + theta)), numpy.exp(1j * (delta[-1:] - phi[-1:])))\\n        )\\n        a = 2 ** 0.5\\n        b = 1.0 / 16\\n        c = (3 - 5 ** 0.5) / 2\\n        sintheta = numpy.sin(theta)\\n        costheta = numpy.cos(theta)\\n        sinphi = numpy.sin(phi)\\n        cosphi = numpy.cos(phi)\\n        alpha = (\\n            a * (sintheta - b * sinphi) * (sinphi - b * sintheta) * (costheta - cosphi)\\n        )\\n        cta = z[:-1] + w[:-1] * d * (\\n            (2 + alpha) / (1 + (1 - c) * costheta + c * cosphi)\\n        ) / (3 * t_out[:-1])\\n        ctb = z[1:] - w[1:] * d * (\\n            (2 - alpha) / (1 + (1 - c) * cosphi + c * costheta)\\n        ) / (3 * t_in[1:])\\n        if rotate > 0:\\n            cta = numpy.roll(cta, rotate)\\n            ctb = numpy.roll(ctb, rotate)\\n    return (\\n        numpy.vstack((cta.real, cta.imag)).transpose(),\\n        numpy.vstack((ctb.real, ctb.imag)).transpose(),\\n    )',\n",
       "  '# test_source.py\\nimport numpy\\nimport source  # assuming the code is in source.py\\n\\ndef test_hobby():\\n    # we assume the source code is correct and we only focus on the output of the function\\n    points = numpy.array([[1, 1], [2, 3], [3, 2]])\\n    angles = [numpy.pi / 2, None, numpy.pi / 4]\\n    t_in = [1, 2, 3]\\n    t_out = [4, 5, 6]\\n    curl_start = 2\\n    curl_end = 3\\n    cycle = True\\n\\n    output = source._hobby(points, angles, curl_start, curl_end, t_in, t_out, cycle)\\n\\n    # We check the shape of the output.\\n    assert isinstance(output, tuple) and len(output) == 2\\n    # We check the shape of each element of the output.\\n    for arr in output:\\n        assert isinstance(arr, numpy.ndarray) and arr.ndim == 2\\n\\n# We can add more tests if needed.',\n",
       "  63.0),\n",
       " ('import numpy\\n\\ndef _hobby(points, angles=None, curl_start=1, curl_end=1, t_in=1, t_out=1, cycle=False):\\n    \\n    z = points[:, 0] + 1j * points[:, 1]\\n    n = z.size\\n    if numpy.isscalar(t_in):\\n        t_in = t_in * numpy.ones(n)\\n    else:\\n        t_in = numpy.array(t_in)\\n    if numpy.isscalar(t_out):\\n        t_out = t_out * numpy.ones(n)\\n    else:\\n        t_out = numpy.array(t_out)\\n    if angles is None:\\n        angles = [None] * n\\n    rotate = 0\\n    if cycle and any(a is not None for a in angles):\\n        while angles[rotate] is None:\\n            rotate += 1\\n        angles = [angles[(rotate + j) % n] for j in range(n + 1)]\\n        z = numpy.hstack((numpy.roll(z, -rotate), z[rotate : rotate + 1]))\\n        t_in = numpy.hstack((numpy.roll(t_in, -rotate), t_in[rotate : rotate + 1]))\\n        t_out = numpy.hstack((numpy.roll(t_out, -rotate), t_out[rotate : rotate + 1]))\\n        cycle = False\\n    if cycle:\\n        # Closed curve\\n        v = numpy.roll(z, -1) - z\\n        d = numpy.abs(v)\\n        delta = numpy.angle(v)\\n        psi = (delta - numpy.roll(delta, 1) + numpy.pi) % (2 * numpy.pi) - numpy.pi\\n        coef = numpy.zeros(2 * n)\\n        coef[:n] = -psi\\n        m = numpy.zeros((2 * n, 2 * n))\\n        i = numpy.arange(n)\\n        i1 = (i + 1) % n\\n        i2 = (i + 2) % n\\n        ni = n + i\\n        m[i, i] = 1\\n        m[i, n + (i - 1) % n] = 1\\n        # A_i\\n        m[ni, i] = d[i1] * t_in[i2] * t_in[i1] ** 2\\n        # B_{i+1}\\n        m[ni, i1] = -d[i] * t_out[i] * t_out[i1] ** 2 * (1 - 3 * t_in[i2])\\n        # C_{i+1}\\n        m[ni, ni] = d[i1] * t_in[i2] * t_in[i1] ** 2 * (1 - 3 * t_out[i])\\n        # D_{i+2}\\n        m[ni, n + i1] = -d[i] * t_out[i] * t_out[i1] ** 2\\n        sol = numpy.linalg.solve(m, coef)\\n        theta = sol[:n]\\n        phi = sol[n:]\\n        w = numpy.exp(1j * (theta + delta))\\n        a = 2 ** 0.5\\n        b = 1.0 / 16\\n        c = (3 - 5 ** 0.5) / 2\\n        sintheta = numpy.sin(theta)\\n        costheta = numpy.cos(theta)\\n        sinphi = numpy.sin(phi)\\n        cosphi = numpy.cos(phi)\\n        alpha = (\\n            a * (sintheta - b * sinphi) * (sinphi - b * sintheta) * (costheta - cosphi)\\n        )\\n        cta = z + w * d * ((2 + alpha) / (1 + (1 - c) * costheta + c * cosphi)) / (\\n            3 * t_out\\n        )\\n        ctb = numpy.roll(z, -1) - numpy.roll(w, -1) * d * (\\n            (2 - alpha) / (1 + (1 - c) * cosphi + c * costheta)\\n        ) / (3 * numpy.roll(t_in, -1))\\n    else:\\n        # Open curve(s)\\n        n = z.size - 1\\n        v = z[1:] - z[:-1]\\n        d = numpy.abs(v)\\n        delta = numpy.angle(v)\\n        psi = (delta[1:] - delta[:-1] + numpy.pi) % (2 * numpy.pi) - numpy.pi\\n        theta = numpy.empty(n)\\n        phi = numpy.empty(n)\\n        i = 0\\n        if angles[0] is not None:\\n            theta[0] = angles[0] - delta[0]\\n        while i < n:\\n            j = i + 1\\n            while j < n + 1 and angles[j] is None:\\n                j += 1\\n            if j == n + 1:\\n                j -= 1\\n            else:\\n                phi[j - 1] = delta[j - 1] - angles[j]\\n                if j < n:\\n                    theta[j] = angles[j] - delta[j]\\n            # Solve open curve z_i, ..., z_j\\n            nn = j - i\\n            coef = numpy.zeros(2 * nn)\\n            coef[1:nn] = -psi[i : j - 1]\\n            m = numpy.zeros((2 * nn, 2 * nn))\\n            if nn > 1:\\n                ii = numpy.arange(nn - 1)  # [0 .. nn-2]\\n                i0 = i + ii  # [i .. j-1]\\n                i1 = 1 + i0  # [i+1 .. j]\\n                i2 = 2 + i0  # [i+2 .. j+1]\\n                ni = nn + ii  # [nn .. 2*nn-2]\\n                ii1 = 1 + ii  # [1 .. nn-1]\\n                m[ii1, ii1] = 1\\n                m[ii1, ni] = 1\\n                # A_ii\\n                m[ni, ii] = d[i1] * t_in[i2] * t_in[i1] ** 2\\n                # B_{ii+1}\\n                m[ni, ii1] = -d[i0] * t_out[i0] * t_out[i1] ** 2 * (1 - 3 * t_in[i2])\\n                # C_{ii+1}\\n                m[ni, ni] = d[i1] * t_in[i2] * t_in[i1] ** 2 * (1 - 3 * t_out[i0])\\n                # D_{ii+2}\\n                m[ni, ni + 1] = -d[i0] * t_out[i0] * t_out[i1] ** 2\\n            if angles[i] is None:\\n                to3 = t_out[0] ** 3\\n                cti3 = curl_start * t_in[1] ** 3\\n                # B_0\\n                m[0, 0] = to3 * (1 - 3 * t_in[1]) - cti3\\n                # D_1\\n                m[0, nn] = to3 - cti3 * (1 - 3 * t_out[0])\\n            else:\\n                coef[0] = theta[i]\\n                m[0, 0] = 1\\n                m[0, nn] = 0\\n            if angles[j] is None:\\n                ti3 = t_in[n] ** 3\\n                cto3 = curl_end * t_out[n - 1] ** 3\\n                # A_{nn-1}\\n                m[2 * nn - 1, nn - 1] = ti3 - cto3 * (1 - 3 * t_in[n])\\n                # C_nn\\n                m[2 * nn - 1, 2 * nn - 1] = ti3 * (1 - 3 * t_out[n - 1]) - cto3\\n            else:\\n                coef[2 * nn - 1] = phi[j - 1]\\n                m[2 * nn - 1, nn - 1] = 0\\n                m[2 * nn - 1, 2 * nn - 1] = 1\\n            if nn > 1 or angles[i] is None or angles[j] is None:\\n                # print(\"range:\", i, j)\\n                # print(\"A =\", m)\\n                # print(\"b =\", coef)\\n                sol = numpy.linalg.solve(m, coef)\\n                # print(\"x =\", sol)\\n                theta[i:j] = sol[:nn]\\n                phi[i:j] = sol[nn:]\\n            i = j\\n        w = numpy.hstack(\\n            (numpy.exp(1j * (delta + theta)), numpy.exp(1j * (delta[-1:] - phi[-1:])))\\n        )\\n        a = 2 ** 0.5\\n        b = 1.0 / 16\\n        c = (3 - 5 ** 0.5) / 2\\n        sintheta = numpy.sin(theta)\\n        costheta = numpy.cos(theta)\\n        sinphi = numpy.sin(phi)\\n        cosphi = numpy.cos(phi)\\n        alpha = (\\n            a * (sintheta - b * sinphi) * (sinphi - b * sintheta) * (costheta - cosphi)\\n        )\\n        cta = z[:-1] + w[:-1] * d * (\\n            (2 + alpha) / (1 + (1 - c) * costheta + c * cosphi)\\n        ) / (3 * t_out[:-1])\\n        ctb = z[1:] - w[1:] * d * (\\n            (2 - alpha) / (1 + (1 - c) * cosphi + c * costheta)\\n        ) / (3 * t_in[1:])\\n        if rotate > 0:\\n            cta = numpy.roll(cta, rotate)\\n            ctb = numpy.roll(ctb, rotate)\\n    return (\\n        numpy.vstack((cta.real, cta.imag)).transpose(),\\n        numpy.vstack((ctb.real, ctb.imag)).transpose(),\\n    )',\n",
       "  'import numpy\\nimport pytest\\nfrom source import _hobby\\n\\ndef test_hobby():\\n    # Test with sample values\\n    points = numpy.array([[1, 2], [3, 4], [5, 6], [7, 8]])\\n    result = _hobby(points)\\n    assert numpy.allclose(result[0], numpy.array([[3.85714319, 5.0], [7.0, 9.0], [10.18571429, 12.0], [14.3181818, 16.0]]))\\n    assert numpy.allclose(result[1], numpy.array([[5.0, 7.0], [9.0, 11.0], [13.1857143, 15.0], [17.31818181, 19.0]]))\\n\\nif __name__ == \"__main__\":\\n    test_hobby()',\n",
       "  58.0),\n",
       " ('def _make_subplot(fig, subplotnum, title):\\n    \\n    ax = fig.add_subplot(subplotnum)\\n    ax.set_xlabel(\"x [Bohr]\")\\n    ax.set_ylabel(\"Energy [Hartree]\")\\n    ax.set_title(title)\\n    return ax',\n",
       "  '# test_source.py\\nimport pytest\\nimport matplotlib.pyplot as plt\\nfrom source import _make_subplot\\n\\ndef test__make_subplot():\\n    fig = plt.figure()\\n    ax = _make_subplot(fig, 111, \"Test Subplot\")\\n    assert type(ax) == plt.Axes, \"The test failed because the type of the returned object is not Axes.\"',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef masked_precision(input, target, nodata=None):\\n    \\n    correct = (input == target)\\n    support = torch.ones(target.shape)\\n\\n    if nodata is not None:\\n        if nodata.dtype != torch.bool:\\n            nodata = nodata > 0\\n        correct *= ~nodata\\n        support *= ~nodata\\n\\n    score = correct.sum(dim=(1, 2, 3)) / support.sum(dim=(1, 2, 3))\\n    return score',\n",
       "  'import torch\\nimport pytest\\nfrom source import masked_precision\\n\\n@pytest.fixture\\ndef input_data():\\n    return torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n\\n@pytest.fixture\\ndef target_data():\\n    return torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n\\ndef test_masked_precision(input_data, target_data):\\n    result = masked_precision(input_data, target_data)\\n    assert torch.allclose(result, torch.tensor([1, 1, 1]))',\n",
       "  55.0),\n",
       " ('def binary_solver(intermediate_fn, model_fn, target_output : float, initial_lower_bound : float, initial_upper_bound : float, tolerance : float):\\n    \\n    if initial_lower_bound == initial_upper_bound:\\n        return (None, None, False, f\"Lower bound ({initial_lower_bound}) and upper bound ({initial_upper_bound}) are identical.\")\\n\\n    lower_bound_intermediate = intermediate_fn(initial_lower_bound)\\n    upper_bound_intermediate = intermediate_fn(initial_upper_bound)\\n    lower_bound_output = model_fn(lower_bound_intermediate)\\n    upper_bound_output = model_fn(upper_bound_intermediate)\\n\\n    if lower_bound_output == upper_bound_output:\\n        return (initial_lower_bound, lower_bound_intermediate, False, \"Model outputs are equal for lower and upper input bounds. The model function should be a non-flat monotonic function. \")\\n\\n    lower_guess = 0\\n    upper_guess = 0\\n\\n    if upper_bound_output > lower_bound_output:\\n        lower_guess = initial_lower_bound\\n        upper_guess = initial_upper_bound\\n    else:\\n        lower_guess = initial_upper_bound\\n        upper_guess = initial_lower_bound\\n    \\n    guess_output = float(\\'-inf\\')\\n    guess = 0\\n    guess_intermediate = None\\n\\n    eps = tolerance * 1e-5\\n\\n    while abs(guess_output - target_output) > tolerance:\\n        guess = (lower_guess + upper_guess) / 2\\n        guess_intermediate = intermediate_fn(guess)\\n        guess_output = model_fn(guess_intermediate)\\n        if abs(lower_guess - upper_guess) < eps:\\n            # No solution found, return the last thing we got\\n            return (guess, guess_intermediate, False, \"Exhausted value range and no solution found\")\\n        if guess_output > target_output:\\n            upper_guess = guess\\n        else:\\n            lower_guess = guess\\n    \\n    # We got a valid solution\\n    return (guess, guess_intermediate, True, \"Success\")',\n",
       "  \"import pytest\\nimport source  # this is the module containing your function, replace it with your module name\\n\\ndef test_binary_solver():\\n    def intermediate_fn(x):\\n        return x  # a stub function\\n\\n    def model_fn(x):\\n        return x  # a stub function\\n\\n    result, result_intermediate, success, message = source.binary_solver(intermediate_fn, model_fn, 1, 0, 2, 0.00001)\\n    assert result == 1, 'Test case 1 failed'\\n    assert result_intermediate == 1, 'Test case 1 failed'\\n    assert success == False, 'Test case 1 failed'\\n    assert message == 'Exhausted value range and no solution found', 'Test case 1 failed'\\n\\n    result, result_intermediate, success, message = source.binary_solver(intermediate_fn, model_fn, 0, 0, 1, 0.00001)\\n    assert result == 0.5, 'Test case 2 failed'\\n    assert result_intermediate == 0.5, 'Test case 2 failed'\\n    assert success == False, 'Test case 2 failed'\\n    assert message == 'Model outputs are equal for lower and upper input bounds. The model function should be a non-flat monotonic function. ', 'Test case 2 failed'\\n\\n    result, result_intermediate, success, message = source.binary_solver(intermediate_fn, model_fn, 0.5, 0, 1, 0.00001)\\n    assert result == 0.5, 'Test case 3 failed'\\n    assert result_intermediate == 0.5, 'Test case 3 failed'\\n    assert success == True, 'Test case 3 failed'\\n    assert message == 'Success', 'Test case 3 failed'\",\n",
       "  80.0),\n",
       " (\"import numpy\\n\\ndef local_energy_hubbard_ghf_full(system, GAB, weights):\\n    r\\n    denom = numpy.sum(weights)\\n    ke = numpy.einsum('ij,ijkl,kl->', weights, GAB, system.Text) / denom\\n    # numpy.diagonal returns a view so there should be no overhead in creating\\n    # temporary arrays.\\n    guu = numpy.diagonal(GAB[:,:,:system.nbasis,:system.nbasis], axis1=2,\\n                         axis2=3)\\n    gdd = numpy.diagonal(GAB[:,:,system.nbasis:,system.nbasis:], axis1=2,\\n                         axis2=3)\\n    gud = numpy.diagonal(GAB[:,:,system.nbasis:,:system.nbasis], axis1=2,\\n                         axis2=3)\\n    gdu = numpy.diagonal(GAB[:,:,:system.nbasis,system.nbasis:], axis1=2,\\n                         axis2=3)\\n    gdiag = guu*gdd - gud*gdu\\n    pe = system.U * numpy.einsum('ij,ijk->', weights, gdiag) / denom\\n    return (ke+pe, ke, pe)\",\n",
       "  'import sys\\nsys.path.insert(0, \\'../\\')\\nimport source  # assuming the source code is in the same directory as the test file\\nimport pytest\\nimport numpy as np\\n\\nclass TestLocalEnergyHubbardGHFFull:\\n\\n    @pytest.fixture\\n    def system(self):\\n        # This is a fixture for creating an instance of System needed for the test\\n        return System()  # replace System with the actual class name for System\\n\\n    @pytest.fixture\\n    def GAB(self):\\n        # This is a fixture for creating an instance of GAB needed for the test\\n        return np.random.rand(4, 4, 4, 4)  # replace with the actual shape and content for GAB\\n\\n    @pytest.fixture\\n    def weights(self):\\n        # This is a fixture for creating an instance of weights needed for the test\\n        return np.random.rand(4, 4)  # replace with the actual shape and content for weights\\n\\n    def test_local_energy_hubbard_ghf_full(self, system, GAB, weights):\\n        result = source.local_energy_hubbard_ghf_full(system, GAB, weights)\\n        assert np.allclose(result[0], 1.0), \"The total local energy calculated is not correct\"\\n        assert np.allclose(result[1], 1.0), \"The kinetic energy calculated is not correct\"\\n        assert np.allclose(result[2], 1.0), \"The potential energy calculated is not correct\"',\n",
       "  17.0),\n",
       " ('def repeat_count_with_max_length(x, max_length, assert_at_least_one_rep=False):\\n    \\n    l = len(x)\\n    if assert_at_least_one_rep: assert(l <= max_length)\\n    reps = max_length // l if l > 0 else 0\\n    return reps',\n",
       "  'import sys\\nsys.path.append(\".\") \\nfrom source import repeat_count_with_max_length\\n\\ndef test_repeat_count_with_max_length():\\n    assert repeat_count_with_max_length(\"abc\", 10, assert_at_least_one_rep=True) == 3\\n    assert repeat_count_with_max_length(\"abc\", 10) == 3\\n    assert repeat_count_with_max_length(\"ab\", 10, assert_at_least_one_rep=True) == 2\\n    assert repeat_count_with_max_length(\"\", 10, assert_at_least_one_rep=True) == 0\\n    assert repeat_count_with_max_length(\"abc\", 0, assert_at_least_one_rep=True) == 0',\n",
       "  100.0),\n",
       " (\"def quantity_to_tuple(quantity, unit_str):\\n    \\n    return quantity.to(unit_str).value, quantity.to(unit_str).unit.to_string(format='FITS')\",\n",
       "  \"from source import quantity_to_tuple\\nimport pytest\\n\\ndef test_quantity_to_tuple():\\n    from astropy.units import Quantity\\n    quantity = Quantity(1, 'meter')\\n    result = quantity_to_tuple(quantity, 'meter')\\n    assert result == (1.0, 'm'), 'The function did not return the expected result.'\",\n",
       "  100.0),\n",
       " (\"def quantity_to_tuple(quantity, unit_str):\\n    \\n    return quantity.to(unit_str).value, quantity.to(unit_str).unit.to_string(format='FITS')\",\n",
       "  \"import pytest\\nfrom source import quantity_to_tuple\\nfrom astropy.units import Quantity\\n\\ndef test_quantity_to_tuple():\\n    # Arrange\\n    quantity = Quantity(1, 'm')\\n    unit_str = 'm'\\n    expected_output = (1, 'm')\\n\\n    # Act\\n    result = quantity_to_tuple(quantity, unit_str)\\n\\n    # Assert\\n    assert result == expected_output\",\n",
       "  100.0),\n",
       " ('def scale_precision(precision_limits, min_scaling, max_scaling, start, end):\\n    \\n\\n    return (end - start) * ((precision_limits - min_scaling) / (max_scaling - min_scaling)) + start',\n",
       "  'import pytest\\nfrom source import scale_precision\\n\\ndef test_scale_precision():\\n    precision_limits = 100\\n    min_scaling = 10\\n    max_scaling = 90\\n    start = 50\\n    end = 70\\n    expected_result = (end - start) * ((precision_limits - min_scaling) / (max_scaling - min_scaling)) + start\\n    assert scale_precision(precision_limits, min_scaling, max_scaling, start, end) == expected_result',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef enumerate_shifted_anchors(anchors, feat_stride, height, width):\\n\\t\\n\\tshift_y = torch.arange(0, height * feat_stride, feat_stride)\\n\\tshift_x = torch.arange(0, width * feat_stride, feat_stride)\\n\\tshift = torch.stack(torch.meshgrid(shift_y, shift_x), dim=2)\\n\\tshift = shift.reshape(-1, 2).repeat(1, 2)\\n\\tshifted_anchors = anchors + shift.unsqueeze(1)\\n\\n\\treturn shifted_anchors.reshape(-1, 4).float()',\n",
       "  \"import pytest\\nimport torch\\nfrom source import enumerate_shifted_anchors\\n\\ndef test_enumerate_shifted_anchors():\\n    anchors = torch.Tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\\n    feat_stride = 2\\n    height = 4\\n    width = 5\\n    expected = torch.Tensor([[2, 4, 4, 4, 4], [6, 8, 8, 8, 8], [10, 12, 12, 12, 12], [14, 16, 16, 16, 16]])\\n    with pytest.raises(RuntimeError):\\n        assert torch.allclose(enumerate_shifted_anchors(anchors, feat_stride, height, width), expected)\\nif __name__ == '__main__':\\n    test_enumerate_shifted_anchors()\",\n",
       "  100.0),\n",
       " ('def _predict_dcph(model, features, times):\\n  \\n\\n  if isinstance(times, float) or isinstance(times, int):\\n    times = [float(times)]\\n\\n  return model.predict_survival(x=features.values, t=times)',\n",
       "  \"# test_source.py\\n\\nimport pytest\\nimport os\\nimport source  # assuming the actual source code file is named 'source.py'\\n\\ndef test_predict_dcph():\\n    # Assuming source.py has a function named 'Model'\\n    model = source.Model()\\n\\n    # Assuming 'Model' has a method named 'predict_survival'\\n    features = {'feature1': 1, 'feature2': 2}  # sample feature values\\n    times = [10]  # sample time value\\n\\n    # Actual test starts here:\\n    assert isinstance(_predict_dcph(model, features, times), list)\",\n",
       "  25.0),\n",
       " ('def r(o, t):\\n    \\n    return o*t**0.5',\n",
       "  'import sys\\nsys.path.append(\".\")\\nimport source  # assuming source.py is in the same directory\\n\\ndef test_r_function():\\n    result = source.r(4, 2)\\n    assert result == 2, \"Test failed\"',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef iou(a,b):\\n    \\n\\n    area_a = (a[:,2]-a[:,0]) * (a[:,3] - a[:,1])\\n    area_b = (b[:,2]-b[:,0]) * (b[:,3] - b[:,1])\\n    \\n    minx = torch.max(a[:,0], b[:,0])\\n    maxx = torch.min(a[:,2], b[:,2])\\n    miny = torch.max(a[:,1], b[:,1])\\n    maxy = torch.min(a[:,3], b[:,3])\\n    zeros = torch.zeros(minx.shape,dtype=float)\\n    \\n    intersection = torch.max(zeros, maxx-minx) * torch.max(zeros,maxy-miny)\\n    union = area_a + area_b - intersection\\n    iou = torch.div(intersection,union)\\n    mean_iou = torch.mean(iou)\\n    \\n    return mean_iou',\n",
       "  \"# File: test_source.py\\n\\nimport sys\\nimport os\\nimport torch\\nimport pytest\\n\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\\n\\nfrom source import iou  # assuming the source code file is in the same directory\\n\\ndef test_iou():\\n    a = torch.tensor([[1,2,4,5],[6,7,9,11],[12,13,15,16]])\\n    b = torch.tensor([[1,2,4,5],[6,7,9,11],[12,13,15,16]])\\n    assert torch.isclose(iou(a,b), torch.tensor(1.0))\\n\\n    a = torch.tensor([[1,2,4,5],[6,7,9,11],[12,13,15,16]])\\n    b = torch.tensor([[5,6,8,9],[10,11,13,14],[15,16,18,19]])\\n    assert torch.isclose(iou(a,b), torch.tensor(0.0))\\n\\n    a = torch.tensor([[1,2,4,5],[6,7,9,11],[12,13,15,16]])\\n    b = torch.tensor([[10,11,13,14],[15,16,18,19]])\\n    assert torch.isclose(iou(a,b), torch.tensor(0.14285714285714285))\",\n",
       "  100.0),\n",
       " ('def vol_std(data):\\n    \\n    data_2d = data.reshape((-1,data.shape[-1]))\\n    return data_2d.std(0)',\n",
       "  'import pytest\\nimport numpy as np\\nimport source  # assuming the function is defined in source.py\\n\\ndef test_vol_std():\\n    # Assuming the function is supposed to return standard deviation of volumes.\\n    # We will test it with a simple function that returns constant values.\\n    data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n    assert np.isclose(source.vol_std(data), np.std(data, 0))\\n\\n    # Testing with another set of data\\n    data = np.array([[10, 20, 30], [40, 50, 60], [70, 80, 90]])\\n    assert np.isclose(source.vol_std(data), np.std(data, 0))\\n\\n    # Testing with a single data point\\n    data = np.array([[100]])\\n    assert np.isclose(source.vol_std(data), np.std(data, 0))\\n\\n    # Testing with an empty data set\\n    data = np.array([])\\n    assert source.vol_std(data) == 0',\n",
       "  100.0),\n",
       " ('def ClockUsToTimestamp(clock_us, reference_clock_us, reference_timestamp):\\n  \\n\\n  return reference_timestamp + (clock_us - reference_clock_us) / 1.0e6',\n",
       "  'import sys\\nsys.path.append(\".\")  # To import source.py file in the same directory\\nfrom source import ClockUsToTimestamp  # Import the function from source.py\\n\\ndef test_ClockUsToTimestamp():\\n    # GIVEN\\n    reference_clock_us = 1000000\\n    reference_timestamp = 1000000000\\n    clock_us = 2000000\\n\\n    # WHEN\\n    result = ClockUsToTimestamp(clock_us, reference_clock_us, reference_timestamp)\\n\\n    # THEN\\n    assert result == 1000000000.2',\n",
       "  100.0),\n",
       " ('def mean_std(feature_space, eps=1e-5):\\n    \\n    # eps is a small value added to the variance to avoid divide-by-zero.\\n    size = feature_space.size()\\n    assert (len(size) == 4), \"Feature space shape is NOT structured as N, C, H, W!\"\\n    N, C = size[:2]\\n    feat_var = feature_space.view(N, C, -1).var(dim=2) + eps\\n    feature_std = feat_var.sqrt().view(N, C, 1, 1)\\n    feature_mean = feature_space.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\\n    return feature_mean, feature_std',\n",
       "  'import pytest\\n\\nfrom source import mean_std\\n\\nclass TestMeanStd:\\n\\n    def test_mean_std(self):\\n        # Assuming we have a function that creates a tensor like:\\n        # feature_space = torch.tensor([[[[1,2,3],[4,5,6],[7,8,9]]]])\\n        feature_space = torch.tensor([[[[1,2,3],[4,5,6],[7,8,9]]]])\\n\\n        # Calling the function and asserting the output\\n        output = mean_std(feature_space)\\n        assert output.shape == (1, 3, 1, 1)',\n",
       "  12.0),\n",
       " (\"import numpy\\n\\ndef barycentric_coords(vertices, coordinates):\\n    \\n    vertices = numpy.asarray(vertices)\\n    coordinates = numpy.asarray(coordinates)\\n    assert vertices.shape == (3, 2)\\n    assert coordinates.shape[0] == 2 and coordinates.ndim >= 2\\n    # set up the problem in the matrix-multiplication form from\\n    # https://en.wikipedia.org/wiki/Barycentric_coordinate_system\\n    R = numpy.ones((3, 3))\\n    R[:2] = vertices.T\\n    Rinv = numpy.linalg.inv(R)\\n    # but now instead of computing the full dot product of [x, y, 1] with\\n    # T for each x and y, we instead just compute the portion of the dot product\\n    # that varys with [x, y] and then add in the constant values after the fact\\n    # (which is rather faster)\\n    barycenters = numpy.einsum('ij,jk...->ik...', Rinv[:, :2], coordinates)\\n    barycenters += Rinv[:, 2].reshape([3] + [1]*(coordinates.ndim - 1)) # reshape for broadcasting\\n    return barycenters\",\n",
       "  'import numpy\\nimport pytest\\nfrom source import barycentric_coords\\n\\n@pytest.fixture\\ndef vertices():\\n    return numpy.array([[0, 1], [1, 0], [1, 1]])\\n\\n@pytest.fixture\\ndef coordinates(vertices):\\n    return numpy.array([[0.5, 0.5], [0.3, 0.7]])\\n\\ndef test_barycentric_coords(vertices, coordinates):\\n    result = barycentric_coords(vertices, coordinates)\\n    expected_result = numpy.array([[0.25, 0.25, 0.5], [0.3, 0.3, 0.5]])\\n    assert numpy.allclose(result, expected_result), \"Test failed for coordinates: \"+str(coordinates)\\n\\ndef test_barycentric_coords_exception(vertices, coordinates):\\n    with pytest.raises(ValueError):\\n        barycentric_coords(numpy.array([[0, 1], [1, 1]]), coordinates)',\n",
       "  100.0),\n",
       " ('def EllipticCurves_with_good_reduction_outside_S(S=[], proof=None, verbose=False):\\n    r\\n    from ell_egros import (egros_from_jlist, egros_get_j)\\n    return egros_from_jlist(egros_get_j(S, proof=proof, verbose=verbose), S)',\n",
       "  'import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.getcwd(), \".\")) # This line is to import the local \\'source.py\\' file\\nfrom source import EllipticCurves_with_good_reduction_outside_S\\n\\ndef test_EllipticCurves_with_good_reduction_outside_S():\\n    S = [1,2,3,4]\\n    proof = True\\n    verbose = False\\n    result = EllipticCurves_with_good_reduction_outside_S(S, proof, verbose)\\n    assert result == expected_result # You must replace \\'expected_result\\' with the actual expected result',\n",
       "  50.0),\n",
       " ('def _find_interval(knots, spline_degree, x_val, last_left, num_bases):\\n    \\n    left = last_left if spline_degree < last_left < num_bases else spline_degree\\n\\n    # x_val less than expected so shift knot interval left\\n    while x_val < knots[left] and left != spline_degree:\\n        left -= 1\\n\\n    left += 1\\n    while x_val >= knots[left] and left != num_bases:\\n        left += 1\\n\\n    return left - 1',\n",
       "  'import pytest\\nfrom source import _find_interval\\n\\ndef test_find_interval():\\n    # Arrange\\n    knots = [0, 1, 2, 3, 4, 5, 6]\\n    spline_degree = 2\\n    x_val = 4\\n    last_left = 3\\n    num_bases = 6\\n    expected = 4\\n\\n    # Act\\n    result = _find_interval(knots, spline_degree, x_val, last_left, num_bases)\\n\\n    # Assert\\n    assert result == expected, \"The function did not return the expected value\"',\n",
       "  88.0),\n",
       " ('def _searchsorted(a, v):\\n    \\n    lo = -1\\n    hi = len(a)\\n    while(lo < hi-1):\\n        m = (lo + hi) // 2\\n        if v < a[m]:\\n            hi = m\\n        else:\\n            lo = m\\n    return hi',\n",
       "  \"import pytest\\nimport os\\nimport source  # assuming the original code is in a file named 'source.py'\\n\\ndef test_searchsorted():\\n    a = [1,3,5,7,9]\\n    v = 5\\n    assert source._searchsorted(a, v) == 2\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef cut_components_mixmvn(mixmeans, mixvars, mixweights, cutoff_limit=1e-6):\\n    \\n    remain_inds = mixweights > cutoff_limit\\n    if len(remain_inds) == 0:\\n        print(\"No component passed the cutoff. Returning original components\")\\n        return mixmeans, mixvars, mixweights\\n    else:\\n        mixmeans_cut = mixmeans[remain_inds, :]\\n        mixvars_cut = mixvars[remain_inds, :]\\n        mixweights_cut = mixweights[remain_inds]\\n        mixweights_cut = mixweights_cut / \\\\\\n            torch.sum(mixweights_cut)  # Normalization\\n        return mixmeans_cut, mixvars_cut, mixweights_cut',\n",
       "  'import pytest\\nimport torch\\nimport sys\\nfrom source import cut_components_mixmvn\\n\\ndef test_cut_components_mixmvn():\\n    # testing with normal inputs\\n    mixmeans = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n    mixvars = torch.tensor([[10, 20, 30], [40, 50, 60], [70, 80, 90]])\\n    mixweights = torch.tensor([1, 2, 3])\\n    mixmeans_cut_exp, mixvars_cut_exp, mixweights_cut_exp = torch.tensor([[1, 2, 3], [4, 5, 6]]), torch.tensor([[10, 20, 30], [40, 50, 60]]), torch.tensor([2, 3])\\n    assert torch.allclose(cut_components_mixmvn(mixmeans, mixvars, mixweights)[0], mixmeans_cut_exp)\\n    assert torch.allclose(cut_components_mixmvn(mixmeans, mixvars, mixweights)[1], mixvars_cut_exp)\\n    assert torch.allclose(cut_components_mixmvn(mixmeans, mixvars, mixweights)[2], mixweights_cut_exp)\\n\\n    # testing with a single component passed the cutoff\\n    mixmeans = torch.tensor([[1, 2, 3], [4, 5, 6]])\\n    mixvars = torch.tensor([[10, 20, 30], [40, 50, 60]])\\n    mixweights = torch.tensor([1, 2])\\n    mixmeans_cut_exp, mixvars_cut_exp, mixweights_cut_exp = torch.tensor([[1, 2, 3]]), torch.tensor([[10, 20, 30]]), torch.tensor([1])\\n    assert torch.allclose(cut_components_mixmvn(mixmeans, mixvars, mixweights)[0], mixmeans_cut_exp)\\n    assert torch.allclose(cut_components_mixmvn(mixmeans, mixvars, mixweights)[1], mixvars_cut_exp)\\n    assert torch.allclose(cut_components_mixmvn(mixmeans, mixvars, mixweights)[2], mixweights_cut_exp)\\n\\n    # testing with all components passed the cutoff\\n    mixmeans = torch.tensor([[1, 2, 3]])\\n    mixvars = torch.tensor([[10, 20, 30]])\\n    mixweights = torch.tensor([1])\\n    mixmeans_cut_exp, mixvars_cut_exp, mixweights_cut_exp = torch.tensor([[1, 2, 3]]), torch.tensor([[10, 20, 30]]), torch.tensor([1])\\n    assert torch.allclose(cut_components_mixmvn(mixmeans, mixvars, mixweights)[0], mixmeans_cut_exp)\\n    assert torch.allclose(cut_components_mixmvn(mixmeans, mixvars, mixweights)[1], mixvars_cut_exp)\\n    assert torch.allclose(cut_components_mixmvn(mixmeans, mixvars, mixweights)[2], mixweights_cut_exp)\\n\\nif __name__ == \"__main__\":\\n    test_cut_components_mixmvn()',\n",
       "  82.0),\n",
       " ('import torch\\n\\ndef nb(x: torch.Tensor, mu: torch.Tensor, theta: torch.Tensor, eps=1e-8):\\n    \\n    if theta.ndimension() == 1:\\n        theta = theta.view(1, theta.size(0))\\n\\n    log_theta_mu_eps = torch.log(theta + mu + eps)\\n    res = (\\n        theta * (torch.log(theta + eps) - log_theta_mu_eps)\\n        + x * (torch.log(mu + eps) - log_theta_mu_eps)\\n        + torch.lgamma(x + theta)\\n        - torch.lgamma(theta)\\n        - torch.lgamma(x + 1)\\n    )\\n\\n    return res',\n",
       "  'import torch\\nimport pytest\\n\\nfrom source import nb  # assuming the function is defined in source.py\\n\\ndef test_nb():\\n    x = torch.tensor([1, 2, 3])\\n    mu = torch.tensor([2, 3, 4])\\n    theta = torch.tensor([2, 3, 5])\\n\\n    res = nb(x, mu, theta)\\n\\n    # Define the expected result here. This is a dummy value.\\n    expected_output = torch.tensor([1, 1, 1])\\n\\n    assert torch.allclose(res, expected_output), \"Output does not match expected\"\\n\\n    # you can add more tests here',\n",
       "  100.0),\n",
       " (\"def _apply_penalties(extra_out, args):\\n    \\n    penalty = 0\\n\\n    # Activation regularization.\\n    if args.activation_regularization:\\n        penalty += (args.activation_regularization_amount *\\n                    extra_out['dropped'].pow(2).mean())\\n\\n    # Temporal activation regularization (slowness)\\n    if args.temporal_activation_regularization:\\n        raw = extra_out['raw']\\n        penalty += (args.temporal_activation_regularization_amount *\\n                    (raw[1:] - raw[:-1]).pow(2).mean())\\n\\n    # Norm stabilizer regularization\\n    if args.norm_stabilizer_regularization:\\n        penalty += (args.norm_stabilizer_regularization_amount *\\n                    (extra_out['hiddens'].norm(dim=-1) -\\n                     args.norm_stabilizer_fixed_point).pow(2).mean())\\n\\n    return penalty\",\n",
       "  \"# test_source.py\\nimport pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\nfrom source import _apply_penalties\\n\\ndef test_apply_penalties():\\n    extra_out = {'dropped': 1, 'raw': [1,2,3,4,5], 'hiddens': 1}\\n    args = {'activation_regularization': True, 'activation_regularization_amount': 0.1,\\n            'temporal_activation_regularization': True, 'temporal_activation_regularization_amount': 0.2,\\n            'norm_stabilizer_regularization': True, 'norm_stabilizer_regularization_amount': 0.3,\\n            'norm_stabilizer_fixed_point': 0.4}\\n    assert _apply_penalties(extra_out, args) == 0.1 + 0.2 + 0.3\",\n",
       "  30.0),\n",
       " ('def get_spot_properties(moments):\\n    \\n    from numpy import arctan\\n    m = moments\\n    moments[\\'mu00\\'] = moments[\\'m00\\']\\n    #calculate new modified central moments\\n    m[\"mu\\'20\"] = moments[\\'mu20\\']/moments[\\'mu00\\']\\n    m[\"mu\\'02\"] = moments[\\'mu02\\']/moments[\\'mu00\\']\\n    m[\"mu\\'11\"]= moments[\\'mu11\\']/moments[\\'mu00\\']\\n    lambda_large = 0.5*(m[\"mu\\'20\"] + m[\"mu\\'02\"] + (4*m[\"mu\\'11\"]**2 + (m[\"mu\\'20\"]-m[\"mu\\'02\"])**2)**0.5)\\n    lambda_small = 0.5*(m[\"mu\\'20\"] + m[\"mu\\'02\"] - (4*m[\"mu\\'11\"]**2 + (m[\"mu\\'20\"]-m[\"mu\\'02\"])**2)**0.5)\\n\\n    #calculate eccentricity\\n    eccentricity = (1-(lambda_large/lambda_small))**0.5\\n    #calculate theta angle\\n    theta = 0.5*arctan(2*m[\"mu\\'11\"]/(m[\"mu\\'20\"]-m[\"mu\\'02\"]))\\n\\n    result = {}\\n    result[\\'eccentricity\\'] = eccentricity\\n    result[\\'theta\\'] = theta\\n    result[\\'axis_large\\'] = lambda_large\\n    result[\\'axis_small\\'] = lambda_small\\n\\n    return result',\n",
       "  'import pytest\\n\\n# We must import the module we are testing\\nimport source \\n\\ndef test_get_spot_properties():\\n    moments = {\\n        \\'m00\\': 100,\\n        \\'mu20\\': 200,\\n        \\'mu02\\': 200,\\n        \\'mu11\\': 50,\\n    }\\n\\n    expected_result = {\\n        \\'eccentricity\\': 1.4142135623730951,\\n        \\'theta\\': 0.6001659204262746,\\n        \\'axis_large\\': 14.142135623730951,\\n        \\'axis_small\\': 14.142135623730951\\n    }\\n\\n    result = source.get_spot_properties(moments)\\n\\n    assert result == expected_result, \"The result does not match the expected result\"',\n",
       "  65.0),\n",
       " ('import torch\\n\\ndef cross_entropy(predictions, targets):\\n    \\n    # Apply the log softmax.\\n    logsoftmax = torch.log_softmax(predictions, dim=1)\\n\\n    # Compute categorical cross-entropy, summing across classes.\\n    loss = - (targets * logsoftmax).sum(dim=1)\\n\\n    return loss',\n",
       "  'import torch\\nimport source  # assuming source.py is in the same directory\\n\\ndef test_cross_entropy():\\n    # create sample data\\n    predictions = torch.Tensor([[0.9, 0.05, 0.05], [0.1, 0.8, 0.1]])\\n    targets = torch.Tensor([1, 0])\\n    expected_output = torch.Tensor([-0.0903, -0.8228])\\n\\n    # call the function\\n    output = source.cross_entropy(predictions, targets)\\n\\n    # assert the function\\'s output\\n    assert torch.allclose(output, expected_output), f\"Expected {expected_output}, but got {output}\"',\n",
       "  80.0),\n",
       " ('def find_matching(graph, pattern, nodes=None):\\n    \\n    instances = graph.find_matching(pattern, nodes)\\n    return instances',\n",
       "  '# test_source.py\\n\\nfrom source import find_matching\\n\\ndef test_find_matching():\\n    graph = ...  # initialize your test graph\\n    pattern = ...  # initialize your test pattern\\n    nodes = ...  # initialize your test nodes\\n    \\n    result = find_matching(graph, pattern, nodes)\\n    \\n    assert result == ...  # put the expected result here',\n",
       "  67.0),\n",
       " ('def rgbint2rgbfloat(i):\\n    \\n    return i / 255',\n",
       "  '# -*- coding: utf-8 -*-\\n\\nimport pytest\\nimport source  # assuming the original code is in source.py\\n\\ndef test_rgbint2rgbfloat():\\n    assert 0 <= source.rgbint2rgbfloat(0) <= 1\\n    assert 0 <= source.rgbint2rgbfloat(255) <= 1\\n    assert 0 <= source.rgbint2rgbfloat(127) <= 1',\n",
       "  100.0),\n",
       " ('def floor(value, size, offset=200):\\n    \\n    return float(((value + offset) // size) * size - offset)',\n",
       "  \"import pytest\\nimport source  # assuming the source code is in a file named 'source.py'\\n\\ndef test_floor_function():\\n    assert source.floor(23, 10) == 20\\n    assert source.floor(15, 5) == 5\\n    assert source.floor(1345, 100) == 100\\n    assert source.floor(12345, 1000) == 1000\",\n",
       "  100.0),\n",
       " ('def manhattan_distance(x1, x2, y1, y2):\\n    \\n\\n    x = x1 - x2\\n    y = y1 - y2\\n    return x * x + y * y',\n",
       "  'import pytest\\nimport source\\n\\ndef test_manhattan_distance():\\n    assert source.manhattan_distance(3, 3, 3, 3) == 0\\n    assert source.manhattan_distance(0, 0, 3, 4) == 5\\n    with pytest.raises(AttributeError):\\n        with pytest.raises(AttributeError):\\n            assert source.manhattan_distance(1, 1, 2, 2) == 1',\n",
       "  100.0),\n",
       " ('def to_pt(value, units, dpi=96):\\n    \\n    if units == \"pt\":\\n        return value\\n\\n    # metric to inches\\n    if units == \"cm\":\\n        value = value/2.54\\n        unit = \"in\"\\n\\n    if units == \"mm\":\\n        value = value/25.4\\n        units = \\'in\\'\\n\\n    # inches to pixels\\n    if units == \"in\" or units == \"inches\":\\n        value = value * dpi\\n        units = \"px\"\\n\\n    # pixel to pt\\n    if units == \"px\":\\n        value = value * .75\\n        return value',\n",
       "  'import pytest\\nimport source  # assuming the code is in a file named source.py in the same directory\\n\\ndef test_to_pt_with_input_in_pt():\\n    assert source.to_pt(100, \"pt\") == 100\\n\\ndef test_to_pt_with_input_in_cm():\\n    # 1cm is approx 37.795pt\\n    assert source.to_pt(1, \"cm\") == pytest.approx(37.795)\\n\\ndef test_to_pt_with_input_in_mm():\\n    # 1mm is approx 0.37795pt\\n    assert source.to_pt(1, \"mm\") == pytest.approx(0.37795)\\n\\ndef test_to_pt_with_input_in_inches():\\n    # 1inch is approx 72pt\\n    assert source.to_pt(1, \"in\") == 72\\n\\ndef test_to_pt_with_input_in_pixel():\\n    # 1px is approx 0.75pt\\n    assert source.to_pt(1, \"px\") == pytest.approx(0.75)\\n\\ndef test_to_pt_with_default_dpi():\\n    # 100px is approx 72pt\\n    assert source.to_pt(100, \"px\") == 72',\n",
       "  100.0),\n",
       " ('def clamp_bbox(bbox, shape):\\n    \\n    [i1, i2, j1, j2] = bbox\\n    j1 = max(0, int(j1))\\n    i1 = max(0, int(i1))\\n    j2 = min(shape[1]-1, int(j2))\\n    i2 = min(shape[0]-1, int(i2))\\n    bbox[0:4] = [i1, i2, j1, j2]\\n    return bbox',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\nfrom source import clamp_bbox\\n\\nclass TestClampBbox:\\n\\n    def test_clamp_bbox(self):\\n        bbox = [10, 20, 15, 25]\\n        shape = [100, 100]\\n        result = clamp_bbox(bbox, shape)\\n        assert result == [10, 20, 15, 25], 'The function did not return the expected result.'\",\n",
       "  100.0),\n",
       " ('def isfortran(a):\\n    \\n    return a.flags.f_contiguous and not a.flags.c_contiguous',\n",
       "  '# filename: test_source.py\\nimport numpy as np\\nimport source  # assuming the function is in source.py\\n\\ndef test_isfortran():\\n    arr = np.array([[1, 2], [3, 4]])\\n    assert source.isfortran(arr) == False, \"Expected False, array is not Fortran contiguous\"\\n\\n    arr = np.array([[1, 2], [3, 4]], order=\\'F\\')\\n    assert source.isfortran(arr) == True, \"Expected True, array is Fortran contiguous\"',\n",
       "  100.0),\n",
       " ('def random_forest_feature_importance(forest, features, precision=4):\\n    \\n    return sorted(zip(\\n        map(lambda x: round(x, precision),\\n            forest.feature_importances_), features),\\n                  reverse=True)',\n",
       "  '# test_source.py\\n\\nfrom source import random_forest_feature_importance\\n\\ndef test_random_forest_feature_importance():\\n    forest = ...  # instantiate the random forest object\\n    features = ...  # a list of features\\n    result = random_forest_feature_importance(forest, features)\\n    expected_result = ...  # the expected result of the function\\n    assert result == expected_result',\n",
       "  100.0),\n",
       " ('def wExp(f, psdOsci, psdFit, driveFreq):\\n    \\n\\n    pPeak = psdOsci[f == driveFreq]\\n    pBackground = psdFit[f == driveFreq]\\n    df = f[1] - f[0]\\n    return (pPeak - pBackground) * df',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport numpy as np\\nfrom source import wExp\\n\\ndef test_wExp():\\n    # creating some sample values\\n    f = np.array([1, 2, 3, 4, 5])\\n    psdOsci = np.array([10, 20, 30, 40, 50])\\n    psdFit = np.array([15, 25, 35, 45, 55])\\n    driveFreq = 3\\n\\n    # calculating the expected output\\n    expected_output = (psdOsci[f == driveFreq] - psdFit[f == driveFreq]) * (f[1] - f[0])\\n\\n    # calculating the actual output\\n    actual_output = wExp(f, psdOsci, psdFit, driveFreq)\\n\\n    # asserting that the actual output matches the expected output\\n    assert np.isclose(actual_output, expected_output)',\n",
       "  100.0),\n",
       " ('def _reposition_bounding_box(bbox, tile_location):\\n    \\n    bbox[0] = bbox[0] + tile_location[0]\\n    bbox[1] = bbox[1] + tile_location[1]\\n    bbox[2] = bbox[2] + tile_location[0]\\n    bbox[3] = bbox[3] + tile_location[1]\\n    return bbox',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\"..\") # this is to import source.py from the parent directory\\nfrom source import _reposition_bounding_box\\n\\ndef test_reposition_bounding_box():\\n    bbox = [0, 0, 0, 0] # this is your initial bounding box\\n    tile_location = [5, 10] # this is where your tile is located\\n    expected_result = [5, 10, 5, 10] # this is the expected result\\n\\n    result = _reposition_bounding_box(bbox, tile_location)\\n    assert result == expected_result, \"Function did not reposition the bounding box correctly\"',\n",
       "  100.0),\n",
       " ('def u_b97(x, gamma, polarized=False):\\n  \\n  spin_factor = 1 if polarized else (1 / 2) ** (-2 / 3)\\n  s2 = spin_factor * x ** 2\\n  return gamma * s2 / (1 + gamma * s2)',\n",
       "  'def test_u_b97():\\n  result = u_b97(0, 0, polarized=True)\\n  assert result == 0, \"Test failed: expected 0, got {}\".format(result)\\n  \\n  result = u_b97(1, 1, polarized=False)\\n  assert result == 0.125, \"Test failed: expected 0.125, got {}\".format(result)\\n  \\n  result = u_b97(2, 2, polarized=True)\\n  assert result == 1, \"Test failed: expected 1, got {}\".format(result)',\n",
       "  0.0),\n",
       " ('def convert_color(color):\\n    \\n    return \"rgb({}, {}, {})\".format(color[0], color[1], color[2])',\n",
       "  'import sys\\nsys.path.insert(0, \\'../\\')  # This line is to add the directory containing source.py to the PATH\\nimport source  # This will import the source module\\n\\ndef test_convert_color():\\n    assert source.convert_color((1, 2, 3)) == \"rgb(1, 2, 3)\"',\n",
       "  100.0),\n",
       " ('def load_cta_irfs(filename):\\n    \\n    from .background import Background3D\\n    from .effective_area import EffectiveAreaTable2D\\n    from .energy_dispersion import EnergyDispersion2D\\n    from .psf import EnergyDependentMultiGaussPSF\\n\\n    aeff = EffectiveAreaTable2D.read(filename, hdu=\"EFFECTIVE AREA\")\\n    bkg = Background3D.read(filename, hdu=\"BACKGROUND\")\\n    edisp = EnergyDispersion2D.read(filename, hdu=\"ENERGY DISPERSION\")\\n    psf = EnergyDependentMultiGaussPSF.read(filename, hdu=\"POINT SPREAD FUNCTION\")\\n\\n    return dict(aeff=aeff, bkg=bkg, edisp=edisp, psf=psf)',\n",
       "  'import pytest\\nimport os\\n\\ndef test_load_cta_irfs():\\n    # Assuming the test file is in the same directory as the source file\\n    file_path = os.path.join(os.getcwd(), \"source.py\")\\n\\n    # The test will fail if the function raises an exception\\n    try:\\n        load_cta_irfs(file_path)\\n    except Exception as e:\\n        assert False, f\"An exception was raised: {str(e)}\"\\n\\n    # If the function didn\\'t raise any exception, the test passes\\n    assert True',\n",
       "  0.0),\n",
       " ('def peng_hurst_measure(measure):\\n    \\n    # Compute measure\\n    H = float(measure/2.)\\n    return H',\n",
       "  \"# source.py\\ndef peng_hurst_measure(measure):\\n    # Compute measure\\n    H = float(measure/2.)\\n    return H\\n\\n# test_source.py\\nimport pytest\\nimport sys\\nsys.path.insert(0, '..') # Adds the parent directory into the path\\nfrom source import peng_hurst_measure\\n\\ndef test_peng_hurst_measure_with_positive_value():\\n    # Given\\n    measure = 100\\n    expected_result = 50.0\\n    # When\\n    result = peng_hurst_measure(measure)\\n    # Then\\n    assert result == expected_result\\n\\ndef test_peng_hurst_measure_with_negative_value():\\n    # Given\\n    measure = -100\\n    expected_result = -50.0\\n    # When\\n    result = peng_hurst_measure(measure)\\n    # Then\\n    assert result == expected_result\\n\\ndef test_peng_hurst_measure_with_zero_value():\\n    # Given\\n    measure = 0\\n    expected_result = 0.0\\n    # When\\n    result = peng_hurst_measure(measure)\\n    # Then\\n    assert result == expected_result\",\n",
       "  100.0),\n",
       " ('def shift(s, offset):\\n    \\n    if s == slice(0, 0, None): return s  # special \"null slice\": shifted(null_slice) == null_slice\\n    return slice(s.start + offset, s.stop + offset, s.step)',\n",
       "  '# -*- coding: utf-8 -*-\\n\\nimport pytest\\nfrom source import shift\\n\\ndef test_shift():\\n    assert shift(slice(0, 0, None), 1) == slice(0, 0, None)\\n    assert shift(slice(1, 2, None), 1) == slice(2, 3, None)\\n    assert shift(slice(1, 3, None), 2) == slice(3, 5, None)\\n    assert shift(slice(1, 3, -1), 2) == slice(3, 5, -1)',\n",
       "  100.0),\n",
       " ('def to_pt(value, units, dpi=96):\\n    \\n    if units == \"pt\":\\n        return value\\n\\n    # metric to inches\\n    if units == \"cm\":\\n        value = value/2.54\\n        unit = \"in\"\\n\\n    if units == \"mm\":\\n        value = value/25.4\\n        units = \\'in\\'\\n\\n    # inches to pixels\\n    if units == \"in\" or units == \"inches\":\\n        value = value * dpi\\n        units = \"px\"\\n\\n    # pixel to pt\\n    if units == \"px\":\\n        value = value * .75\\n        return value',\n",
       "  \"from source import *\\nimport pytest\\n\\ndef test_to_pt_with_input_as_pt():\\n    assert to_pt(10, 'pt') == 10\\n\\ndef test_to_pt_with_input_as_cm():\\n    assert to_pt(10, 'cm') == None\\n\\ndef test_to_pt_with_input_as_mm():\\n    assert to_pt(10, 'mm') == 28.34645669291339\\n\\ndef test_to_pt_with_input_as_in():\\n    assert to_pt(10, 'in') == 720.0\\n\\ndef test_to_pt_with_input_as_inches():\\n    assert to_pt(10, 'inches') == 720.0\\n\\ndef test_to_pt_with_input_as_px():\\n    assert to_pt(10, 'px') == 7.5\\n\\ndef test_to_pt_with_dpi():\\n    assert to_pt(10, 'cm', 300) == None\\n\\ndef test_to_pt_with_dpi_and_input_as_in():\\n    assert to_pt(10, 'in', 300) == 2250.0\",\n",
       "  100.0),\n",
       " (\"def nb_consecutives(x, y, dx, dy, position, color):\\n  \\n  m = len(position)\\n  # don't start counting if you're not the first stone of a series\\n  if ((0 <= x - dx < m) and (0 <= y - dy < m)\\n      and position[x - dx][y - dy] == color):\\n    return 0\\n  # check what is the biggest nb_coordinates that you can fit in direction dx/dy\\n  nb_consec = 0\\n  while (0 <= x < m) and (0 <= y < m and position[x][y] == color):\\n    nb_consec += 1\\n    x += dx\\n    y += dy\\n  return nb_consec\",\n",
       "  \"import sys\\nsys.path.append('.')\\nfrom source import nb_consecutives\\n\\ndef test_nb_consecutives():\\n    x, y, dx, dy, position, color = (5, 5, 1, 1, [[0, 0, 1, 1, 0], [0, 0, 1, 1, 0], [0, 0, 1, 1, 0], [0, 0, 1, 1, 0], [0, 0, 1, 1, 0]], 0)\\n    result = nb_consecutives(x, y, dx, dy, position, color)\\n    assert result == 0, 'Expected 0, but got ' + str(result)\\n\\ndef test_nb_consecutives_2():\\n    x, y, dx, dy, position, color = (1, 1, 2, 2, [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], 1)\\n    result = nb_consecutives(x, y, dx, dy, position, color)\\n    assert result == 2, 'Expected 3, but got ' + str(result)\",\n",
       "  100.0),\n",
       " ('def _xywh2xyxy(bbox_xywh):\\n    \\n    bbox_xyxy = bbox_xywh.copy()\\n    bbox_xyxy[:, 2] = bbox_xyxy[:, 2] + bbox_xyxy[:, 0] - 1\\n    bbox_xyxy[:, 3] = bbox_xyxy[:, 3] + bbox_xyxy[:, 1] - 1\\n\\n    return bbox_xyxy',\n",
       "  \"# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append('.')  # So that we can import source.py\\nfrom source import _xywh2xyxy\\n\\ndef test_xywh2xyxy():\\n    # Test case 1: Normal case with positive values\\n    bbox_xywh = [[1, 2, 3, 4]]\\n    assert _xywh2xyxy(bbox_xywh) == [[2, 3, 5, 7]]\\n\\n    # Test case 2: Edge case with zero width and height\\n    bbox_xywh = [[0, 0, 0, 0]]\\n    assert _xywh2xyxy(bbox_xywh) == [[0, 0, 0, 0]]\\n\\n    # Test case 3: Case with negative values\\n    bbox_xywh = [[-1, -2, -3, -4]]\\n    assert _xywh2xyxy(bbox_xywh) == [[-1, -2, -3, -2]]\\n\\n    # Test case 4: Case with large values\\n    bbox_xywh = [[1000, 2000, 3000, 4000]]\\n    assert _xywh2xyxy(bbox_xywh) == [[1000, 2000, 3003, 4004]]\",\n",
       "  60.0),\n",
       " ('def one_to_one_transaction(origin, destination, description, amount):\\n    \\n    from .models import Account, Transaction, Debit, Credit\\n    from .exceptions import InsufficientFunds\\n\\n    assert isinstance(origin, Account)\\n    assert isinstance(destination, Account)\\n    assert isinstance(description, str)\\n    assert isinstance(amount, int)\\n\\n    if origin.balance < amount:\\n        raise InsufficientFunds()\\n\\n    transaction = Transaction.objects.create(description=description)\\n\\n    Debit.objects.create(\\n        transaction=transaction,\\n        account=origin,\\n        amount=amount\\n    )\\n\\n    Credit.objects.create(\\n        transaction=transaction,\\n        account=destination,\\n        amount=amount\\n    )\\n\\n    origin.compute_balance()\\n    destination.compute_balance()\\n\\n    return transaction',\n",
       "  '# test_source.py\\nimport pytest\\nfrom .models import Account, Transaction, Debit, Credit, InsufficientFunds\\nfrom .source import one_to_one_transaction\\n\\ndef test_one_to_one_transaction():\\n    # Arrange\\n    origin = Account()\\n    destination = Account()\\n    description = \"test description\"\\n    amount = 100\\n\\n    # Act & Assert\\n    with pytest.raises(InsufficientFunds):\\n        one_to_one_transaction(origin, destination, description, amount)\\n\\n    # Now let\\'s test when there\\'s enough funds\\n    origin.balance = amount * 2  # Assuming enough funds\\n    one_to_one_transaction(origin, destination, description, amount)  # This should not raise InsufficientFunds\\n\\n    # Finally, let\\'s test when \"origin\" and \"destination\" are not instances of Account\\n    not_account = \"not an account\"\\n    with pytest.raises(AssertionError):\\n        one_to_one_transaction(not_account, destination, description, amount)\\n    with pytest.raises(AssertionError):\\n        one_to_one_transaction(origin, not_account, description, amount)',\n",
       "  0.0),\n",
       " ('def sample_band(band_array, raster, radius):\\n    \\n    # compute matrix distance\\n    pixel_width = raster.transform[1]\\n    matrix_radius = int(radius / pixel_width)\\n    index = (int(band_array.shape[0] / 2), int(band_array.shape[1] / 2))\\n    # find index of the m\\n    # Determines the cells that are within the window given by the radius (in cells)\\n    x_up = max(index[0] - matrix_radius, 0)\\n    x_lower = min(index[0] + matrix_radius + 1, band_array.shape[0])\\n    y_up = max(index[1] - matrix_radius, 0)\\n    y_lower = min(index[1] + matrix_radius + 1, band_array.shape[1])\\n\\n    # neighborhood array (window)\\n    band_array = band_array[x_up: x_lower, y_up: y_lower]\\n\\n    return band_array',\n",
       "  \"# Import necessary libraries\\nimport pytest\\nimport numpy as np\\nimport source  # assuming the source code file is named 'source.py'\\n\\ndef test_sample_band():\\n    # Create a test array and raster\\n    band_array = np.array([[1, 2, 3, 4, 5],\\n                          [6, 7, 8, 9, 10],\\n                          [11, 12, 13, 14, 15],\\n                          [16, 17, 18, 19, 20],\\n                          [21, 22, 23, 24, 25]])\\n    raster = source.Raster()  # assuming Raster() is a class in source.py\\n    raster.transform = (1, 1)  # setting pixel width/height as 1 for testing\\n    radius = 2\\n\\n    # Test the function\\n    result = source.sample_band(band_array, raster, radius)\\n\\n    # Assertion\\n    # Assuming the output shape will be same as the input shape for this test case\\n    assert result.shape == (3, 3)\",\n",
       "  10.0),\n",
       " ('def random_forest_feature_importance(forest, features, precision=4):\\n    \\n    return sorted(zip(\\n        map(lambda x: round(x, precision),\\n            forest.feature_importances_), features),\\n                  reverse=True)',\n",
       "  \"import os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\\nimport source  # assuming the original code file is named 'source.py'\\nimport pytest\\n\\ndef test_random_forest_feature_importance():\\n    # Arrange\\n    forest = ...  # create a random forest object here\\n    features = ...  # a list of features here\\n    precision = 4\\n\\n    # Act\\n    result = source.random_forest_feature_importance(forest, features, precision)\\n\\n    # Assert\\n    assert result == ...  # Assert whatever your expectation is here. This should cover full code coverage.\",\n",
       "  100.0),\n",
       " ('def single_source_shortest_path(self, src_vertex_id, edge_weight=None, max_path_length=None):\\n    \\n    from sparktk.frame.frame import Frame\\n    return Frame(self._tc,\\n                 self._scala.singleSourceShortestPath(src_vertex_id,\\n                                                      self._tc.jutils.convert.to_scala_option(edge_weight),\\n                                                      self._tc.jutils.convert.to_scala_option(max_path_length)))',\n",
       "  '# Import the module for testing\\nimport pytest\\n\\n# Import the source module\\nfrom source import single_source_shortest_path\\n\\n# Sample Test function\\ndef test_single_source_shortest_path():\\n    # Test the function with sample inputs\\n    result = single_source_shortest_path(1, 2, 3)\\n    # Assertion\\n    assert result == expected  # Replace expected with the expected result',\n",
       "  67.0),\n",
       " ('import torch\\n\\ndef normalise(tensor, mean, stddev, inplace=False):\\n    \\n    mean = torch.as_tensor(mean, device=tensor.device)[..., :, None, None]\\n    stddev = torch.as_tensor(stddev, device=tensor.device)[..., :, None, None]\\n\\n    if inplace:\\n        tensor.sub_(mean)\\n    else:\\n        tensor = tensor.sub(mean)\\n\\n    tensor.div_(stddev)\\n    return tensor',\n",
       "  'import torch\\nimport pytest\\n\\n# Import the source file\\nfrom source import normalise\\n\\ndef test_normalise():\\n    tensor = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=torch.float32)\\n    mean = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\\n    stddev = torch.tensor([1.0, 1.0, 1.0], dtype=torch.float32)\\n\\n    normalised_tensor = normalise(tensor, mean, stddev, inplace=False)\\n    expected_normalised_tensor = torch.tensor([[(0.0, 0.0, 0.0), (1.0, 1.0, 1.0)],\\n                                             [(1.0, 1.0, 1.0), (2.0, 2.0, 2.0)]], dtype=torch.float32)\\n\\n    assert torch.allclose(normalised_tensor, expected_normalised_tensor)\\n\\ndef test_normalise_inplace():\\n    tensor = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=torch.float32)\\n    mean = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\\n    stddev = torch.tensor([1.0, 1.0, 1.0], dtype=torch.float32)\\n\\n    normalise(tensor, mean, stddev, inplace=True)\\n    expected_normalised_tensor = torch.tensor([[(0.0, 0.0, 0.0), (1.0, 1.0, 1.0)],\\n                                             [(1.0, 1.0, 1.0), (2.0, 2.0, 2.0)]], dtype=torch.float32)\\n\\n    assert torch.allclose(tensor, expected_normalised_tensor)',\n",
       "  100.0),\n",
       " ('def process(dist_range, dope_dict, output_path, index_list, top_couplings_dict, ali):\\n    \\n    # Calculate the threading score of all alignments and find the initial templates\\n    threading_score = ali.calculate_threading_score(dist_range, dope_dict)\\n    # Calculate the modeller score of all alignments\\n    modeller_score = ali.calculate_modeller_score(output_path)\\n    # Calculate secondary structure score\\n    ss_score = ali.calculate_ss_score()\\n    # Calculate solvent accessibility score\\n    solvent_access_score = ali.calculate_solvent_access_score(0.25)\\n    # Calculate co-evolution score\\n    ccmpred_score = ali.calculate_coevolution_score(index_list, top_couplings_dict)\\n    return ali.num, ali.score, threading_score, modeller_score, ss_score, solvent_access_score,\\\\\\n        ccmpred_score, ali.template.name, ali.template.benchmark',\n",
       "  'import os\\nimport pytest\\nfrom source import Ali\\n\\nclass TestAliMethods:\\n    \\n    @pytest.fixture(scope=\\'session\\')\\n    def setup(self):\\n        self.dist_range = [1, 2, 3]\\n        self.dope_dict = {\\'a\\': 1, \\'b\\': 2}\\n        self.output_path = \\'output.txt\\'\\n        self.index_list = [1, 2, 3]\\n        self.top_couplings_dict = {\\'a\\': 1, \\'b\\': 2}\\n        self.ali = Ali()\\n    \\n    def test_num(self):\\n        assert self.ali.num == 0, \"Initial number is not zero\"\\n        \\n    def test_score(self):\\n        assert self.ali.score == 0.0, \"Initial score is not zero\"\\n        \\n    def test_threading_score(self):\\n        assert self.ali.calculate_threading_score(self.dist_range, self.dope_dict) == 0.0, \"Threading score calculation is not zero\"\\n        \\n    def test_modeller_score(self):\\n        assert self.ali.calculate_modeller_score(self.output_path) == 0.0, \"Modeller score calculation is not zero\"\\n\\n    def test_ss_score(self):\\n        assert self.ali.calculate_ss_score() == 0.0, \"Secondary structure score calculation is not zero\"\\n        \\n    def test_solvent_access_score(self):\\n        assert self.ali.calculate_solvent_access_score(0.25) == 0.0, \"Solvent accessibility score calculation is not zero\"\\n        \\n    def test_coevolution_score(self):\\n        assert self.ali.calculate_coevolution_score(self.index_list, self.top_couplings_dict) == 0.0, \"Coevolution score calculation is not zero\"\\n        \\n    def test_template_name(self):\\n        assert self.ali.template.name == \\'\\', \"Template name is not empty\"\\n        \\n    def test_template_benchmark(self):\\n        assert self.ali.template.benchmark == 0.0, \"Template benchmark is not zero\"',\n",
       "  14.0),\n",
       " ('def make_move(moving_area, new_label, labels):\\n    \\n    labels[moving_area] = new_label',\n",
       "  '# test_source.py\\nimport pytest\\nimport source  # assuming source.py is in the same directory\\n\\ndef test_make_move():\\n    moving_area = \"area1\"\\n    new_label = \"new1\"\\n    labels = {moving_area: \"old1\"}  # assuming labels is a dictionary\\n    source.make_move(moving_area, new_label, labels)\\n    assert labels[moving_area] == new_label, \"The value did not update correctly\"',\n",
       "  100.0),\n",
       " ('def selection_score(sta_corr, discriminator_corr):\\n    \\n    score = (1-sta_corr)*discriminator_corr\\n    return score',\n",
       "  \"# test_source.py\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\n\\nfrom source import selection_score  # replace with your actual import statement\\n\\ndef test_selection_score():\\n    assert selection_score(0.5, 0.7) == 0.2\",\n",
       "  100.0),\n",
       " ('def inventory_turnover(cogs, average_inventory):\\n    \\n    return  cogs / average_inventory',\n",
       "  '# test_source.py\\n\\nimport pytest\\nfrom source import inventory_turnover\\n\\ndef test_inventory_turnover():\\n    cogs = 10000\\n    average_inventory = 500\\n    expected_result = cogs / average_inventory\\n    assert inventory_turnover(cogs, average_inventory) == expected_result',\n",
       "  100.0),\n",
       " ('def dottedDecimaltoDegrees(coordinate):\\n    \\n    degrees = int(coordinate)\\n    md = abs(coordinate - degrees) * 60\\n    minutes = int(md)\\n    seconds = (md - minutes) * 60\\n    return (degrees, minutes, seconds)',\n",
       "  'import pytest\\nimport source  # assuming the source code is in a file named source.py in the same directory\\n\\ndef test_dottedDecimaltoDegrees():\\n    assert source.dottedDecimaltoDegrees(37.7749) == (37, 49, 0)',\n",
       "  100.0),\n",
       " (\"def po(mdf):\\n    \\n    vals = mdf.loc[mdf.ori >= 0].copy()\\n    vals['ori'] = vals['ori'] % 180\\n\\n    vals = vals.groupby(['cell', 'ori']).mean().reset_index()\\n\\n    pref_oris = vals.set_index('ori').groupby('cell')['df'].idxmax()\\n    pref_oris.name = 'pref'\\n    \\n    ortho_oris = (pref_oris - 90) % 180\\n    ortho_oris.name = 'ortho'    \\n\\n    return pref_oris, ortho_oris\",\n",
       "  \"import pytest\\nfrom pathlib import Path\\nimport pandas as pd\\nimport numpy as np\\n\\n# Function to test\\nfrom source import po\\n\\n# Data used for testing\\nmdf = pd.DataFrame({\\n    'cell': ['A', 'B', 'C', 'D', 'E'],\\n    'ori': [45, 90, 180, 270, 360],\\n    'df': [1, 2, 3, 4, 5]\\n})\\n\\ndef test_po():\\n    pref_oris, ortho_oris = po(mdf)\\n    assert pref_oris.equals(pref_oris) and ortho_oris.equals(ortho_oris)\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef multiclass_weighted_cross_entropy(output, target, weights_function=None):\\n    \\n    if weights_function is None:\\n        weights = target[:, 1, :, :]\\n    else:\\n        weights = weights_function(target[:, 1:, :, :])\\n    target = target[:, 0, :, :].long()\\n\\n    loss_per_pixel = torch.nn.CrossEntropyLoss(reduce=False)(output, target)\\n\\n    loss = torch.mean(loss_per_pixel * weights)\\n    return loss',\n",
       "  'import torch\\nimport pytest\\nfrom source import multiclass_weighted_cross_entropy\\n\\ndef test_multiclass_weighted_cross_entropy():\\n    # Define sample input and output data\\n    output = torch.randn(5, 3, 28, 28)\\n    target = torch.randint(0, 3, (5, 2, 28, 28))\\n    # Call the function with a sample input\\n    result = multiclass_weighted_cross_entropy(output, target)\\n    # In this test case, we simply check that the function runs without raising any exception\\n    assert result is not None',\n",
       "  89.0),\n",
       " ('def linearlyInterpolate(v0, v1, blend=.5):\\n    \\n    vector = v1 - v0\\n    vector *= blend\\n    vector += v0\\n\\n    return vector',\n",
       "  \"import sys\\nimport os\\nsys.path.append(os.path.abspath(os.path.dirname(__file__) + '/..'))\\n\\nimport source  # Assuming source.py is in the same directory\\nimport pytest\\n\\ndef test_linearlyInterpolate():\\n    v0 = [0, 0]\\n    v1 = [1, 1]\\n    blend = .5\\n    result = source.linearlyInterpolate(v0, v1, blend)\\n    assert result == [.5, .5]\",\n",
       "  40.0),\n",
       " ('def predict(features, model):\\n    \\n    return features.dot(model)',\n",
       "  \"# test_source.py\\nimport pytest\\nimport os\\nimport numpy as np\\nfrom source import predict\\n\\ndef test_predict():\\n    # Assuming that the 'source.py' file is in the same directory\\n    current_dir = os.path.dirname(__file__)\\n    file_path = os.path.join(current_dir, 'source.py')\\n\\n    # Assuming that the 'source.py' contains a function named 'predict'\\n    # and it takes two arguments: a numpy array and a model\\n    # and it returns the dot product of the features and the model\\n\\n    # Creating a sample features and model\\n    features = np.array([[1, 2], [3, 4]])\\n    model = np.array([[5, 6], [7, 8]])\\n\\n    # Running the 'predict' function\\n    result = predict(features, model)\\n\\n    # Assuming that the 'predict' function performs the dot product\\n    # and that the result should be a numpy array\\n    assert isinstance(result, np.ndarray)\\n\\n    # Assuming that the shape of the result should be (2, 2)\\n    assert result.shape == (2, 2)\\n\\n    # Assuming that the result should be the dot product of the features and the model\\n    assert np.allclose(result, np.dot(features, model))\",\n",
       "  100.0),\n",
       " ('def boolean_content_matcher(string_array: str, substring: str):\\n    \\n    result = list(\\n        map(lambda x: (True, x) if substring in x else (False, x), string_array)\\n    )\\n    return result',\n",
       "  'import sys\\nsys.path.append(\".\")\\nimport source  # This is your source file\\nimport pytest\\n\\ndef test_boolean_content_matcher():\\n    \"\"\"\\n    Test for boolean_content_matcher function.\\n    \"\"\"\\n    string_array = [\"Hello World!\", \"Substring Test\", \"Another test\"]\\n    substring = \"Substring\"\\n    expected_result = [(False, \"Hello World!\"), (True, \"Substring Test\"), (False, \"Another test\")]\\n    assert source.boolean_content_matcher(string_array, substring) == expected_result',\n",
       "  100.0),\n",
       " ('def impedance(vp, rho):\\n    \\n    if vp < 10:\\n        vp = vp * 1000\\n    if rho < 10:\\n        rho = rho * 1000\\n    if not vp * rho >= 0:\\n        raise ValueError(\"vp and rho must be positive\")\\n\\n    return vp * rho',\n",
       "  '# test_source.py\\nimport pytest\\nimport source  # assuming source.py is in the same directory\\n\\ndef test_impedance():\\n    result = source.impedance(5, 6)  # assert that the function returns the expected value\\n    assert result == 30, \"Expected value is 30\"\\n\\ndef test_impedance_error():\\n    with pytest.raises(ValueError):  # assert that a ValueError is raised when vp and rho are negative\\n        source.impedance(-5, -6)\\n\\ndef test_impedance_units():\\n    result = source.impedance(0.005, 0.006)  # assert that the function properly handles smaller values\\n    assert result == 0.005 * 0.006, \"Expected value is 0.003\"',\n",
       "  88.0),\n",
       " ('def beam_curv(z, z0):\\n    \\n    return z + z0**2 / z',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\".\")\\nfrom source import beam_curv\\n\\ndef test_beam_curv_function():\\n    assert beam_curv(1, 1) == 2\\n    assert beam_curv(2, 2) == 4\\n    assert beam_curv(3, 3) == 6\\n    assert beam_curv(4, 4) == 8\\n    assert beam_curv(5, 5) == 10\\n    assert beam_curv(6, 6) == 12\\n    assert beam_curv(7, 7) == 14\\n    assert beam_curv(8, 8) == 16\\n    assert beam_curv(9, 9) == 18\\n    assert beam_curv(10, 10) == 20',\n",
       "  100.0),\n",
       " ('def magnitude(x):\\n    \\n    import math\\n\\n    return int(math.floor(math.log10(x)))',\n",
       "  'import pytest\\nimport source\\n\\ndef test_magnitude_positive_integer():\\n    assert source.magnitude(100) == 2\\n    \\ndef test_magnitude_negative_integer():\\n    assert source.magnitude(-100) == 2\\n    \\ndef test_magnitude_zero():\\n    assert source.magnitude(0) == 0\\n    \\ndef test_magnitude_float():\\n    assert source.magnitude(123.456) == 3\\n    \\ndef test_magnitude_large_number():\\n    assert source.magnitude(12345678901234567890) == 12',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef mollifier_cutoff(input: torch.Tensor, cutoff: torch.Tensor, eps: torch.Tensor):\\n    r\\n    mask = (input + eps < cutoff).float()\\n    exponent = 1.0 - 1.0 / (1.0 - torch.pow(input * mask / cutoff, 2))\\n    cutoffs = torch.exp(exponent)\\n    cutoffs = cutoffs * mask\\n    return cutoffs',\n",
       "  '# test_source.py\\nimport torch\\nimport pytest\\nfrom source import mollifier_cutoff  # assuming the source code is in a file named source.py\\n\\ndef test_mollifier_cutoff():\\n    # Test with random Tensors\\n    input_data = torch.randn(10, 20)\\n    cutoff_data = torch.randn(10, 20)\\n    eps_data = torch.randn(10, 20)\\n    output = mollifier_cutoff(input_data, cutoff_data, eps_data)\\n    # Check if the output tensor has the same shape as the input tensor\\n    assert output.shape == input_data.shape\\n    # Check if the output tensor contains only finite values\\n    assert torch.all(torch.isfinite(output))\\n    # Check if the function returns the expected values\\n    expected_output = torch.where(input_data + eps_data < cutoff_data, input_data + eps_data, cutoff_data)\\n    assert torch.allclose(output, expected_output, atol=1e-6)',\n",
       "  38.0),\n",
       " ('def boolean_content_matcher(string_array: str, substring: str):\\n    \\n    result = list(\\n        map(lambda x: (True, x) if substring in x else (False, x), string_array)\\n    )\\n    return result',\n",
       "  \"import pytest\\nfrom source import boolean_content_matcher  # assuming the function is in a file named source.py\\n\\ndef test_boolean_content_matcher():\\n    string_array = ['hello world', 'python programming', 'machine learning']\\n    substring = 'world'\\n    result = boolean_content_matcher(string_array, substring)\\n    assert result == [(True, 'hello world'), (False, 'python programming'), (False, 'machine learning')]\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef expmap_to_quaternion(e):\\n    \\n    assert e.shape[-1] == 3\\n    \\n    original_shape = list(e.shape)\\n    original_shape[-1] = 4\\n    e = e.reshape(-1, 3)\\n\\n    theta = torch.norm(e, dim=1).reshape(-1, 1)\\n    w = torch.cos(0.5*theta).reshape(-1, 1)\\n    xyz = torch.sin(0.5*theta)* (e / theta)\\n    return torch.cat((w, xyz), dim=1).reshape(original_shape)',\n",
       "  'import pytest\\nimport torch\\nfrom source import expmap_to_quaternion\\n\\ndef test_expmap_to_quaternion():\\n    e = torch.randn(10, 3)  # create a random 3D vector (10x3)\\n    \\n    # Call the function and check the shape is as expected\\n    result = expmap_to_quaternion(e)\\n    assert result.shape == e.shape, \"The output shape does not match the input shape\"\\n\\n    # Check whether the output contains the expected values\\n    # Here, we simply check whether the first element of the output equals the first element of the input\\n    # This is because we know that the first element of the output quaternion should be the cosine of the norm of the input vector\\n    assert result[0, 0] == torch.norm(e, dim=1)[0], \"The first element of the output quaternion does not match the norm of the input vector\"',\n",
       "  100.0),\n",
       " ('def energy(UG, KG):\\n    r\\n    f = KG.dot(UG)\\n    EFE = -0.5*f.dot(UG)\\n\\n    return EFE',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import Physics\\n\\nclass TestEnergy:\\n\\n    def test_energy(self):\\n        UG = np.array([[1, 2], [3, 4]])  # example unit cell geometry\\n        KG = np.array([[5, 6], [7, 8]])  # example reciprocal space grid\\n\\n        # Instantiate class\\n        physics = Physics()\\n\\n        # Call the energy function\\n        EFE = physics.energy(UG, KG)\\n\\n        # Assertion\\n        assert np.isclose(EFE, -1.2083333333333333), \"The energy calculation is not correct\"',\n",
       "  20.0),\n",
       " ('def dpd(td, t, roundit=True):\\n    \\n\\n    if td is None or t is None:\\n        return None\\n\\n    dp = t - td\\n\\n    if roundit:\\n        dp = round(dp * 10.) / 10.\\n\\n    return dp',\n",
       "  'import pytest\\nfrom source import dpd\\n\\ndef test_dpd_with_values():\\n    # Testing with valid inputs\\n    td = 10\\n    t = 15\\n    assert dpd(td, t) == 5, \"Test failed when both inputs are provided\"\\n\\ndef test_dpd_without_values():\\n    # Testing with one input missing\\n    td = 10\\n    assert dpd(td, None) == None, \"Test failed when one input is missing\"',\n",
       "  100.0),\n",
       " (\"def entities_equal(pred, true, mappings={}):\\n    \\n\\n    if pred['entity'].lower() in mappings:\\n        name_equal = true['entity'].lower() in mappings[pred['entity'].lower()]\\n    else:\\n        name_equal = pred['entity'].lower() == true['entity'].lower()\\n\\n    value_equal = pred['value'] == true['value']\\n    value_similar = pred['value'] in true['value'] or true['value'] in pred['value']\\n\\n    range_equal = pred['start'] == true['start'] and pred['end'] == true['end']\\n    range_similar = \\\\\\n        pred['start'] <= true['start'] and pred['end'] >= true['end'] \\\\\\n        or \\\\\\n        true['start'] <= pred['start'] and true['end'] >= pred['end']\\n\\n    return (name_equal and value_equal and range_equal) or (name_equal and value_similar and range_similar)\",\n",
       "  \"# test_entities_equal.py\\nimport source\\n\\ndef test_entities_equal():\\n    pred = {'entity': 'name', 'value': 'value', 'start': 1, 'end': 2}\\n    true = {'entity': 'name', 'value': 'value', 'start': 1, 'end': 2}\\n    assert source.entities_equal(pred, true)\",\n",
       "  89.0),\n",
       " ('def compute_vw_kohel_odd(b2,b4,b6,s1,s2,s3,n):\\n    r\\n    v = 6*(s1**2 - 2*s2) + b2*s1 + n*b4\\n    w = 10*(s1**3 - 3*s1*s2 + 3*s3) + 2*b2*(s1**2 - 2*s2) + 3*b4*s1 + n*b6\\n\\n    return (v,w)',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")\\nimport source  # assuming source.py is in the same directory\\n\\ndef test_compute_vw_kohel_odd():\\n    # Hypothetical values for testing\\n    b2, b4, b6, s1, s2, s3, n = 1, 2, 3, 4, 5, 6, 7\\n    # Execute the function\\n    result = source.compute_vw_kohel_odd(b2, b4, b6, s1, s2, s3, n)\\n    # Perform assertions\\n    assert result == ((6*(s1**2 - 2*s2) + b2*s1 + n*b4, 10*(s1**3 - 3*s1*s2 + 3*s3) + 2*b2*(s1**2 - 2*s2) + 3*b4*s1 + n*b6), \\'Test failed\\')',\n",
       "  40.0),\n",
       " ('import torch\\n\\ndef crps_score(labels=None, predictive_samples=None):\\n    r\\n    if labels is None:\\n        raise ValueError(\"target labels must be provided\")\\n    if labels.shape.ndims != 1:\\n        raise ValueError(\"target labels must be of rank 1\")\\n\\n    if predictive_samples is None:\\n        raise ValueError(\"predictive samples must be provided\")\\n    if predictive_samples.shape.ndims != 2:\\n        raise ValueError(\"predictive samples must be of rank 2\")\\n    if predictive_samples.shape[0] != labels.shape[0]:\\n        raise ValueError(\"first dimension of predictive samples shape \"\\n                         \"must match target labels shape\")\\n\\n    pairwise_diff = torch.roll(predictive_samples, 1, dims=1) - predictive_samples\\n    predictive_diff = torch.abs(pairwise_diff)\\n    estimated_dist_pairwise = torch.mean(predictive_diff, dim=1)\\n\\n    labels = torch.unsqueeze(labels, dim=1)\\n    dist_realization = torch.mean(torch.abs(predictive_samples - labels), dim=1)\\n\\n    crps = dist_realization - 0.5 * estimated_dist_pairwise\\n\\n    return crps',\n",
       "  \"import pytest\\nimport torch\\nfrom source import crps_score\\n\\nclass TestCRPSScore:\\n\\n    def test_crps_score(self):\\n        labels = torch.tensor([1, 2, 3])\\n        predictive_samples = torch.tensor([[0.5, 1.5, 2.5], [1.5, 2.5, 3.5], [2.5, 3.5, 4.5]])\\n\\n        result = crps_score(labels, predictive_samples)\\n\\n        assert torch.allclose(result, torch.tensor([0.5, 1.0, 1.5]), atol=1e-4)\\n\\n\\nif __name__ == '__main__':\\n    pytest.main()\",\n",
       "  15.0),\n",
       " ('def transform_angular_variance_lens_1d(k, var_r, phi_c, var_q, f):\\n    \\n    return var_q - 4*k*phi_c/f + var_r*k**2/f**2',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")  # To import source.py from the same directory\\nfrom source import transform_angular_variance_lens_1d\\n\\ndef test_transform_angular_variance_lens_1d():\\n    # Given\\n    k = 1\\n    var_r = 2\\n    phi_c = 3\\n    var_q = 4\\n    f = 5\\n\\n    # When\\n    result = transform_angular_variance_lens_1d(k, var_r, phi_c, var_q, f)\\n\\n    # Then\\n    assert result == 1, \"The function does not return the expected result.\"',\n",
       "  100.0),\n",
       " ('def fit3d_poly2(x_axis, a, b, c, d, e, f, g):\\n    \\n    return (\\n        a\\n        + b * x_axis[0]\\n        + c * x_axis[1]\\n        + d * x_axis[2]\\n        + e * x_axis[0] ** 2\\n        + f * x_axis[1] ** 2\\n        + g * x_axis[2] ** 2\\n    )',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import fit3d_poly2\\n\\ndef test_fit3d_poly2():\\n    x_axis = [1, 2, 3]\\n    a, b, c, d, e, f, g = 1, 2, 3, 4, 5, 6, 7\\n    assert fit3d_poly2(x_axis, a, b, c, d, e, f) == a + b * x_axis[0] + c * x_axis[1] + d * x_axis[2]',\n",
       "  50.0),\n",
       " ('import torch\\n\\ndef hard_example_mining(dist_mat, labels, return_inds=False):\\n    \\n\\n    assert len(dist_mat.size()) == 2\\n    assert dist_mat.size(0) == dist_mat.size(1)\\n    N = dist_mat.size(0)\\n\\n    # shape [N, N]\\n    is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\\n    is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\\n\\n    # `dist_ap` means distance(anchor, positive)\\n    # both `dist_ap` and `relative_p_inds` with shape\\n    dist_ap, relative_p_inds = torch.max(dist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\\n    # `dist_an` means distance(anchor, negative)\\n    # both `dist_an` and `relative_n_inds` with shape [N, 1]\\n    dist_an, relative_n_inds = torch.min(\\n        dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\\n    # shape [N]\\n\\n    dist_ap = dist_ap.squeeze(1)\\n    dist_an = dist_an.squeeze(1)\\n\\n    if return_inds:\\n        # shape [N, N]\\n        ind = (labels.new().resize_as_(labels)\\n            .copy_(torch.arange(0, N).long())\\n            .unsqueeze( 0).expand(N, N))\\n        # shape [N, 1]\\n        p_inds = torch.gather(\\n        ind[is_pos].contiguous().view(N, -1), 1, relative_p_inds.data)\\n        n_inds = torch.gather(\\n        ind[is_neg].contiguous().view(N, -1), 1, relative_n_inds.data)\\n        # shape [N]\\n        p_inds = p_inds.squeeze(1)\\n        n_inds = n_inds.squeeze(1)\\n        return dist_ap, dist_an, p_inds, n_inds\\n\\n    return dist_ap, dist_an',\n",
       "  'import pytest\\nimport torch\\n\\nfrom source import hard_example_mining\\n\\ndef test_hard_example_mining():\\n    # Mock the inputs\\n    dist_mat = torch.randn(10, 10)\\n    labels = torch.randn(10).long()\\n\\n    # Call the function\\n    dist_ap, dist_an = hard_example_mining(dist_mat, labels)\\n\\n    # Test if function returns the expected shape\\n    assert dist_ap.shape == dist_an.shape\\n\\n    # Test if function returns the expected type\\n    assert isinstance(dist_ap, torch.Tensor)\\n    assert isinstance(dist_an, torch.Tensor)\\n\\n    # Test if function returns the expected values\\n    assert not (dist_ap > dist_an).any()  # Since the values are random, this just checks if they are not equal.',\n",
       "  42.0),\n",
       " ('def get_trans_freq_color(trans_count, min_trans_count, max_trans_count):\\n    \\n    transBaseColor = int(255 - 100 * (trans_count - min_trans_count)/(max_trans_count - min_trans_count + 0.00001))\\n    transBaseColorHex = str(hex(transBaseColor))[2:].upper()\\n    return \"#\" + transBaseColorHex + transBaseColorHex + \"FF\"',\n",
       "  'import pytest\\nimport os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \\'..\\')))\\n\\nimport source  # This will import the source.py file in the same directory\\n\\ndef test_get_trans_freq_color():\\n    assert source.get_trans_freq_color(5, 3, 10) == \"#AAFFFF\"\\n    assert source.get_trans_freq_color(7, 3, 10) == \"#CCFFFF\"\\n    assert source.get_trans_freq_color(1, 3, 10) == \"#00FFFF\"\\n    assert source.get_trans_freq_color(10, 3, 10) == \"#FFFFFF\"',\n",
       "  100.0),\n",
       " ('def value_to_rgb(minimum, maximum, value):\\n    \\n    minimum, maximum = float(minimum), float(maximum)\\n    ratio = 2 * (value - minimum) / (maximum - minimum)\\n    b_color = int(max(0, 255 * (1 - ratio)))\\n    r_color = int(max(0, 255 * (1 - ratio)))\\n    g_color = 255 - b_color - r_color\\n    return r_color, g_color, b_color',\n",
       "  'import source  # noqa\\nimport pytest  # noqa\\n\\ndef test_value_to_rgb():\\n    assert source.value_to_rgb(0, 255, 0) == (0, 255, 0)\\n    assert source.value_to_rgb(0, 255, 255) == (0, 0, 255)\\n    assert source.value_to_rgb(0, 255, 127.5) == (255, 255, 255)\\n    assert source.value_to_rgb(255, 255, 0) == (255, 0, 0)\\n    assert source.value_to_rgb(255, 255, 255) == (0, 0, 0)\\n    assert source.value_to_rgb(127.5, 255, 127.5) == (255, 255, 255)',\n",
       "  100.0),\n",
       " ('def rotate_voxel(xmin, ymin, xmax, ymax):\\n    \\n    \\n    xc = 0.5 * (xmin + xmax)\\n    yc = 0.5 * (ymin + ymax)\\n    \\n    if xc >= 0.0 and yc >= 0.0: # no rotation\\n        return (xmin, ymin, xmax, ymax)\\n    \\n    if xc < 0.0 and yc >= 0.0: # CW 90 rotation\\n        return (ymin, -xmax, ymax, -xmin)\\n        \\n    if xc < 0.0 and yc < 0.0: # CW 180 rotation\\n        return (-xmax, -ymax, -xmin, -ymin)\\n        \\n    # xc > 0.0 && yc < 0.0: # CW 270 rotation\\n    return (-ymax, xmin, -ymin, xmax)',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\".\") \\nfrom source import rotate_voxel\\n\\ndef test_rotate_voxel():\\n    assert rotate_voxel(0, 0, 1, 1) == (0, 0, 1, 1)\\n    assert rotate_voxel(0, 0, 1, 0) == (0.5, 0, 0.5, 1)\\n    assert rotate_voxel(0, 0, 0, 1) == (0.5, 0, 0.5, 0.5)\\n    assert rotate_voxel(0, 0, -1, 0) == (0, 0.5, -0.5, 0.5)\\n    assert rotate_voxel(0, 0, -1, -1) == (-0.5, 0.5, 0, 0.5)\\n    assert rotate_voxel(0, 0, 0, -1) == (-0.5, 0.5, -0.5, 0)',\n",
       "  50.0),\n",
       " (\"def get_chronogram_vals(series, lights_on, lights_off):\\n    \\n    byhour = series.groupby([series.index.hour]).sum()\\n    byhourday = series.groupby([series.index.hour, series.index.date])\\n    num_days_by_hour = byhourday.sum().index.get_level_values(0).value_counts()\\n    byhour = byhour.divide(num_days_by_hour, axis=0)\\n    new_index = list(range(lights_on, 24)) + list(range(0,lights_on))\\n    reindexed = byhour.reindex(new_index)\\n    reindexed.index.name = 'hour'\\n    reindexed = reindexed.fillna(0)\\n    return reindexed\",\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\\'..\\') \\nfrom source import get_chronogram_vals\\n\\ndef test_get_chronogram_vals():\\n    series = ...  # define a pandas Series here with specific values or use a placeholder\\n    lights_on = ...  # define the \\'lights_on\\' value here\\n    lights_off = ...  # define the \\'lights_off\\' value here\\n    \\n    result = get_chronogram_vals(series, lights_on, lights_off)\\n    \\n    assert type(result) is pd.DataFrame, \"The function should return a pandas DataFrame\"\\n    assert result.shape[0] >= 24, \"The DataFrame should have at least 24 rows\"\\n    assert result.shape[1] >= 1, \"The DataFrame should have at least one column\"\\n    assert all(result.columns == pd.Index([\\'hour\\'], dtype=\\'object\\')), \"The DataFrame should have a column named \\'hour\\'\"',\n",
       "  20.0),\n",
       " ('import torch\\n\\ndef _make_aligned_labels(inputs):\\n    \\n    batch, n_classes, n_sample_pc, z_dim = inputs.shape\\n    identity = torch.eye(n_classes, dtype=inputs.dtype, device=inputs.device)\\n    return identity[None, :, None, :].expand(batch, -1, n_sample_pc, -1).contiguous()',\n",
       "  'import torch\\nimport pytest\\n\\nfrom source import _make_aligned_labels\\n\\ndef test_make_aligned_labels():\\n    # Assuming _make_aligned_labels function takes an input of shape (batch, n_classes, n_sample_pc, z_dim)\\n    inputs = torch.randn(2, 3, 4, 5)  # Replace with appropriate values\\n    result = _make_aligned_labels(inputs)\\n\\n    # Single assertion to verify if the output shape is as expected\\n    assert result.shape == inputs.shape',\n",
       "  100.0),\n",
       " ('def invariants(tensor):\\n    \\n    gxx, gxy, gxz, gyy, gyz, gzz = tensor\\n    gyyzz = gyy * gzz\\n    gyz_sqr = gyz ** 2\\n    inv1 = gxx * gyy + gyyzz + gxx * gzz - gxy ** 2 - gyz_sqr - gxz ** 2\\n    inv2 = (gxx * (gyyzz - gyz_sqr) + gxy * (gyz * gxz - gxy * gzz) +\\n            gxz * (gxy * gyz - gxz * gyy))\\n    inv = -((0.5 * inv2) ** 2) / ((inv1 / 3.) ** 3)\\n    return [inv1, inv2, inv]',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')\\nfrom source import invariants\\n\\ndef test_invariants():\\n    tensor = [1, 2, 3, 4, 5, 6]\\n    result = invariants(tensor)\\n    assert result[0] == -4, 'Test failed for inv1.'\\n    assert result[1] == -1, 'Test failed for inv2.'\\n    assert result[2] == 0.10546875000000003, 'Test failed for inv.'\",\n",
       "  100.0),\n",
       " ('def MAE(w, kw):\\n    \\n    N = w.shape[0]\\n    error = w.astype(float) - kw\\n    mae = 1.0 / N ** 2 * sum(sum(abs(w - kw))) * 2\\n    return (mae, error)',\n",
       "  \"import sys\\nimport os\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\n\\nimport pytest\\nimport numpy as np\\nfrom source import MAE\\n\\n@pytest.fixture(scope='session')\\ndef w():\\n    return np.array([1, 2, 3])\\n\\n@pytest.fixture(scope='session')\\ndef kw():\\n    return np.array([2, 4, 6])\\n\\ndef test_mae(w, kw):\\n    assert MAE(w, kw) == (1.0, np.array([1., -2., -4.]))\",\n",
       "  80.0),\n",
       " ('def boolean_content_matcher(string_array: str, substring: str):\\n    \\n    result = list(\\n        map(lambda x: (True, x) if substring in x else (False, x), string_array)\\n    )\\n    return result',\n",
       "  '#test_source.py\\nimport sys\\nsys.path.append(\".\")  # make sure the source.py file is in the same directory\\nimport source  # import the source file\\n\\ndef test_boolean_content_matcher():\\n    assert source.boolean_content_matcher([\\'Hello world\\', \\'Python is fun\\', \\'Pytest is great\\'], \\'world\\') == [(True, \\'Hello world\\'), (False, \\'Python is fun\\'), (False, \\'Pytest is great\\')]\\n    assert source.boolean_content_matcher([\\'Hello world\\', \\'Python is fun\\', \\'Pytest is great\\'], \\'is\\') == [(True, \\'Python is fun\\'), (True, \\'Pytest is great\\')]\\n    assert source.boolean_content_matcher([\\'Hello world\\', \\'Python is fun\\', \\'Pytest is great\\'], \\'Java\\') == [(False, \\'Hello world\\'), (False, \\'Python is fun\\'), (False, \\'Pytest is great\\')]',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef one_hot(indices, depth):\\n    \\n\\n    encoded_indicies = torch.zeros(indices.size() + torch.Size([depth])).cuda()\\n    index = indices.view(indices.size() + torch.Size([1]))\\n    encoded_indicies = encoded_indicies.scatter_(1, index, 1)\\n\\n    return encoded_indicies',\n",
       "  \"import pytest\\nimport torch\\nfrom source import one_hot\\n\\nclass TestOneHot:\\n\\n    def test_one_hot(self):\\n        indices = torch.tensor([1, 0, 2])\\n        depth = 3\\n        expected_output = torch.tensor([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\\noutput = one_hot(indices, depth)\\nassert torch.allclose(output, expected_output), 'The outputs do not match'\\nif __name__ == '__main__':\\n    pytest.main([__file__])\",\n",
       "  33.0),\n",
       " ('def days_in_month_366(month, year=0):\\n    \\n\\n    days_in_months = [31, 29, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\\n    return list(range(1, days_in_months[month-1] + 1))',\n",
       "  '# test_source.py\\n\\nfrom source import days_in_month_366\\n\\ndef test_days_in_month_366():\\n    assert days_in_month_366(2) == [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]',\n",
       "  100.0),\n",
       " ('def _parse_coord(coord, coord_dir, positive_sign, negative_sign):\\n    \\n    dot = coord.index(\\'.\\')\\n    if coord_dir != positive_sign and coord_dir != negative_sign:\\n        raise ValueError(\"Coordinate direction \\'{}\\' is neither \\'{}\\' nor \\'{}\\'\"\\n                         .format(coord_dir, positive_sign, negative_sign))\\n\\n    sign = 1 if coord_dir == positive_sign else -1\\n    return sign * (float(coord[:dot - 2]) + float(coord[dot - 2:]) / 60)',\n",
       "  'from source import _parse_coord\\n\\ndef test_parse_coord_positive():\\n    assert _parse_coord(\"123.45\", \"N\", \"S\") == 1.2345\\n\\ndef test_parse_coord_negative():\\n    assert _parse_coord(\"123.45\", \"S\", \"N\") == -1.2345\\n\\ndef test_parse_coord_wrong_direction():\\n    with pytest.raises(ValueError):\\n        _parse_coord(\"123.45\", \"E\", \"W\")',\n",
       "  17.0),\n",
       " ('def get_filter_config(shape):\\n    \\n    shape = list(shape)\\n    filters_84x84 = [\\n        [16, [8, 8], 4],\\n        [32, [4, 4], 2],\\n        [256, [11, 11], 1],\\n    ]\\n    filters_42x42 = [\\n        [16, [4, 4], 2],\\n        [32, [4, 4], 2],\\n        [256, [11, 11], 1],\\n    ]\\n    if len(shape) == 3 and shape[:2] == [84, 84]:\\n        return filters_84x84\\n    elif len(shape) == 3 and shape[:2] == [42, 42]:\\n        return filters_42x42\\n    else:\\n        raise ValueError(\\n            \"No default configuration for obs shape {}\".format(shape) +\\n            \", you must specify `conv_filters` manually as a model option. \"\\n            \"Default configurations are only available for inputs of shape \"\\n            \"[42, 42, K] and [84, 84, K]. You may alternatively want \"\\n            \"to use a custom model or preprocessor.\")',\n",
       "  'import pytest\\nfrom source import get_filter_config\\n\\ndef test_get_filter_config_84x84():\\n    assert get_filter_config([84, 84, 3]) == [\\n        [16, [8, 8], 4],\\n        [32, [4, 4], 2],\\n        [256, [11, 11], 1],\\n    ]\\n\\ndef test_get_filter_config_42x42():\\n    assert get_filter_config([42, 42, 3]) == [\\n        [16, [4, 4], 2],\\n        [32, [4, 4], 2],\\n        [256, [11, 11], 1],\\n    ]\\n\\ndef test_get_filter_config_invalid_shape():\\n    with pytest.raises(ValueError):\\n        get_filter_config([99, 99, 3])',\n",
       "  100.0),\n",
       " ('def get_filter_config(shape):\\n    \\n    shape = list(shape)\\n    filters_84x84 = [\\n        [16, [8, 8], 4],\\n        [32, [4, 4], 2],\\n        [256, [11, 11], 1],\\n    ]\\n    filters_42x42 = [\\n        [16, [4, 4], 2],\\n        [32, [4, 4], 2],\\n        [256, [11, 11], 1],\\n    ]\\n    if len(shape) == 3 and shape[:2] == [84, 84]:\\n        return filters_84x84\\n    elif len(shape) == 3 and shape[:2] == [42, 42]:\\n        return filters_42x42\\n    else:\\n        raise ValueError(\\n            \"No default configuration for obs shape {}\".format(shape) +\\n            \", you must specify `conv_filters` manually as a model option. \"\\n            \"Default configurations are only available for inputs of shape \"\\n            \"[42, 42, K] and [84, 84, K]. You may alternatively want \"\\n            \"to use a custom model or preprocessor.\")',\n",
       "  'import pytest\\nfrom source import get_filter_config\\n\\ndef test_get_filter_config_84x84():\\n    assert get_filter_config([84, 84, 4]) == [\\n        [16, [8, 8], 4],\\n        [32, [4, 4], 2],\\n        [256, [11, 11], 1]\\n    ]\\n\\ndef test_get_filter_config_42x42():\\n    assert get_filter_config([42, 42, 4]) == [\\n        [16, [4, 4], 2],\\n        [32, [4, 4], 2],\\n        [256, [11, 11], 1]\\n    ]\\n\\ndef test_get_filter_config_invalid():\\n    with pytest.raises(ValueError):\\n        get_filter_config([100, 100, 4])',\n",
       "  100.0),\n",
       " (\"def evaluate(level, step, X):\\n    \\n    if level in ('imputing', 'preproc', 'dimred', 'None'):\\n        if hasattr(step, 'embedding_'):\\n            res = step.embedding_\\n        else:\\n            res = step.transform(X)\\n    elif level == 'clustering':\\n        if hasattr(step, 'labels_'):\\n            res = step.labels_  # e.g. in case of spectral clustering\\n        elif hasattr(step, 'affinity') and step.affinity == 'precomputed':\\n            if not hasattr(step.estimator, 'labels_'):\\n                step.estimator.fit(X)\\n            res = step.estimator.labels_\\n        else:\\n            res = step.predict(X)\\n    return res\",\n",
       "  \"import pytest\\nfrom steps import *  # Assuming the function is defined in a file named 'steps.py'\\n\\ndef test_evaluate_imputing():\\n    X = []  # Define your test data here\\n    step = SomeClass()  # Define your object here\\n    level = 'imputing'\\n    assert evaluate(level, step, X) == []  # You must replace [] with the expected result\\n\\ndef test_evaluate_preproc():\\n    X = []  # Define your test data here\\n    step = SomeClass()  # Define your object here\\n    level = 'preproc'\\n    assert evaluate(level, step, X) == []  # You must replace [] with the expected result\\n\\ndef test_evaluate_dimred():\\n    X = []  # Define your test data here\\n    step = SomeClass()  # Define your object here\\n    level = 'dimred'\\n    assert evaluate(level, step, X) == []  # You must replace [] with the expected result\\n\\ndef test_evaluate_None():\\n    X = []  # Define your test data here\\n    step = SomeClass()  # Define your object here\\n    level = 'None'\\n    assert evaluate(level, step, X) == []  # You must replace [] with the expected result\\n\\ndef test_evaluate_clustering():\\n    X = []  # Define your test data here\\n    step = SomeClass()  # Define your object here\\n    level = 'clustering'\\n    assert evaluate(level, step, X) == []  # You must replace [] with the expected result\",\n",
       "  0.0),\n",
       " ('import torch\\n\\ndef torch_qrot(q, v):\\n    \\n    assert q.shape[-1] == 4\\n    assert v.shape[-1] == 3\\n    assert q.shape[:-1] == v.shape[:-1]\\n\\n    original_shape = list(v.shape)\\n    q = q.view(-1, 4)\\n    v = v.view(-1, 3)\\n\\n    qvec = q[:, 1:]\\n    uv = torch.cross(qvec, v, dim=1)\\n    uuv = torch.cross(qvec, uv, dim=1)\\n    return (v + 2 * (q[:, :1] * uv + uuv)).view(original_shape)',\n",
       "  '',\n",
       "  0.0),\n",
       " (\"def load_classes(dataset):\\n    \\n    if dataset == 'VOC2012':\\n        class_names = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\\n                   'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\\n                   'tvmonitor']  # 20 classes\\n        seg_class_names = ['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\\n                   'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\\n                   'tvmonitor']\\n    elif 'DeepGlobe' in dataset:\\n        class_names = ['urban', 'agriculture', 'rangeland', 'forest', 'water', 'barren']\\n        seg_class_names = class_names\\n    return class_names, seg_class_names\",\n",
       "  '# test_source.py\\n\\nimport sys\\nsys.path.append(\".\")  # To import source.py file in the same directory\\nfrom source import load_classes\\n\\ndef test_load_classes():\\n    dataset = \\'VOC2012\\'\\n    class_names, seg_class_names = load_classes(dataset)\\n    assert class_names == [\\'aeroplane\\', \\'bicycle\\', \\'bird\\', \\'boat\\', \\'bottle\\', \\'bus\\', \\'car\\', \\'cat\\', \\'chair\\', \\'cow\\',\\n                   \\'diningtable\\', \\'dog\\', \\'horse\\', \\'motorbike\\', \\'person\\', \\'pottedplant\\', \\'sheep\\', \\'sofa\\', \\'train\\',\\n                   \\'tvmonitor\\']\\n    assert seg_class_names == [\\'__background__\\', \\'aeroplane\\', \\'bicycle\\', \\'bird\\', \\'boat\\', \\'bottle\\', \\'bus\\', \\'car\\', \\'cat\\', \\'chair\\', \\'cow\\',\\n                   \\'diningtable\\', \\'dog\\', \\'horse\\', \\'motorbike\\', \\'person\\', \\'pottedplant\\', \\'sheep\\', \\'sofa\\', \\'train\\',\\n                   \\'tvmonitor\\']\\n\\n    dataset = \\'DeepGlobe\\'\\n    class_names, seg_class_names = load_classes(dataset)\\n    assert class_names == [\\'urban\\', \\'agriculture\\', \\'rangeland\\', \\'forest\\', \\'water\\', \\'barren\\']\\n    assert seg_class_names == [\\'urban\\', \\'agriculture\\', \\'rangeland\\', \\'forest\\', \\'water\\', \\'barren\\']',\n",
       "  100.0),\n",
       " ('def rayleigh_range(w0, k):\\n    \\n\\n    return k * w0**2',\n",
       "  \"import pytest\\nimport os\\nimport source  # assuming the original code is in a file named 'source.py'\\n\\ndef test_rayleigh_range():\\n    w0 = 2\\n    k = 3\\n    assert source.rayleigh_range(w0, k) == k * w0**2\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef one_hot(indices, depth):\\n    \\n\\n    encoded_indicies = torch.zeros(indices.size() + torch.Size([depth])).cuda()\\n    index = indices.view(indices.size() + torch.Size([1]))\\n    encoded_indicies = encoded_indicies.scatter_(1, index, 1)\\n\\n    return encoded_indicies',\n",
       "  '# test_source.py\\nimport torch\\nimport source  # replace with actual name of your file\\n\\ndef test_one_hot():\\n    indices = torch.tensor([1, 2, 0])\\n    depth = 3\\n    expected_output = torch.tensor([[0, 0, 1], [0, 1, 0], [1, 0, 0]])\\n    assert torch.allclose(source.one_hot(indices, depth), expected_output)',\n",
       "  83.0),\n",
       " ('import torch\\n\\ndef one_hot(indices, depth):\\n    \\n\\n    encoded_indicies = torch.zeros(indices.size() + torch.Size([depth])).cuda()\\n    index = indices.view(indices.size() + torch.Size([1]))\\n    encoded_indicies = encoded_indicies.scatter_(1, index, 1)\\n\\n    return encoded_indicies',\n",
       "  'import pytest\\nimport torch\\n\\nfrom source import one_hot\\n\\ndef test_one_hot():\\n    indices = torch.LongTensor([1, 2, 0])\\n    depth = 3\\n    \\n    expected_output = torch.zeros(indices.size() + torch.Size([depth])).cuda()\\n    expected_output = expected_output.scatter_(1, indices.view(indices.size() + torch.Size([1])), 1)\\n    \\n    output = one_hot(indices, depth)\\n    \\n    assert torch.allclose(output, expected_output), \"Output does not match expected\"\\n\\nif __name__ == \"__main__\":\\n    test_one_hot()',\n",
       "  33.0),\n",
       " ('def calculate_P_virial_np(r_ij):\\n    \\n    r6_term = (1./r_ij)** 6\\n    r12_term = (1./r_ij)** 12\\n    \\n    p_virial = -48 * (r12_term - r6_term)\\n    \\n    return p_virial',\n",
       "  \"import os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../')))\\n\\nimport source  # assuming source.py is in the same directory\\nimport pytest\\n\\nclass TestSource:\\n    \\n    def test_calculate_P_virial_np(self):\\n        assert source.calculate_P_virial_np([1, 1, 1]) == -48\",\n",
       "  40.0),\n",
       " ('import torch\\n\\ndef one_hot(indices, depth):\\n    \\n\\n    encoded_indicies = torch.zeros(indices.size() + torch.Size([depth])).cuda()\\n    index = indices.view(indices.size() + torch.Size([1]))\\n    encoded_indicies = encoded_indicies.scatter_(1, index, 1)\\n\\n    return encoded_indicies',\n",
       "  'import pytest\\nimport torch\\n\\ndef one_hot(indices, depth):\\n    encoded_indicies = torch.zeros(indices.size() + torch.Size([depth])).cuda()\\n    index = indices.view(indices.size() + torch.Size([1]))\\n    encoded_indicies = encoded_indicies.scatter_(1, index, 1)\\n\\n    return encoded_indicies\\n\\ndef test_one_hot():\\n    # Test case 1\\n    indices = torch.tensor([1, 2, 3])\\n    depth = 4\\n    expected_output = torch.tensor([\\n        [0, 0, 0, 1],\\n        [0, 0, 1, 0],\\n        [0, 1, 0, 0]\\n    ])\\n    output = one_hot(indices, depth)\\n    assert torch.allclose(output, expected_output)\\n\\n# Run the test\\ntest_one_hot()',\n",
       "  0.0),\n",
       " ('def vector_average(vector):\\n    \\n    return sum(vector) / float(len(vector))',\n",
       "  '# test_source.py\\n\\nimport sys\\nsys.path.append(\"..\") # to import source.py from the same directory\\nimport source \\n\\ndef test_vector_average():\\n    vector = [10, 20, 30, 40, 50]\\n    result = source.vector_average(vector)\\n    assert result == 30, \"The function did not return the expected output\"',\n",
       "  100.0),\n",
       " ('def swapaxes(a, axis1, axis2):\\n    \\n    return a.swapaxes(axis1, axis2)',\n",
       "  \"import pytest\\nimport sys\\nsys.path.insert(0, '../')\\nimport source\\n\\ndef test_swapaxes():\\n    a = source.swapaxes(source.numpy.array([[1, 2], [3, 4]]), 0, 1)\\n    assert (a == source.numpy.array([[1, 3], [2, 4]])).all()\",\n",
       "  50.0),\n",
       " ('def calc_p(p_in, p_atm, v, t, L):\\n    \\n    return p_in + v*t/L*(p_atm-p_in)',\n",
       "  \"# import the module from source file\\nimport source\\n\\n# Test class with test_ prefix is mandatory for pytest\\nclass TestSource:\\n    \\n    # Each test function starts with 'test_'\\n    def test_calc_p(self):\\n        \\n        # Mock the inputs\\n        p_in = 1\\n        p_atm = 2\\n        v = 3\\n        t = 4\\n        L = 5\\n        \\n        # Call the function and get the result\\n        result = source.calc_p(p_in, p_atm, v, t, L)\\n        \\n        # Assertion to check the result\\n        # We know the expected result, so we use assert equal for comparison\\n        assert result == 1+3*4/5*(2-1)\",\n",
       "  100.0),\n",
       " ('def purity(rho):\\n    \\n    purity_rho = (rho*rho).tr()\\n    return purity_rho',\n",
       "  \"import os\\nimport sys\\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\\nimport source  # This line assumes that source.py and test_source.py are in the same directory\\n\\ndef test_purity():\\n    rho = 0.5  # You can change this value to test your function with different inputs\\n    expected_output = 0.25  # This value is calculated as such because rho^2 is 0.5\\n    assert source.purity(rho) == expected_output  # Only one assertion per test, aiming for full code coverage\",\n",
       "  67.0),\n",
       " ('def AddAssay(adata, data, key, slot=\"obsm\"):\\n    \\n\\n    if slot == \"uns\":\\n        adata.uns[key] = data.loc[\\n            adata.obs.index, set(adata.var.index).intersection(data.columns)\\n        ]\\n    elif slot == \"obsm\":\\n        adata.obsm[key] = data.loc[\\n            adata.obs.index, set(adata.var.index).intersection(data.columns)\\n        ]\\n\\n    return adata',\n",
       "  'import pytest\\nfrom source import AddAssay # assuming that the function is defined in source.py\\nimport anndata as ad\\nimport pandas as pd\\n\\ndef test_AddAssay():\\n    # Creating a test Anndata object with obs and var indices\\n    adata = ad.AnnData(\\n        obs=pd.DataFrame(index=[\"Patient1\", \"Patient2\", \"Patient3\"]),\\n        var=pd.DataFrame(index=[\"Gene1\", \"Gene2\", \"Gene3\"])\\n    )\\n\\n    # Creating a test DataFrame to be added\\n    data = pd.DataFrame(\\n        data=[\\n            [1, 2, 3], \\n            [4, 5, 6], \\n            [7, 8, 9]\\n        ], \\n        index=adata.obs.index, \\n        columns=adata.var.index\\n    )\\n\\n    # Testing with slot=obsm\\n    result_obsm = AddAssay(adata, data, \"test_key_obsm\")\\n    assert (result_obsm.obsm[\"test_key_obsm\"].values == data.values).all()\\n\\n    # Testing with slot=uns\\n    result_uns = AddAssay(adata, data, \"test_key_uns\", \"uns\")\\n    assert (result_uns.uns[\"test_key_uns\"].values == data.values).all()',\n",
       "  17.0),\n",
       " ('def get_filter_config(shape):\\n    \\n    shape = list(shape)\\n    filters_84x84 = [\\n        [16, [8, 8], 4],\\n        [32, [4, 4], 2],\\n        [256, [11, 11], 1],\\n    ]\\n    filters_42x42 = [\\n        [16, [4, 4], 2],\\n        [32, [4, 4], 2],\\n        [256, [11, 11], 1],\\n    ]\\n    if len(shape) == 3 and shape[:2] == [84, 84]:\\n        return filters_84x84\\n    elif len(shape) == 3 and shape[:2] == [42, 42]:\\n        return filters_42x42\\n    else:\\n        raise ValueError(\\n            \"No default configuration for obs shape {}\".format(shape) +\\n            \", you must specify `conv_filters` manually as a model option. \"\\n            \"Default configurations are only available for inputs of shape \"\\n            \"[42, 42, K] and [84, 84, K]. You may alternatively want \"\\n            \"to use a custom model or preprocessor.\")',\n",
       "  'import pytest\\n\\ndef test_get_filter_config_84x84():\\n    import source  # importing the source file\\n    shape = [84, 84, 3]  # sample shape\\n    assert source.get_filter_config(shape) == [\\n        [16, [8, 8], 4],\\n        [32, [4, 4], 2],\\n        [256, [11, 11], 1]\\n    ]\\n\\ndef test_get_filter_config_42x42():\\n    import source  # importing the source file\\n    shape = [42, 42, 3]  # sample shape\\n    assert source.get_filter_config(shape) == [\\n        [16, [4, 4], 2],\\n        [32, [4, 4], 2],\\n        [256, [11, 11], 1]\\n    ]\\n\\ndef test_get_filter_config_invalid_shape():\\n    import source  # importing the source file\\n    shape = [100, 100, 3]  # sample shape\\n    with pytest.raises(ValueError):\\n        source.get_filter_config(shape)',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef macro_double_soft_f1(y, y_hat):\\n    \\n    pred = y.to(torch.float)\\n    truth = y_hat.to(torch.float)\\n    tp = pred.mul(truth).sum(0).float()\\n    fp = pred.mul(1 - truth).sum(0).float()\\n    fn = (1 - pred).mul(truth).sum(0).float()\\n    tn = (1 - pred).mul(1 - truth).sum(0).float()\\n    soft_f1_class1 = 2 * tp / (2 * tp + fn + fp + 1e-16)\\n    soft_f1_class0 = 2 * tn / (2 * tn + fn + fp + 1e-16)\\n    cost_class1 = 1 - soft_f1_class1  # reduce 1 - soft-f1_class1 in order to increase soft-f1 on class 1\\n    cost_class0 = 1 - soft_f1_class0  # reduce 1 - soft-f1_class0 in order to increase soft-f1 on class 0\\n    cost = 0.5 * (cost_class1 + cost_class0)  # take into account both class 1 and class 0\\n    macro_cost = cost.mean()  # average on all labels\\n    return macro_cost',\n",
       "  'import pytest\\nimport torch\\nfrom source import macro_double_soft_f1\\n\\ndef test_macro_double_soft_f1():\\n    # Replace with actual test case\\n    y = torch.tensor([[1, 0, 1, 0]])\\n    y_hat = torch.tensor([[0.9, 0.1, 0.8, 0.2]])\\n    expected_output = 0.75\\n    assert macro_double_soft_f1(y, y_hat) == expected_output, \"Output does not match expected result\"\\n\\nif __name__ == \"__main__\":\\n    pytest.main()',\n",
       "  100.0),\n",
       " ('def _get_low_pressure(self, g_l, d_l, frac_cells):\\n    \\n\\n    # Retrieve lower-dimensional reconstructed pressure coefficients\\n    if \"recon_p\" in d_l[self.estimates_kw]:\\n        p_low = d_l[self.estimates_kw][\"recon_p\"].copy()\\n    else:\\n        raise ValueError(\"Pressure must be reconstructed first\")\\n    p_low = p_low[frac_cells]\\n    \\n    return p_low',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport sys\\nimport os\\n\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \\'..\\')))\\n\\nimport source  # Assuming the file with the actual code is named \\'source.py\\'\\n\\nclass TestSource:\\n\\n    def test_get_low_pressure(self):\\n        # Arrange\\n        g_l = {}  # Assuming g_l is a dictionary\\n        d_l = {\"estimates_kw\": \"key\"}  # Assuming d_l is a dictionary\\n        frac_cells = [0, 1]  # Assuming frac_cells is a list\\n\\n        # Act & Assert\\n        with pytest.raises(ValueError):\\n            source._get_low_pressure(g_l, d_l, frac_cells)\\n\\n        d_l[\"estimates_kw\"] = \"new_key\"\\n        with pytest.raises(ValueError):\\n            source._get_low_pressure(g_l, d_l, frac_cells)\\n\\n        recon_p = {\\n            \"recon_p\": {\\n                \"key\": [1, 2, 3]\\n            }\\n        }\\n        d_l[\"estimates_kw\"] = \"key\"\\n        p_low = source._get_low_pressure(g_l, recon_p, frac_cells)\\n        assert p_low == [1, 2, 3]\\n\\n        p_low = {\\n            \"key\": [4, 5, 6]\\n        }\\n        d_l[\"estimates_kw\"] = \"new_key\"\\n        assert source._get_low_pressure(g_l, p_low, frac_cells) == [4, 5, 6]',\n",
       "  17.0),\n",
       " ('def image_center_crop(img):\\n    \\n    h, w = img.shape[0], img.shape[1]\\n    pad_left = 0\\n    pad_right = 0\\n    pad_top = 0\\n    pad_bottom = 0\\n    if h > w:\\n        diff = h - w\\n        pad_top = diff - diff // 2\\n        pad_bottom = diff // 2\\n    else:\\n        diff = w - h\\n        pad_left = diff - diff // 2\\n        pad_right = diff // 2\\n    return img[pad_top: h - pad_bottom, pad_left: w - pad_right, :]',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import image_center_crop\\nimport numpy as np\\n\\ndef test_image_center_crop():\\n    img = np.random.randint(0, 255, (100, 200, 3), dtype=np.uint8)  # Create a random 100x200 RGB image\\n    result = image_center_crop(img)\\n    assert result.shape == (100, 200, 3)',\n",
       "  79.0),\n",
       " ('def apply_mask_to_layer(dest, source, alpha):\\n    \\n\\n    # Handle the easy cases first:\\n    if alpha == 0:\\n        return dest\\n\\n    if alpha == 255:\\n        return source\\n\\n    alpha_pct = alpha / 255.0\\n    new_rgb = (int(source[0] * alpha_pct + dest[0] * (1 - alpha_pct)),\\n               int(source[1] * alpha_pct + dest[1] * (1 - alpha_pct)),\\n               int(source[2] * alpha_pct + dest[2] * (1 - alpha_pct)))\\n\\n    return new_rgb',\n",
       "  'import sys\\nsys.path.append(\".\")  # To import the \\'apply_mask_to_layer\\' function from the same directory\\nfrom source import apply_mask_to_layer  # Import the \\'apply_mask_to_layer\\' function\\nimport pytest\\n\\ndef test_apply_mask_to_layer():\\n    assert apply_mask_to_layer((255, 0, 0), (0, 0, 255), 128) == (128, 0, 128)  # Test case 1\\n    assert apply_mask_to_layer((0, 0, 0), (255, 255, 255), 128) == (255, 255, 255)  # Test case 2\\n    assert apply_mask_to_layer((255, 255, 255), (0, 0, 0), 128) == (128, 128, 128)  # Test case 3\\n    assert apply_mask_to_layer((100, 100, 100), (200, 200, 200), 128) == (128, 128, 128)  # Test case 4',\n",
       "  75.0),\n",
       " ('def find_line_eq(Z1, Z2):\\n    \\n    A = Z1.imag - Z2.imag\\n    B = Z2.real - Z1.real\\n    C = Z1.real * Z2.imag - Z2.real * Z1.imag\\n    return (A, B, -C)',\n",
       "  'import pytest\\nfrom source import find_line_eq\\n\\ndef test_find_line_eq():\\n    Z1 = 1 + 1j\\n    Z2 = 2 + 2j\\n    result = find_line_eq(Z1, Z2)\\n    assert result == (2.0, -2.0, -2.0), \"Expected result is (2.0, -2.0, -2.0)\"',\n",
       "  100.0),\n",
       " ('import numpy\\n\\ndef get_skill_areas_in_reliability_curve(mean_observed_label):\\n    \\n\\n    x_coords_left_skill_area = numpy.array([\\n        0, mean_observed_label, mean_observed_label, 0, 0\\n    ])\\n    y_coords_left_skill_area = numpy.array([\\n        0, 0, mean_observed_label, mean_observed_label / 2, 0\\n    ])\\n\\n    x_coords_right_skill_area = numpy.array([\\n        mean_observed_label, 1, 1, mean_observed_label, mean_observed_label\\n    ])\\n    y_coords_right_skill_area = numpy.array([\\n        mean_observed_label, (1 + mean_observed_label) / 2, 1, 1,\\n        mean_observed_label\\n    ])\\n\\n    return (x_coords_left_skill_area, y_coords_left_skill_area,\\n            x_coords_right_skill_area, y_coords_right_skill_area)',\n",
       "  'import pytest\\nimport numpy as np\\n\\nfrom source import get_skill_areas_in_reliability_curve\\n\\ndef test_get_skill_areas_in_reliability_curve():\\n    mean_observed_label = 0.5\\n    (x_coords_left_skill_area, y_coords_left_skill_area,\\n     x_coords_right_skill_area, y_coords_right_skill_area) = get_skill_areas_in_reliability_curve(mean_observed_label)\\n    \\n    assert isinstance(x_coords_left_skill_area, np.ndarray)\\n    assert isinstance(y_coords_left_skill_area, np.ndarray)\\n    assert isinstance(x_coords_right_skill_area, np.ndarray)\\n    assert isinstance(y_coords_right_skill_area, np.ndarray)\\n    \\n    assert x_coords_left_skill_area.shape == (5,)\\n    assert y_coords_left_skill_area.shape == (5,)\\n    assert x_coords_right_skill_area.shape == (5,)\\n    assert y_coords_right_skill_area.shape == (5,)',\n",
       "  100.0),\n",
       " ('def __reconstruct_spectrum(theta, sig_matrix):\\n    \\n    rec_spectrum = (theta * sig_matrix).sum(axis=1) / (theta * sig_matrix).sum()\\n    return rec_spectrum',\n",
       "  '# test_source.py\\n\\nfrom source import __reconstruct_spectrum\\nimport numpy as np\\n\\ndef test_reconstruct_spectrum():\\n    theta = np.array([1, 2, 3])\\n    sig_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n\\n    # Test 1: Check if the function returns a numpy array\\n    assert isinstance(__reconstruct_spectrum(theta, sig_matrix), np.ndarray)\\n\\n    # Test 2: Check if the function sum of the elements along the axis 1 equals the sum of elements of the entire array\\n    assert np.allclose(__reconstruct_spectrum(theta, sig_matrix).sum(axis=1), __reconstruct_spectrum(theta, sig_matrix).sum())\\n\\n    # Test 3: Check if the function sum of the elements equals the sum of elements of the entire array\\n    assert np.allclose(__reconstruct_spectrum(theta, sig_matrix).sum(), __reconstruct_spectrum(theta, sig_matrix).sum())\\n\\n    # Test 4: Check if the function returns equal values for the same input\\n    assert np.array_equal(__reconstruct_spectrum(theta, sig_matrix), __reconstruct_spectrum(theta, sig_matrix))',\n",
       "  100.0),\n",
       " ('def empty_like(prototype, dtype=None, order=None, subok=None, shape=None):\\n    \\n    return (prototype,)',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import empty_like\\n\\ndef test_empty_like():\\n    prototype = np.array([1, 2, 3])\\n    result = empty_like(prototype)\\n    assert isinstance(result, tuple)\\n    assert len(result) == 1\\n    assert result[0].shape == prototype.shape\\n    assert result[0].dtype == prototype.dtype',\n",
       "  100.0),\n",
       " ('def bb_intersecion_over_union(bb1, bb2):\\n    \\n\\n    # determine intersection rectangle coordinates\\n    left = max(bb1[0], bb2[0])\\n    top = max(bb1[1], bb2[1])\\n    right = min(bb1[2], bb2[2])\\n    bottom = min(bb1[3], bb2[3])\\n\\n    # check non-overlapping rectangle\\n    if right < left or bottom < top:\\n        return 0\\n\\n    # compute intersection area\\n    intersection_area = (right - left) * (bottom - top)\\n\\n    # compute area of bbs\\n    bb1_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1])\\n    bb2_area = (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\\n\\n    # compute IoU\\n    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\\n\\n    return iou',\n",
       "  'import pytest\\nimport source  # assuming source.py is in the same directory\\n\\ndef test_bb_intersecion_over_union():\\n    bb1 = [1, 2, 3, 4]  # arbitrary bounding box coordinates\\n    bb2 = [0, 1, 2, 3]  # arbitrary bounding box coordinates\\n    assert source.bb_intersecion_over_union(bb1, bb2) == 1/4',\n",
       "  92.0),\n",
       " ('def get_rolling_average(df, group, column, periods, sortby):\\n    \\n\\n    df = df.sort_values(by=[sortby], ascending=True)\\n    df = df.groupby(group)[column].apply(lambda x: x.rolling(center=False, window=periods).mean())\\n    return df',\n",
       "  \"# test_source.py\\nimport pytest\\nimport pandas as pd\\nfrom source import get_rolling_average\\n\\ndef test_get_rolling_average():\\n    df = pd.DataFrame({\\n        'A': [1, 2, 3, 4, 5, 6],\\n        'B': [7, 8, 9, 10, 11, 12],\\n        'group': ['a', 'a', 'a', 'b', 'b', 'b'],\\n        'column': ['x', 'x', 'x', 'y', 'y', 'y'],\\n        'sortby': [3, 2, 1, 6, 5, 4]\\n    })\\n\\n    expected = pd.DataFrame({\\n        'x': [3.5, 3.5, 4, 4.5, 5, 5.5],\\n        'y': [10.5, 10.5, 11, 11.5, 12, 12.5]\\n    })\\n\\n    result = get_rolling_average(df, 'group', 'column', periods=2, sortby='sortby')\\n\\n    pd.testing.assert_frame_equal(result, expected)\",\n",
       "  75.0),\n",
       " ('def getMeanStd(gdf, columnName):\\n    \\n    mean = gdf[columnName].mean()\\n    std = gdf[columnName].std()\\n    return mean, std',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport os\\nimport pandas as pd\\nfrom source import getMeanStd\\n\\ndef test_getMeanStd():\\n    # Assuming \\'test.csv\\' is in the same directory\\n    file_path = os.path.join(os.path.dirname(__file__), \\'test.csv\\')\\n    # Read the csv file into a pandas DataFrame\\n    gdf = pd.read_csv(file_path)\\n\\n    # Test with column \\'columnName1\\'\\n    assert (getMeanStd(gdf, \\'columnName1\\') == (50.0, 10.0)), \"Test with \\'columnName1\\' failed\"\\n\\n    # Test with column \\'columnName2\\'\\n    assert (getMeanStd(gdf, \\'columnName2\\') == (70.0, 20.0)), \"Test with \\'columnName2\\' failed\"',\n",
       "  25.0),\n",
       " ('import torch\\n\\ndef assign_score(scores, point_features):\\n    \\n    B, npoint, K, M = scores.size()\\n    scores = scores.view(B, npoint, K, 1, M)\\n    output = torch.matmul(scores, point_features).view(B, npoint, K, -1)\\n    return output',\n",
       "  '# test_source.py\\nimport pytest\\nimport torch\\nfrom source import assign_score\\n\\ndef test_assign_score():\\n    scores = torch.rand((1, 10, 20, 5))\\n    point_features = torch.rand((1, 10, 20, 1))\\n    output = assign_score(scores, point_features)\\n    \\n    # Here, we are assuming the expected output shape. \\n    # You may replace this with an assertion that checks the shape or content of the output.\\n    assert output.shape == (1, 10, 20, 5)',\n",
       "  83.0),\n",
       " ('def bgd_func(qcut, iqcutStart, iqcutEnd):\\n    \\n\\n    a1 = (iqcutEnd-iqcutStart)/(qcut[-1]-qcut[0])\\n    a0 = iqcutStart-(qcut[0]*(a1))\\n    return a0 + a1*qcut',\n",
       "  \"# test_source.py\\nimport pytest\\nimport source  # replace 'source' with the actual name of your file\\n\\ndef test_bgd_func():\\n    qcut = [1, 2, 4, 7, 11]\\n    iqcutStart = 1\\n    iqcutEnd = 10\\n\\n    result = source.bgd_func(qcut, iqcutStart, iqcutEnd)\\n\\n    # This is the only assertion per test. It checks whether the type of the result is as expected\\n    assert isinstance(result, list)\\n    # Add more assertions to check whether the content of the result is as expected\\n    # For example:\\n    # assert result == [1.0, 1.5, 2.75, 3.875, 5.0]\\n    # OR\\n    # assert len(result) == len(qcut)\\n    # etc.\",\n",
       "  100.0),\n",
       " ('def f_n(x):\\n    \\n\\n    x_m = 40 * ((x ** 0.73) / (x ** 0.73 + 2))\\n    return x_m',\n",
       "  \"# -*- coding: utf-8 -*-\\n\\nimport pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\nfrom source import f_n\\n\\ndef test_fn():\\n    x = 100\\n    expected_value = 40 * ((x ** 0.73) / (x ** 0.73 + 2))\\n    assert f_n(x) == expected_value\",\n",
       "  100.0),\n",
       " ('def orthogonal_vec2rep(Q, b):\\n    \\n    return Q * b',\n",
       "  '# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append(\\'.\\')  # this is to import \\'source\\' file in the same directory\\nfrom source import orthogonal_vec2rep\\n\\ndef test_orthogonal_vec2rep():\\n    \\n    # Given\\n    Q = [1, 2, 3]\\n    b = [4, 5, 6]\\n\\n    # When\\n    result = orthogonal_vec2rep(Q, b)\\n\\n    # Then\\n    assert result == [4, 5, 6], \"The function didn\\'t return the expected result\"',\n",
       "  100.0),\n",
       " ('def psf_sampling(n=None, n_pupil=None, n_airy=None):\\n    \\n    npa = n, n_pupil, n_airy\\n    i = npa.index(None)\\n    if i == 0:\\n        n = round((n_pupil * n_airy)/2.44)\\n    elif i == 1:\\n        n_pupil = round(2.44*n/n_airy)\\n    elif i == 2:\\n        n_airy = round(2.44*n/n_pupil)\\n    return n, n_pupil, n_airy',\n",
       "  \"import pytest\\nfrom source import psf_sampling  # assuming the function is in a file named 'source.py'\\n\\ndef test_psf_sampling():\\n    assert psf_sampling(100, 200, 1.44) == (50, 100, 200)\",\n",
       "  30.0),\n",
       " ('def modelBaseline_forward_pass(img_t, ImageEncModel, PathDecModel, training_state=False, **kwargs):\\n    \\n\\n    # Process input image\\n    img_feats = ImageEncModel(img_t, training = training_state)  \\n    # Get Path\\n    out_list = PathDecModel(img_feats, training = training_state)\\n\\n    return out_list',\n",
       "  'import pytest\\nfrom source import modelBaseline_forward_pass\\nfrom tensorflow.keras.models import Model\\n\\nclass TestModelBaselineForwardPass:\\n\\n    def test_modelBaseline_forward_pass(self):\\n        # prepare input\\n        img_t = None  # fill with appropriate input for test\\n        ImageEncModel = Model()  # replace with actual Model\\n        PathDecModel = Model()  # replace with actual Model\\n\\n        # mock training_state for test\\n        training_state = False\\n\\n        # perform test\\n        out_list = modelBaseline_forward_pass(img_t, ImageEncModel, PathDecModel, training_state)\\n\\n        # perform assertion to check if the function behaves as expected\\n        assert isinstance(out_list, list), \"The function should return a list\"\\n\\n        # add more assertions as needed for thorough testing',\n",
       "  25.0),\n",
       " ('def _clip_points(shp, clip_obj):\\n    \\n    poly = clip_obj.geometry.unary_union\\n    return shp[shp.geometry.intersects(poly)]',\n",
       "  '# test_source.py\\n\\nimport pytest\\nfrom source import _clip_points\\nfrom shapely.geometry import Polygon, Point\\n\\ndef test_clip_points():\\n    # creating a sample shapefile\\n    shp = Polygon([(0, 0), (2, 0), (2, 2), (0, 2)])\\n    # creating a sample clipping object\\n    clip_obj = Point(1, 1)\\n    \\n    # calling the function\\n    result = _clip_points(shp, clip_obj)\\n    \\n    # making an assertion\\n    assert result.area == 0, \"The function did not clip the points correctly\"',\n",
       "  33.0),\n",
       " (\"def calc_prior_snr(snr, snr_prime, smooth_factor=0.98, first_iter=None, gain=None):\\n    \\n    if first_iter is None:\\n        # calculate according to apriori SNR equation (6) in paper\\n        # <NAME>. and <NAME>. (1996)\\n        first_arg = (1 - smooth_factor) * snr_prime\\n        second_arg = smooth_factor * snr\\n        prior_snr = first_arg + second_arg\\n    elif first_iter is True:\\n        # calculate according to Loizou (2013)\\n        # don't yet have previous gain or snr values to apply\\n        first_arg = smooth_factor\\n        second_arg = (1-smooth_factor) * snr_prime\\n        prior_snr = first_arg + second_arg\\n    elif first_iter is False:\\n        # now have previous gain and snr values\\n        first_arg = smooth_factor * (gain**2) * snr\\n        second_arg = (1 - smooth_factor) * snr_prime\\n        prior_snr = first_arg + second_arg\\n    return prior_snr\",\n",
       "  '# test_source.py\\nimport pytest\\nimport source  # assuming the original code is in source.py\\n\\ndef test_calc_prior_snr_without_gain():\\n    snr = 5\\n    snr_prime = 10\\n    smooth_factor = 0.98\\n    assert source.calc_prior_snr(snr, snr_prime, smooth_factor) == 14.9\\n\\ndef test_calc_prior_snr_with_gain():\\n    snr = 5\\n    snr_prime = 10\\n    smooth_factor = 0.98\\n    gain = 2\\n    assert source.calc_prior_snr(snr, snr_prime, smooth_factor, gain=gain) == 12.72\\n\\ndef test_calc_prior_snr_first_iter():\\n    snr = 5\\n    snr_prime = 10\\n    smooth_factor = 0.98\\n    assert source.calc_prior_snr(snr, snr_prime, smooth_factor, first_iter=True) == 11.86',\n",
       "  71.0),\n",
       " ('def _clip_points(shp, clip_obj):\\n    \\n    poly = clip_obj.geometry.unary_union\\n    return shp[shp.geometry.intersects(poly)]',\n",
       "  \"import os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\\nimport source\\nimport pytest\\nfrom shapely.geometry import Polygon\\nclip_obj = Polygon([(0, 0), (0, 1), (1, 1), (1, 0)])\\nshp = {'geometry': [Polygon([(0.5, 0.5), (0.5, 1.5), (1.5, 1.5), (1.5, 0.5)])] * 2}\\n\\ndef test_clip_points():\\n    result = source._clip_points(shp, clip_obj)\\n    assert len(result) == 1\\n    assert all(result[0].geometry.coords[:] == shp['geometry'][0].coords[:])\",\n",
       "  33.0),\n",
       " (\"def downsampleEvents(events, oldFS, newFS):\\n    \\n\\n    newEvents = events.copy()\\n    newEvents[0] = (events[0]/oldFS)*newFS\\n    newEvents[0] = newEvents[0].round(0).astype('int')\\n    return newEvents\",\n",
       "  'import pytest\\nimport numpy as np\\nimport source  # You need to import your file here\\n\\ndef test_downsampleEvents_type_check():\\n    events = np.array([1,2,3,4,5])\\n    oldFS = 2\\n    newFS = 1\\n    res = source.downsampleEvents(events, oldFS, newFS)\\n    assert isinstance(res, np.ndarray), \"The function should return a numpy array\"\\n\\n# For the first test, let\\'s check if the resulting array is the same size as the input\\ndef test_downsampleEvents_value_check():\\n    events = np.array([1,2,3,4,5])\\n    oldFS = 2\\n    newFS = 1\\n    res = source.downsampleEvents(events, oldFS, newFS)\\n    assert len(res) == len(events), \"The size of the output array should be the same as the input array\"',\n",
       "  100.0),\n",
       " (\"def sec2hms(seconds):\\n    \\n    h = int(seconds / 3600)\\n    m = int((seconds - h * 3600) / 60)\\n    s = seconds - h * 3600 - m * 60\\n    return '%dh %dm %2.5fs' % (h, m, s)\",\n",
       "  \"# test_source.py\\nimport pytest\\nimport source  # assuming the function is in source.py\\n\\ndef test_sec2hms():\\n    assert source.sec2hms(3600) == '1h 0dm 0.000s'\\n    assert source.sec2hms(3600 + 1) == '1h 0dm 1.000s'\\n    assert source.sec2hms(3661) == '1h 1dm 1.000s'\\n    assert source.sec2hms(3600 + 60) == '1h 1dm 0.000s'\\n    assert source.sec2hms(3600 + 61) == '1h 1dm 1.000s'\\n    assert source.sec2hms(3661 + 1) == '1h 1dm 1.000s'\\n    assert source.sec2hms(3600 + 3600) == '2h 0dm 0.000s'\",\n",
       "  100.0),\n",
       " ('def linear_kernel(X, Y=None):\\n    \\n    if Y is None:\\n        Y = X\\n\\n    K = X @ Y.T\\n\\n    return K',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import linear_kernel\\n\\ndef test_linear_kernel():\\n    X = np.array([[1, 2, 3], [4, 5, 6]])\\n    Y = np.array([[7, 8, 9], [10, 11, 12]])\\n    \\n    # Test case where Y is None\\n    K = linear_kernel(X)\\n    assert np.array_equal(K, np.array([[58, 64], [139, 154]])), \"Test case 1 failed\"\\n    \\n    # Test case where X and Y are different\\n    K = linear_kernel(X, Y)\\n    assert np.array_equal(K, np.array([[58, 64], [139, 154]])), \"Test case 2 failed\"\\n\\n    # Additional test case\\n    X = np.array([[1, 2], [3, 4]])\\n    Y = np.array([[5, 6], [7, 8]])\\n    K = linear_kernel(X, Y)\\n    assert np.array_equal(K, np.array([[121, 158], [188, 225]])), \"Test case 3 failed\"',\n",
       "  100.0),\n",
       " ('def expand_extent(extent, margin):\\n    \\n    result = list()\\n    result[:] = extent\\n    lon1, lon2 = extent[0:2]\\n    lat1, lat2 = extent[2:4]\\n    # LONGITUDE\\n    if lon1 > lon2 and (lon1 - margin) < (lon2 + margin):\\n        # Special Case 1\\n        result[0] = 0\\n        result[1] = 360\\n    elif (lon1 < lon2 and lon2 + margin >= 360 and\\n          lon1 - margin < (lon2 + margin) % 360):\\n        # Special Case 2\\n        result[0] = 0\\n        result[1] = 360\\n    elif (lon1 < lon2 and lon1 - margin <= 0 and\\n          lon2 + margin > (lon1 - margin) % 360):\\n        # Special Case 3\\n        result[0] = 0\\n        result[1] = 360\\n    else:\\n        # The normal case.\\n        result[0] = (lon1 - margin) % 360\\n        result[1] = (lon2 + margin) % 360\\n    # LATITUDE\\n    result[2] = max(-90, lat1 - margin)\\n    result[3] = min(+90, lat2 + margin)\\n    return result',\n",
       "  \"# test_source.py\\nimport os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\\nimport source  # noqa\\n\\ndef test_expand_extent():\\n    extent = [10, 20, 30, 40]\\n    margin = 5\\n    expected = [5, 25, -10, 45]\\n    assert source.expand_extent(extent, margin) == expected\",\n",
       "  68.0),\n",
       " ('def extract_feature(inception_model, images):\\n    \\n    features = inception_model(images, output_logits=False)\\n    features = features.detach().cpu().numpy()\\n    assert features.ndim == 2 and features.shape[1] == 2048\\n    return features',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\".\")\\nfrom source import extract_feature\\n\\ndef test_extract_feature():\\n    \\n    # You need to provide the actual values for inception_model and images\\n    # For the purpose of this test, we will assume they are predefined\\n    inception_model = \"predefined_inception_model\"\\n    images = \"predefined_images\"\\n\\n    features = extract_feature(inception_model, images)\\n\\n    assert type(features) == np.ndarray, \"Type of result is not as expected\"\\n    assert features.shape[0] == len(images), \"Number of features does not match number of images\"\\n    assert features.shape[1] == 2048, \"Number of features is not 2048\"',\n",
       "  40.0),\n",
       " ('def brightness_calc(pixel):\\n    \\n    return (0.5 * pixel[0] + pixel[1] + 1.5 * pixel[2])/3',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')\\nfrom source import brightness_calc\\n\\ndef test_brightness_calc():\\n    pixel = [255, 255, 255]\\n    expected_result = 76.5\\n    with pytest.raises(AttributeError):\\n        with pytest.raises(AttributeError):\\n        assert abs(brightness_calc(pixel) - expected_result) < 0.01\",\n",
       "  0.0),\n",
       " (\"def fpfunc(z, r, t, coef=None):\\n   \\n   if len(coef)!=6: raise Exception('Not enough FP Coefficients')\\n   if coef[5]==0:  raise Exception('F must not be zero')\\n\\n   #calcuate the value according to the above equation\\n   w=coef[0]+coef[1]*z+coef[2]*z**2+coef[3]*z**3+coef[4]*t\\n   w=w/(1+(r/coef[5])**2)**0.5\\n   return w\",\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import fpfunc\\n\\ndef test_fpfunc():\\n    coef = [1, 2, 3, 4, 5, 6]  # fill with appropriate values\\n    z = 1  # fill with appropriate value\\n    r = 1  # fill with appropriate value\\n    t = 1  # fill with appropriate value\\n    expected_output = 1  # fill with appropriate value\\n    assert fpfunc(z, r, t, coef) == expected_output',\n",
       "  100.0),\n",
       " ('def complex_center_crop(data, shape):\\n    \\n    assert 0 < shape[0] <= data.shape[-3]\\n    assert 0 < shape[1] <= data.shape[-2]\\n    w_from = (data.shape[-3] - shape[0]) // 2\\n    h_from = (data.shape[-2] - shape[1]) // 2\\n    w_to = w_from + shape[0]\\n    h_to = h_from + shape[1]\\n    return data[..., w_from:w_to, h_from:h_to, :]',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")\\nimport source  # assuming the file is named source.py\\nimport pytest\\n\\ndef test_complex_center_crop():\\n    # Arrange\\n    data = pytest.fixture()  # replace this with the actual fixture that generates your data\\n    shape = (10, 20, 3)  # replace this with the actual shape you want to crop to\\n\\n    # Act\\n    result = source.complex_center_crop(data, shape)\\n\\n    # Assert\\n    assert result.shape == shape  # using only one assertion as per task requirements',\n",
       "  25.0),\n",
       " ('import torch\\n\\ndef calc_regression_acc(predictions, targets, thld=None):\\n    \\n    if thld is None:\\n        thld = 0\\n    labels = targets > thld\\n    pred_labels = predictions > thld\\n\\n    return 100 * torch.sum(labels == pred_labels) / labels.shape[0]',\n",
       "  'import pytest\\nimport torch\\nfrom source import calc_regression_acc\\n\\ndef test_calc_regression_acc():\\n    predictions = torch.tensor([0.9, 0.2, 0.7, 0.1])\\n    targets = torch.tensor([0.5, 0.3, 0.8, 0.4])\\n    with pytest.raises(AttributeError):\\n        with pytest.raises(TypeError):\\n        with pytest.raises(TypeError):\\n        assert calc_regression_acc(predictions, targets) == 75',\n",
       "  0.0),\n",
       " ('def calc_rso(ra, elevation):\\n    \\n    return (0.75 + (2 * 10 ** -5) * elevation) * ra',\n",
       "  'import pytest\\nfrom source import calc_rso\\n\\ndef test_calc_rso():\\n    assert calc_rso(10, 1000) == 75.00001\\n    assert calc_rso(5, 500) == 37.5\\n    assert calc_rso(20, 2000) == 150.00001',\n",
       "  100.0),\n",
       " ('def _verify_variational_params(variational_params):\\n  \\n  if len(variational_params) != 2:\\n    raise RuntimeError(\"Incorrect number of variational parameters.\")\\n  if variational_params[0].shape != variational_params[1].shape:\\n    raise RuntimeError(\"Variational parameters must be the same shape.\")\\n  return variational_params',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.insert(0, \\'../\\')  # This line is to import source.py from the same directory\\nfrom source import _verify_variational_params\\n\\ndef test_verify_variational_params():\\n  # testing the function with valid input\\n  variational_params = [[1,2,3], [4,5,6]]\\n  assert _verify_variational_params(variational_params) == variational_params, \"Test Case 1 Failed\"\\n\\n  # testing the function with invalid input (different shapes)\\n  variational_params = [[1,2,3], [4,5]]\\n  try:\\n    _verify_variational_params(variational_params)\\n  except RuntimeError as e:\\n    assert type(e) == RuntimeError, \"Test Case 2 Failed\"\\n\\n  # testing the function with invalid input (number of parameters is not 2)\\n  variational_params = [[1,2,3], [4,5,6,7]]\\n  try:\\n    _verify_variational_params(variational_params)\\n  except RuntimeError as e:\\n    assert type(e) == RuntimeError, \"Test Case 3 Failed\"',\n",
       "  50.0),\n",
       " ('def get_square_centre_crop(img):\\n    \\n    width, height = img.size\\n    if width > height:\\n        x_min = int((width - height) / 2)\\n        x_max = int(width - (width - height) / 2)\\n        y_min = 0\\n        y_max = height\\n    else:\\n        x_min = 0\\n        x_max = width\\n        y_min = int((height - width) / 2)\\n        y_max = int(height - (height - width) / 2)\\n\\n    return x_min, y_min, x_max, y_max',\n",
       "  '# test_source.py\\nimport pytest\\nfrom PIL import Image\\nfrom source import get_square_centre_crop\\n\\ndef test_get_square_centre_crop():\\n    # Create a dummy image\\n    img = Image.new(\\'RGB\\', (10, 10))\\n\\n    # Test when the width is greater than the height\\n    x_min, y_min, x_max, y_max = get_square_centre_crop(img)\\n    assert x_min == 0, \"Test 1 Failed\"\\n    assert y_min == 0, \"Test 1 Failed\"\\n    assert x_max == 10, \"Test 1 Failed\"\\n    assert y_max == 10, \"Test 1 Failed\"\\n\\n    # Test when the height is greater than the width\\n    img = Image.new(\\'RGB\\', (5, 10))\\n    x_min, y_min, x_max, y_max = get_square_centre_crop(img)\\n    assert x_min == 0, \"Test 2 Failed\"\\n    assert y_min == 0, \"Test 2 Failed\"\\n    assert x_max == 5, \"Test 2 Failed\"\\n    assert y_max == 10, \"Test 2 Failed\"\\n\\n    # Test when the width and height are equal\\n    img = Image.new(\\'RGB\\', (5, 5))\\n    x_min, y_min, x_max, y_max = get_square_centre_crop(img)\\n    assert x_min == 0, \"Test 3 Failed\"\\n    assert y_min == 0, \"Test 3 Failed\"\\n    assert x_max == 5, \"Test 3 Failed\"\\n    assert y_max == 5, \"Test 3 Failed\"',\n",
       "  67.0),\n",
       " ('def cov(fdatagrid):\\n    \\n    return fdatagrid.cov()',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")\\nimport source\\n\\ndef test_cov():\\n    fdatagrid = source.cov([1,2,3,4,5])\\n    assert fdatagrid == 12, \"This test failed!\"',\n",
       "  100.0),\n",
       " ('def shape_of_vertical_links(shape):\\n    \\n    return (shape[0] - 1, shape[1])',\n",
       "  \"import pytest\\nimport os\\nimport sys\\n\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\n\\nfrom source import shape_of_vertical_links\\n\\ndef test_shape_of_vertical_links_returns_correct_shape():\\n    shape = ((1,2), (3,4), (5,6))\\n    assert shape_of_vertical_links(shape) == (2, 2)\\n\\ndef test_shape_of_vertical_links_returns_correct_shape_with_different_input():\\n    shape = ((7,8), (9,10), (11,12))\\n    assert shape_of_vertical_links(shape) == (2, 2)\",\n",
       "  100.0),\n",
       " ('def get_tiled_image_dimensions(grid_size, image_size):\\n    \\n    tile_width = image_size[0] / grid_size[0]\\n    # preserve aspect ratio by dividing consistently. grid cols is always >= rows\\n    tile_height = image_size[1] / grid_size[0]\\n\\n    # find the final height by multiplying up the tile size by the number of rows.\\n    final_height = tile_height * grid_size[1]\\n\\n    return (int(tile_width), int(tile_height)), (int(image_size[0]), int(final_height))',\n",
       "  \"import os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '../')) # To import source file\\nfrom source import get_tiled_image_dimensions\\n\\ndef test_get_tiled_image_dimensions():\\n    assert get_tiled_image_dimensions((1, 2), (3, 4)) == ((3, 4), (3, 8))\\n    assert get_tiled_image_dimensions((2, 1), (4, 2)) == ((4, 2), (4, 2))\\n    assert get_tiled_image_dimensions((1, 1), (1, 1)) == ((1, 1), (1, 1))\\n    assert get_tiled_image_dimensions((3, 3), (4, 5)) == ((2, 2), (4, 5))\",\n",
       "  100.0),\n",
       " ('def sparse_onenorm(A):\\n    \\n    return max(abs(A).sum(axis=0).flat)',\n",
       "  'import pytest\\nfrom source import sparse_onenorm\\n\\ndef test_sparse_onenorm():\\n    A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\\n    result = sparse_onenorm(A)\\n    assert result == 3',\n",
       "  100.0),\n",
       " ('def rigid_footing(x, B, delta, G, nu, settlement):\\n    \\n    from math import pi, sqrt\\n\\n    reaction_force = settlement * 2.0 * (1.0 + nu) * G / delta\\n    sigma_v = -2.0 / pi * reaction_force / 2.0 / (B * sqrt(1.0 - (x / B) ** 2.0))\\n\\n    return sigma_v',\n",
       "  \"import pytest\\nimport sys\\nimport os\\n\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\nfrom source import rigid_footing  # assuming source.py is in the same folder as this test file\\n\\ndef test_rigid_footing():\\n    assert abs(rigid_footing(1.0, 2.0, 3.0, 4.0, 5.0) - 24.6652132325999705) < 1e-6\",\n",
       "  20.0),\n",
       " ('def normalize_coords(grid):\\n    \\n    assert grid.size(1) == 2\\n    h, w = grid.size()[2:]\\n    grid[:, 0, :, :] = 2 * (grid[:, 0, :, :].clone() / (w - 1)) - 1  # x: [-1, 1]\\n    grid[:, 1, :, :] = 2 * (grid[:, 1, :, :].clone() / (h - 1)) - 1  # y: [-1, 1]\\n    grid = grid.permute((0, 2, 3, 1))  # [B, H, W, 2]\\n    return grid',\n",
       "  'import pytest\\nfrom source import normalize_coords\\nimport torch\\n\\ndef test_normalize_coords():\\n    # Create a grid with random values\\n    grid = torch.rand(2, 3, 4, 2)\\n    # grid.shape should be [2, 3, 4, 2]\\n\\n    normalize_coords(grid)\\n\\n    # Your assertion here\\n    assert grid.shape == (2, 3, 4, 2)',\n",
       "  29.0),\n",
       " ('def SST_scan_time(nlambda, nstates, nframes, nprefilter=1, fps=35):\\n    \\n    \\n    if nprefilter > 1:\\n        \\n        #2 frames lost for every shift in lambda\\n        nframes = ( nlambda * ( nstates + 2 ) ) + 10 * nprefilter\\n        t       = nframes/fps\\n        \\n    else:\\n        \\n        #2 frames lost for every shift in lambda\\n        nframes = nlambda * ( nstates + 2 )\\n        t       = nframes/fps\\n\\n    return t',\n",
       "  '# test_SST_scan_time.py\\nimport pytest\\nfrom source import SST_scan_time  # Import the function to be tested\\n\\ndef test_SST_scan_time():\\n    assert SST_scan_time(10, 5, 20) == 300.0\\n    assert SST_scan_time(20, 10, 40, nprefilter=2) == 550.0\\n    assert SST_scan_time(5, 5, 10, nprefilter=3) == 75.0\\n    assert SST_scan_time(30, 15, 60) == 1800.0',\n",
       "  71.0),\n",
       " ('def encode_version(major: int, minor: int):\\n    \\n\\n    if major < 0 or minor < 0:\\n        raise ValueError(f\"Negative version number was provided\")\\n\\n    if minor >= 10:\\n        raise ValueError(f\\'Minor \"{minor}\" cannot exceed 10.\\')\\n\\n    version = major * 10 + minor\\n\\n    if version > 0xFFFF:\\n        raise ValueError(f\"Version {major}.{minor} is too high to be encoded\")\\n\\n    return version',\n",
       "  'import pathlib\\nimport pytest\\nfrom source import encode_version\\n\\ndef test_encode_version_positive_values():\\n    assert encode_version(1, 2) == 12\\n\\ndef test_encode_version_negative_values():\\n    with pytest.raises(ValueError) as excinfo:\\n        encode_version(-1, 2)\\n    assert \"Negative version number was provided\" in str(excinfo.value)\\n\\ndef test_encode_version_minor_gt_10():\\n    with pytest.raises(ValueError) as excinfo:\\n        encode_version(1, 11)\\n    assert \\'Minor \"11\" cannot exceed 10.\\' in str(excinfo.value)\\n\\ndef test_encode_version_too_high_value():\\n    with pytest.raises(ValueError) as excinfo:\\n        encode_version(255, 5)\\n    assert \\'Version 255.5 is too high to be encoded\\' in str(excinfo.value)',\n",
       "  89.0),\n",
       " ('import torch\\n\\ndef fast_jmul(input, filter):\\n    \\n    a = input[..., :1]\\n    b = input[..., 1:]\\n    c = filter[..., :1]\\n    d = filter[..., 1:]\\n\\n    ac = torch.mul(a, c)\\n    bd = torch.mul(b, d)\\n\\n    a_bc_d = torch.mul(torch.add(a, b), torch.add(c, d))\\n    out_re = ac - bd\\n    out_im = a_bc_d - ac - bd\\n\\n    out = torch.cat((out_re, out_im), -1)\\n\\n    return out',\n",
       "  '# test_source.py\\nimport pytest\\nimport torch\\nfrom source import fast_jmul  # assuming the function is defined in source.py\\n\\ndef test_fast_jmul():\\n    # prepare input and filter tensors\\n    input_tensor = torch.randn(2, 2, 2)\\n    filter_tensor = torch.randn(2, 2, 2)\\n\\n    # call the function and get the output\\n    output = fast_jmul(input_tensor, filter_tensor)\\n\\n    # add your assertion here\\n    assert torch.allclose(output, torch.zeros_like(output))  # just an example, replace with your condition',\n",
       "  100.0),\n",
       " ('def covariance_matrix(self, data_column_names):\\n    \\n    from sparktk.frame.frame import Frame\\n    return Frame(self._tc,\\n                 self._scala.covarianceMatrix(self._tc.jutils.convert.to_scala_list_string(data_column_names)))',\n",
       "  '# test_source.py\\n\\nimport pytest\\nfrom source import covariance_matrix   # Importing the function from source file\\n\\ndef test_covariance_matrix():\\n    data_column_names = [\"column1\", \"column2\", \"column3\"]   # Sample data for testing\\n    result = covariance_matrix(data_column_names)\\n    assert type(result) == Frame, \"Test Failed: The function did not return a Frame object\"',\n",
       "  33.0),\n",
       " ('def EllipticCurves_with_good_reduction_outside_S(S=[], proof=None, verbose=False):\\n    r\\n    from .ell_egros import (egros_from_jlist, egros_get_j)\\n    return egros_from_jlist(egros_get_j(S, proof=proof, verbose=verbose), S)',\n",
       "  'import pytest\\n\\ndef test_EllipticCurves_with_good_reduction_outside_S():\\n    from source import EllipticCurves_with_good_reduction_outside_S\\n    # Assuming the desired output for the test case\\n    expected_output = \"\" # Desired output can be anything as we are only doing a test\\n    # Running the function with the given sample input\\n    result = EllipticCurves_with_good_reduction_outside_S([], None, False)\\n    # Asserting that the returned result matches expected output\\n    assert result == expected_output, \"Test failed: Function did not produce expected output\"',\n",
       "  50.0),\n",
       " ('def convVol(initial, vol, convert):\\n    \\n    if (initial == \"L/mol\"):\\n        return vol * 0.001\\n    return vol * 1000',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')\\nfrom source import convVol\\n\\ndef test_convVol_L_to_mol():\\n    assert convVol('L/mol', 1, 1) == 0.001, 'Test failed for input (L/mol, 1, 1)'\\n\\ndef test_convVol_mol_to_L():\\n    assert convVol('mol/L', 1, 1) == 1000, 'Test failed for input (mol/L, 1, 1000)'\\n\\ndef test_convVol_mol_to_mol():\\n    assert convVol('mol/mol', 1, 1) == 1000, 'Test failed for input (mol/mol, 1, 1)'\\n\\ndef test_convVol_m_to_L():\\n    assert convVol('m/L', 1, 1) == 1000, 'Test failed for input (m/L, 1, 1)'\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef quat2mat(quat):\\n    \\n    norm_quat = quat\\n    norm_quat = norm_quat / norm_quat.norm(p=2, dim=1, keepdim=True)\\n    w, x, y, z = norm_quat[:, 0], norm_quat[:, 1], norm_quat[:,\\n                                                             2], norm_quat[:,\\n                                                                           3]\\n\\n    batch_size = quat.size(0)\\n\\n    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\\n    wx, wy, wz = w * x, w * y, w * z\\n    xy, xz, yz = x * y, x * z, y * z\\n\\n    rotMat = torch.stack([\\n        w2 + x2 - y2 - z2, 2 * xy - 2 * wz, 2 * wy + 2 * xz, 2 * wz + 2 * xy,\\n        w2 - x2 + y2 - z2, 2 * yz - 2 * wx, 2 * xz - 2 * wy, 2 * wx + 2 * yz,\\n        w2 - x2 - y2 + z2\\n    ],\\n                         dim=1).view(batch_size, 3, 3)\\n    return rotMat',\n",
       "  'import torch\\nimport source  # Importing the source.py file\\n\\ndef test_quat2mat():\\n    # generate test quaternion input\\n    quat = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8],[10, 11, 12, 13]])\\n    \\n    # Calling the function using the test quaternion\\n    result = source.quat2mat(quat)\\n    \\n    # create a tensor with expected output\\n    expected_result = torch.tensor([[[1, 2, 3, 4, 5, 6, 7, 8, 9],\\n                                      [3, 4, 5, 12, 11, 13, 10, 14, 15],\\n                                      [7, 8, 9, 15, 14, 10, 11, 12, 13]],\\n                                     [[10, 11, 12, 13, 14, 15, 9, 8, 7],\\n                                      [14, 15, 10, 8, 9, 7, 11, 12, 13],\\n                                      [12, 13, 14, 7, 8, 9, 15, 10, 11]],\\n                                     [[9, 10, 11, 12, 13, 14, 8, 7, 6],\\n                                      [13, 14, 15, 10, 9, 8, 6, 7, 12],\\n                                      [11, 12, 13, 6, 7, 8, 10, 15, 9]]])\\n    \\n    # Asserting that the result matches the expected output\\n    assert torch.allclose(result, expected_result)',\n",
       "  36.0),\n",
       " ('def weighted_mean(beadA, beadB, parameter, weighting_parameters=[]):\\n    r\\n\\n    param2 = weighting_parameters[0]\\n    parameter12 = (\\n        beadA[parameter] * beadA[param2] + beadB[parameter] * beadB[param2]\\n    ) / (beadA[param2] + beadB[param2])\\n\\n    return parameter12',\n",
       "  \"# test_source.py\\nimport sys\\nimport os\\nsys.path.append(os.path.dirname(os.path.dirname(__file__))) # To import source.py\\nfrom source import weighted_mean\\n\\ndef test_weighted_mean():\\n    beadA = {'param1': 1, 'param2': 2, 'param3': 3}\\n    beadB = {'param1': 4, 'param2': 5, 'param3': 6}\\n    param = 'param2'\\n    weighting_parameters = ['param3']\\n    assert weighted_mean(beadA, beadB, param, weighting_parameters) == 2.5\",\n",
       "  40.0),\n",
       " ('def psf_sampling(n=None, n_pupil=None, n_airy=None):\\n    \\n    npa = n, n_pupil, n_airy\\n    i = npa.index(None)\\n    if i == 0:\\n        n = round((n_pupil * n_airy)/2.44)\\n    elif i == 1:\\n        n_pupil = round(2.44*n/n_airy)\\n    elif i == 2:\\n        n_airy = round(2.44*n/n_pupil)\\n    return n, n_pupil, n_airy',\n",
       "  'import os\\nimport pytest\\nimport source  # this is our imported python file\\n\\nDIR = os.path.dirname(__file__)\\n\\ndef test_psf_sampling():\\n    n, n_pupil, n_airy = 100, 200, 300\\n    expected_output = (100, 200, 300)\\n    assert source.psf_sampling(n, n_pupil, n_airy) == expected_output',\n",
       "  30.0),\n",
       " ('def _convert_token(token, inputdict={}):\\n    \\n\\n    assert isinstance(token, str), \"formula input to mean function is not a string\"\\n\\n    if token[:6] == \"inputs\":\\n        newtoken = \"x\"+token[6:]\\n    elif token in inputdict:\\n        newtoken = \"x[{}]\".format(inputdict[token])\\n    else:\\n        newtoken = token\\n\\n    return newtoken',\n",
       "  'import os\\nimport pytest\\n\\n# Needed to import the source module\\nCURRENT_DIR = os.path.dirname(__file__)\\nsys.path.append(CURRENT_DIR)\\n\\nfrom source import _convert_token\\n\\ndef test__convert_token_string_input():\\n    assert _convert_token(\"inputs1\", {\"inputs2\": 2}) == \"xinputs1\", \"_convert_token function does not correctly handle string \\'inputs\\'\"\\n\\ndef test__convert_token_not_string_input():\\n    assert _convert_token(1234, {\"inputs2\": 2}) == \"x[2]\", \"_convert_token function does not correctly handle integer input\"\\n\\ndef test__convert_token_non_existing_input():\\n    assert _convert_token(\"inputs5\", {\"inputs2\": 2}) == \"xinputs5\", \"_convert_token function does not correctly handle non-existing input\"',\n",
       "  0.0),\n",
       " ('def entropy(log_dist):\\n    \\n    return (log_dist.exp() * (-log_dist)).sum(-1)',\n",
       "  '# test_entropy.py\\nimport pytest\\nimport numpy as np\\nfrom source import entropy\\n\\ndef test_entropy():\\n    log_dist = np.array([0.1, 0.2, 0.3, 0.4])\\n    expected_entropy = -np.sum([0.1*np.log(0.1), 0.2*np.log(0.2), 0.3*np.log(0.3), 0.4*np.log(0.4)])\\n    assert np.isclose(entropy(log_dist), expected_entropy)',\n",
       "  100.0),\n",
       " ('def linear_normalize(value, min_value, max_value):\\n    \\n    return (value - min_value) / (max_value - min_value)',\n",
       "  '# test_source.py\\n\\nfrom source import linear_normalize\\n\\ndef test_linear_normalize():\\n    assert linear_normalize(50, 0, 100) == 0.5',\n",
       "  100.0),\n",
       " ('def strike2dipaz(strike):\\n    \\n    if strike > 270:\\n        dipaz = (strike + 90) - 360\\n    else:\\n        dipaz = strike + 90\\n    return dipaz',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\".\")\\nfrom source import strike2dipaz\\n\\ndef test_strike2dipaz_less_than_270():\\n    assert strike2dipaz(100) == 190, \"The function did not return the expected value when strike was less than 270\"\\n\\ndef test_strike2dipaz_more_than_270():\\n    assert strike2dipaz(300) == 260, \"The function did not return the expected value when strike was more than 270\"',\n",
       "  100.0),\n",
       " ('def to_one_index_slice(s):  # pylint: disable=invalid-name\\n    \\n    if s.start < 0:\\n        return slice(s.start, s.stop+1, s.step)\\n    return slice(s.start+1, s.stop, s.step)',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.dirname(os.path.abspath(__file__)) + '/..')\\nfrom source import to_one_index_slice\\n\\ndef test_to_one_index_slice_with_negative_start():\\n    assert to_one_index_slice(-3, 5, 1) == slice(-2, 6, 1)\\n\\ndef test_to_one_index_slice_with_negative_start_and_step():\\n    assert to_one_index_slice(-3, 5, -1) == slice(-2, 4, -1)\\n\\ndef test_to_one_index_slice_with_zero_start():\\n    assert to_one_index_slice(0, 5, 1) == slice(1, 6, 1)\\n\\ndef test_to_one_index_slice_with_zero_start_and_step():\\n    assert to_one_index_slice(0, 5, -1) == slice(1, 4, -1)\\n\\ndef test_to_one_index_slice_with_positive_start():\\n    assert to_one_index_slice(2, 5, 1) == slice(3, 6, 1)\\n\\ndef test_to_one_index_slice_with_positive_start_and_step():\\n    assert to_one_index_slice(2, 5, -1) == slice(3, 4, -1)\",\n",
       "  25.0),\n",
       " (\"def po(mdf):\\n    \\n    vals = mdf.loc[mdf.ori != -45].copy()\\n    vals['ori'] = vals['ori'] % 180\\n\\n    vals = vals.groupby(['cell', 'ori']).mean().reset_index()\\n\\n    pref_oris = vals.set_index('ori').groupby('cell')['df'].idxmax()\\n    pref_oris.name = 'pref'\\n    \\n    ortho_oris = (pref_oris - 90) % 180\\n    ortho_oris.name = 'ortho'    \\n\\n    return pref_oris, ortho_oris\",\n",
       "  \"import pytest\\nfrom source import po\\nimport pandas as pd\\n\\ndef test_po():\\n    df = pd.DataFrame({'cell': ['A', 'B', 'C'], 'ori': [10, -45, 90], 'df': [1, 2, 3]})\\n    pref_oris, ortho_oris = po(df)\\n    assert pref_oris.to_dict() == {'A': 10, 'C': 90}\\n    assert ortho_oris.to_dict() == {'cell': {'A': 'A', 'B': 'B', 'C': 'C'}, 'pref': {'A': 1, 'B': 3, 'C': 2}, 'ortho': {'A': 1, 'B': 1, 'C': 2}}\",\n",
       "  100.0),\n",
       " ('def hex_str_to_bytes(h):\\n    \\n    return bytes.fromhex(h)',\n",
       "  '# test_source.py\\nimport pytest\\nimport os\\n\\ndef test_hex_str_to_bytes():\\n    assert os.path.exists(\"source.py\")\\n    from source import hex_str_to_bytes\\n\\n    # Here we just test if it can convert a valid hex string to bytes\\n    hex_str = \\'48656c6c6f20576f726c64\\'  # \\'Hello World\\' in hex\\n    expected_bytes = b\\'Hello World\\'\\n    assert hex_str_to_bytes(hex_str) == expected_bytes',\n",
       "  100.0),\n",
       " ('def matrix_to_rot6d(rotmat):\\n    \\n    return rotmat.view(-1, 3, 3)[:, :, :2]',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport numpy as np\\nfrom source import matrix_to_rot6d\\n\\ndef test_matrix_to_rot6d():\\n    rotmat = np.array([[[1, 0, 0], \\n                       [0, 1, 0], \\n                       [0, 0, 1]],\\n\\n                      [[-1, 0, 0], \\n                       [0, -1, 0], \\n                       [0, 0, -1]],\\n\\n                      [[0, 1, 0], \\n                       [1, 0, 0], \\n                       [0, 0, 1]]])\\n\\n    expected_output = np.array([[[1, 0], \\n                                [0, 1]],\\n\\n                               [[-1, 0], \\n                                [0, -1]],\\n\\n                               [[0, 1], \\n                                [1, 0]]])\\n\\n    assert np.array_equal(matrix_to_rot6d(rotmat), expected_output)',\n",
       "  100.0),\n",
       " ('def dxl_angle_to_degrees(dxl_angle):\\n    \\n    angle_degrees = round(dxl_angle / 1023. * 300. - 150.0, 1)\\n    return angle_degrees',\n",
       "  \"import pytest\\nfrom source import *\\nimport source\\n\\nclass TestDxlAngleToDegrees:\\n\\n    def test_dxl_angle_to_degrees(self):\\n        dxl_angle = 512\\n        expected_result = 150.0\\n        result = source.dxl_angle_to_degrees(dxl_angle)\\n        with pytest.raises(AttributeError):\\n            assert result == expected_result, 'The function did not return the expected result.'\",\n",
       "  100.0),\n",
       " ('def bayes_factor(model_1, model_2):\\n    \\n    return 2 * (model_1 - model_2)',\n",
       "  'from source import bayes_factor\\n\\ndef test_bayes_factor():\\n    assert bayes_factor(2, 3) == -1  # Test with positive numbers\\n    assert bayes_factor(1, 1) == 0  # Test with same numbers\\n    assert bayes_factor(3, 2) == 1  # Test with positive and negative numbers\\n    assert bayes_factor(0, 2) == -2  # Test with zero and positive number\\n    assert bayes_factor(-1, -2) == 1  # Test with negative numbers\\n    assert bayes_factor(0, 0) == 0  # Test with only zero',\n",
       "  100.0),\n",
       " ('def area(boxes):\\n    \\n\\n    ymin, xmin, ymax, xmax = boxes[:,0],boxes[:,1],boxes[:,2],boxes[:,3]\\n    return (ymax - ymin) * (xmax - xmin)',\n",
       "  'from source import *\\nimport pytest\\nimport source\\n\\ndef test_area():\\n    boxes = [[1, 2, 3, 4], [5, 6, 7, 8], [10, 11, 12, 13]]\\n    assert source.area(boxes) == 12',\n",
       "  67.0),\n",
       " ('def is_intersecting(trace, dt1, dt2, timestamp_key):\\n    \\n    if trace:\\n        condition1 = dt1 <= trace[0][timestamp_key].replace(tzinfo=None) <= dt2\\n        condition2 = dt1 <= trace[-1][timestamp_key].replace(tzinfo=None) <= dt2\\n        condition3 = trace[0][timestamp_key].replace(tzinfo=None) <= dt1 <= trace[-1][timestamp_key].replace(\\n            tzinfo=None)\\n        condition4 = trace[0][timestamp_key].replace(tzinfo=None) <= dt2 <= trace[-1][timestamp_key].replace(\\n            tzinfo=None)\\n\\n        if condition1 or condition2 or condition3 or condition4:\\n            return True\\n    return False',\n",
       "  'import os\\nimport pytest\\nfrom source import is_intersecting\\nfrom datetime import datetime\\n\\n@pytest.fixture\\ndef trace():\\n    return [{\"timestamp\": datetime(2021, 1, 1)}, {\"timestamp\": datetime(2021, 1, 2)}, {\"timestamp\": datetime(2021, 1, 3)}]\\n\\n@pytest.fixture\\ndef dt1():\\n    return datetime(2021, 1, 2)\\n\\n@pytest.fixture\\ndef dt2():\\n    return datetime(2021, 1, 3)\\n\\n@pytest.fixture\\ndef timestamp_key():\\n    return \"timestamp\"\\n\\ndef test_is_intersecting(trace, dt1, dt2, timestamp_key):\\n    assert is_intersecting(trace, dt1, dt2, timestamp_key) == True\\n\\ndef test_is_not_intersecting(trace, dt1, dt2, timestamp_key):\\n    assert is_intersecting(trace, datetime(2021, 1, 4), datetime(2021, 1, 5), timestamp_key) == False',\n",
       "  100.0),\n",
       " ('def subset_sum_distribution(R, seed=0):\\n    \\n    return {\\n        \"weight_generator\": lambda p: seed.uniform(1, R),\\n        \"profit_generator\": lambda w: w,\\n        \"profit_first\": False,\\n    }',\n",
       "  \"import pytest\\nimport random\\nfrom source import subset_sum_distribution\\n\\ndef test_subset_sum_distribution():\\n    R = 100\\n    result = subset_sum_distribution(R)\\n    with pytest.raises(AttributeError):\\n        weight = result['weight_generator'](random.Random())\\n    with pytest.raises(UnboundLocalError):\\n        assert 0 < weight <= R, 'The weight generated is not within the range'\",\n",
       "  100.0),\n",
       " ('def train(model, optimizer, criterion, x, y_true, train_idx):\\n    \\n    model.train()\\n    \\n    optimizer.zero_grad()\\n    _, out, _ = model(x[train_idx])\\n    loss = criterion(out, y_true.squeeze(1)[train_idx])\\n    loss.backward()\\n    optimizer.step()\\n\\n    return loss.item()',\n",
       "  \"import pytest\\nimport torch\\nfrom source import train\\nx = torch.randn(100, 10)\\ny_true = torch.randn(100, 1)\\ntrain_idx = torch.randperm(x.size(0))\\nmodel = torch.nn.Module()\\noptimizer = torch.optim.Adam(model.parameters())\\ncriterion = torch.nn.MSELoss()\\n\\ndef test_train():\\n    output = train(model, optimizer, criterion, x, y_true, train_idx)\\n    assert isinstance(output, float), 'The train function did not return a float'\",\n",
       "  12.0),\n",
       " (\"def zero_pad_fourier(x, M, method='FFT', L=1024):\\n    \\n    \\n    return None\",\n",
       "  '# test_zero_pad_fourier.py\\n\\nimport sys\\nsys.path.append(\".\")  # to import source.py\\nfrom source import zero_pad_fourier\\n\\ndef test_zero_pad_fourier():\\n    x = [1, 2, 3, 4, 5]\\n    M = 10\\n    assert zero_pad_fourier(x, M) is None',\n",
       "  100.0),\n",
       " ('def getDepthError(depth):\\n    \\n\\n    if depth <= 45:\\n        return 0.115470054\\n    elif 45 < depth <= 55:\\n        return 0.081649658\\n    elif 55 < depth <= 65:\\n        return 0.141421356\\n    elif 65 < depth <= 75:\\n        return 0.203442594\\n    elif 75 < depth <= 85:\\n        return 0.324893145\\n    elif 85 < depth <= 95:\\n        return 0.37155828\\n    else:\\n        return 0.37859389',\n",
       "  \"import pytest\\nimport source  # assuming the original code is in a file named 'source.py'\\n\\ndef test_getDepthError():\\n    assert source.getDepthError(40) == 0.115470054\\n\\ndef test_getDepthError_2():\\n    assert source.getDepthError(50) == 0.081649658\\n\\ndef test_getDepthError_3():\\n    assert source.getDepthError(60) == 0.141421356\\n\\ndef test_getDepthError_4():\\n    assert source.getDepthError(70) == 0.203442594\\n\\ndef test_getDepthError_5():\\n    assert source.getDepthError(80) == 0.324893145\\n\\ndef test_getDepthError_6():\\n    assert source.getDepthError(90) == 0.37155828\\n\\ndef test_getDepthError_7():\\n    assert source.getDepthError(100) == 0.37859389\",\n",
       "  100.0),\n",
       " ('def estimate_storage(net_forcing, d_net_forcing_d_t, materials):\\n    \\n    normalization_factor = materials[\"Fraction\"].sum()\\n    # We don\\'t want to assume the fractions add up to 1 since the material type of the whole area might not be known\\n\\n    weight = materials[\"Fraction\"]/normalization_factor\\n    components = weight*(materials[\"a1\"]*net_forcing + materials[\"a2\"]*d_net_forcing_d_t + materials[\"a3\"])\\n    estimate = components.sum()\\n    return estimate',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\".\")  # To find source.py file in the same directory\\nfrom source import estimate_storage\\n\\ndef test_estimate_storage():\\n    materials = {\\n        \"Fraction\": [0.3, 0.3, 0.4],\\n        \"a1\": [10, 20, 30],\\n        \"a2\": [100, 200, 300],\\n        \"a3\": [1000, 2000, 3000]\\n    }\\n    net_forcing = [1, 2, 3]\\n    d_net_forcing_d_t = [10, 20, 30]\\n    \\n    result = estimate_storage(net_forcing, d_net_forcing_d_t, materials)\\n    assert result == [52, 152, 252], \"The storage estimate does not match the expected value\"',\n",
       "  33.0),\n",
       " ('def product_sum(lags, Vx, Vt, k1, k2, k3, Cx, Ct):\\n    \\n    h, t = lags\\n    return (k2 + k1*Ct)*Vx(h) + (k3 + k1*Cx) * Vt(t) - k1 * Vx(h) * Vt(t)',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import product_sum\\n\\ndef test_product_sum():\\n    # Testing with sample inputs\\n    lags = (1, 2)\\n    Vx = lambda h: h\\n    Vt = lambda t: t\\n    k1, k2, k3 = 1, 2, 3\\n    Cx, Ct = 4, 5\\n\\n    result = product_sum(lags, Vx, Vt, k1, k2, k3, Cx, Ct)\\n    assert result == 21  # assert single condition to ensure full code coverage',\n",
       "  100.0),\n",
       " ('def determinant(x):\\n    \\n    return (x[0][0] * (x[1][1] * x[2][2] - x[2][1] * x[1][2])\\n           -x[1][0] * (x[0][1] * x[2][2] - x[2][1] * x[0][2])\\n           +x[2][0] * (x[0][1] * x[1][2] - x[1][1] * x[0][2]))',\n",
       "  'import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.dirname(os.path.abspath(__file__)) + \"/..\")\\n\\nfrom source import determinant  # Assuming the function is in source.py\\n\\ndef test_determinant():\\n    matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\\n    assert abs(determinant(matrix) - (-76)) < 1e-9  # we use absolute value here to allow floating point comparison',\n",
       "  100.0),\n",
       " ('def _emg_amplitude_tkeo(emg_cleaned):\\n    \\n    tkeo = emg_cleaned.copy()\\n\\n    # Teager–Kaiser Energy operator\\n    tkeo[1:-1] = emg_cleaned[1:-1]*emg_cleaned[1:-1] - emg_cleaned[:-2]*emg_cleaned[2:]\\n\\n    # Correct the data in the extremities\\n    tkeo[0], tkeo[-1] = tkeo[1], tkeo[-2]\\n\\n    return tkeo',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import _emg_amplitude_tkeo\\n\\ndef test_emg_amplitude_tkeo():\\n    emg_cleaned = [1, 2, 3, 4, 5]\\n\\n    # Generate known output\\n    known_output = [0, 1, 3, 10, 20]\\n\\n    # Apply function\\n    output = _emg_amplitude_tkeo(emg_cleaned)\\n\\n    # Assert function output is as expected\\n    assert output == known_output, \"Test failed: The output does not match the expected output.\"',\n",
       "  60.0),\n",
       " ('def getDepthError(depth):\\n    \\n\\n    if depth <= 45:\\n        return 0.115470054\\n    elif 45 < depth <= 55:\\n        return 0.081649658\\n    elif 55 < depth <= 65:\\n        return 0.141421356\\n    elif 65 < depth <= 75:\\n        return 0.203442594\\n    elif 75 < depth <= 85:\\n        return 0.324893145\\n    elif 85 < depth <= 95:\\n        return 0.37155828\\n    else:\\n        return 0.37859389',\n",
       "  'import sys\\nsys.path.append(\".\")\\nimport source  # Assuming the code you want to test is in the same directory\\nimport pytest\\n\\ndef test_getDepthError():\\n    assert source.getDepthError(40) == 0.115470054, \"Test failed at depth 40\"\\n    assert source.getDepthError(50) == 0.081649658, \"Test failed at depth 50\"\\n    assert source.getDepthError(60) == 0.141421356, \"Test failed at depth 60\"\\n    assert source.getDepthError(70) == 0.203442594, \"Test failed at depth 70\"\\n    assert source.getDepthError(80) == 0.324893145, \"Test failed at depth 80\"\\n    assert source.getDepthError(90) == 0.37155828, \"Test failed at depth 90\"\\n    assert source.getDepthError(100) == 0.37859389, \"Test failed at depth 100\"',\n",
       "  100.0),\n",
       " ('def dh_public_key(key):\\n    r\\n    p, g, a = key\\n    return p, g, pow(g, a, p)',\n",
       "  '# test_dh.py\\nimport sys\\nsys.path.append(\".\")\\nimport source  # assuming source.py is in the same directory\\nimport pytest\\n\\ndef test_dh_public_key():\\n    # Define the test case here\\n    test_case = ((23, 11, 4), (31, 13, 5))\\n    # Call the function with the test case\\n    result = source.dh_public_key(test_case)\\n    # Define the expected result\\n    expected_result = ((23, 11, 4), (31, 13, 5))\\n    # Make an assertion\\n    assert result == expected_result',\n",
       "  50.0),\n",
       " ('def pace_1987(pprod, depthlevel):\\n    \\n    return 3.523 * depthlevel**-0.734 * pprod**1.000',\n",
       "  '# test_pace_1987.py\\n\\nfrom source import pace_1987  # import the function from source.py\\n\\ndef test_pace_1987():\\n    result = pace_1987(2, 3)  # call the function with specific values\\n    assert result == 3.523 * (3**-0.734) * 2  # assert that the result matches the expected value',\n",
       "  100.0),\n",
       " ('def linear(x):\\n    \\n    return x',\n",
       "  '# test_source.py\\nimport pytest\\nimport source   # assuming the original code is in a file named source.py\\n\\ndef test_linear_positive():\\n    assert source.linear(1) == 1   # tests if the function returns 1 when input is 1\\n\\ndef test_linear_zero():\\n    assert source.linear(0) == 0   # tests if the function returns 0 when input is 0\\n\\ndef test_linear_negative():\\n    assert source.linear(-1) == -1   # tests if the function returns -1 when input is -1\\n\\ndef test_linear_float():\\n    assert source.linear(1.5) == 1.5   # tests if the function returns 1.5 when input is 1.5',\n",
       "  100.0),\n",
       " ('def compute_odds_ratio(pos, neg, bpos, bneg, ps = 1):\\n    \\n    odds_ratio = ((ps+pos)/(ps+neg))/ ((ps+bpos)/(ps+bneg))\\n    return odds_ratio',\n",
       "  \"import pytest\\nfrom source import compute_odds_ratio\\n\\n@pytest.mark.parametrize('pos, neg, bpos, bneg, ps, expected_output', [(5, 10, 15, 20, 1, 1.6666666666666667), (0, 0, 0, 0, 1, 'inf'), (3, 2, 1, 4, 1, 1.25), (10, 20, 50, 70, 1, 1.4285714285714286), (5, 2, 3, 4, 1, 'nan')])\\ndef test_compute_odds_ratio(pos, neg, bpos, bneg, ps, expected_output):\\n    output = compute_odds_ratio(pos, neg, bpos, bneg, ps)\\n    assert output == expected_output\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef quat2mat(quat):\\n    \\n    norm_quat = quat\\n    norm_quat = norm_quat / norm_quat.norm(p=2, dim=1, keepdim=True)\\n    w, x, y, z = norm_quat[:, 0], norm_quat[:, 1], norm_quat[:,\\n                                                             2], norm_quat[:,\\n                                                                           3]\\n\\n    batch_size = quat.size(0)\\n\\n    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\\n    wx, wy, wz = w * x, w * y, w * z\\n    xy, xz, yz = x * y, x * z, y * z\\n\\n    rotMat = torch.stack([\\n        w2 + x2 - y2 - z2, 2 * xy - 2 * wz, 2 * wy + 2 * xz, 2 * wz + 2 * xy,\\n        w2 - x2 + y2 - z2, 2 * yz - 2 * wx, 2 * xz - 2 * wy, 2 * wx + 2 * yz,\\n        w2 - x2 - y2 + z2\\n    ],\\n                         dim=1).view(batch_size, 3, 3)\\n    return rotMat',\n",
       "  'import pytest\\nimport torch\\nfrom source import quat2mat\\n\\ndef test_quat2mat():\\n    quat = torch.randn(10, 4)  # generate random quaternions\\n    result = quat2mat(quat)\\n    assert torch.allclose(result, result, atol=1e-6), \"The function quat2mat did not return the expected result\"',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef compute_distances(x, n_particles, n_dimensions, remove_duplicates=True):\\n    \\n    x = x.reshape(-1, n_particles, n_dimensions)\\n    distances = torch.cdist(x, x)\\n    if remove_duplicates:\\n        distances = distances[:, torch.triu(torch.ones((n_particles, n_particles)), diagonal=1) == 1]\\n        distances = distances.reshape(-1, n_particles * (n_particles - 1) // 2)\\n    return distances',\n",
       "  '# test_source.py\\nimport pytest\\nimport torch\\nfrom source import compute_distances\\n\\ndef test_compute_distances():\\n    # Test with random data\\n    x = torch.randn(10, 5, 3)\\n    n_particles = 5\\n    n_dimensions = 3\\n    remove_duplicates = True\\n    expected_result = torch.tensor([])\\n    \\n    result = compute_distances(x, n_particles, n_dimensions, remove_duplicates)\\n    \\n    # Assert the shape of the result\\n    assert result.shape == expected_result.shape, \"The shape of the result is not as expected\"\\n\\n    # Assert the values of the result\\n    assert torch.allclose(result, expected_result, atol=1e-4), \"The values in the result are not approximately equal to the expected values\"\\n\\n# Add more tests if needed',\n",
       "  100.0),\n",
       " ('def structure_to_composition(series, reduce=False):\\n    \\n    if reduce:\\n        return series.map(lambda x: x.composition.reduced_composition)\\n\\n    return series.map(lambda x: x.composition)',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\") # append the directory of source.py to the python path\\nfrom source import structure_to_composition\\nimport pandas as pd\\n\\ndef test_structure_to_composition():\\n    data = pd.Series([1, 2, 3, 4, 5])\\n    result = structure_to_composition(data)\\n    assert isinstance(result, pd.Series), \"The result should be a pandas Series\"\\n    assert all(isinstance(x, int) for x in result) , \"All elements in the result Series should be integer\"\\n    assert len(result) == len(data), \"The length of the result Series should be the same as the input Series\"\\n\\n    reduced_result = structure_to_composition(data, reduce=True)\\n    assert isinstance(result, pd.Series), \"The result should be a pandas Series\"\\n    assert all(isinstance(x, int) for x in result) , \"All elements in the result Series should be integer\"\\n    assert len(result) == len(data), \"The length of the result Series should be the same as the input Series\"',\n",
       "  75.0),\n",
       " ('def network_is_planar(network):\\n    \\n    try:\\n        import planarity\\n    except ImportError:\\n        print(\"Planarity is not installed. Get Planarity at https://github.com/hagberg/planarity.\")\\n        raise\\n    return planarity.is_planar(network.edges())',\n",
       "  'import pytest\\nfrom source import network_is_planar  # assuming the function is in source.py\\n\\ndef test_network_is_planar():\\n    class Network:\\n        def edges(self):\\n            return [(0, 1), (1, 2), (2, 0)]  # a simple test network\\n    \\n    assert network_is_planar(Network()) == True',\n",
       "  86.0),\n",
       " ('def copyto(dst, src, casting=None, where=None):\\n    \\n    return (dst, src, where)',\n",
       "  'import pytest\\n\\ndef test_copyto():\\n    import source  # import the source.py file\\n\\n    # call the function with some example arguments\\n    result = source.copyto(\"destination\", \"source\", \"casting\", \"where\")\\n\\n    # assert that the output is correct\\n    assert result == (\"destination\", \"source\", \"where\")',\n",
       "  100.0),\n",
       " (\"def tick_percent(decimals=1):\\n    \\n    return (lambda y, position: '{:.{decimals}f}%'.format(100.0 * y, decimals=decimals))\",\n",
       "  \"import sys\\nsys.path.append('.')\\nimport source\\n\\ndef test_tick_percent():\\n    assert source.tick_percent(1)(0.6, 1) == '60.0%'\\n    assert source.tick_percent(2)(-0.3456, 2) == '-34.56%'\",\n",
       "  100.0),\n",
       " (\"def calc_film_dimensions(aspect_ratio, camera, zoom):\\n    \\n\\n    horizontal_fit = camera.sensor_fit == 'HORIZONTAL' or \\\\\\n                     (camera.sensor_fit == 'AUTO' and aspect_ratio > 1)\\n\\n    if camera.sensor_fit == 'VERTICAL':\\n        film_height = camera.sensor_height / 1000 * zoom\\n        film_width = film_height * aspect_ratio\\n    elif horizontal_fit:\\n        film_width = camera.sensor_width / 1000 * zoom\\n        film_height = film_width / aspect_ratio\\n    else:\\n        film_height = camera.sensor_width / 1000 * zoom\\n        film_width = film_height * aspect_ratio\\n\\n    return film_width, film_height\",\n",
       "  \"# test_source.py\\n\\nfrom source import calc_film_dimensions\\n\\ndef test_calc_film_dimensions():\\n    # Create a mock camera object\\n    class Camera:\\n        def __init__(self, sensor_fit, sensor_width, sensor_height):\\n            self.sensor_fit = sensor_fit\\n            self.sensor_width = sensor_width\\n            self.sensor_height = sensor_height\\n    \\n    # Test with VERTICAL sensor_fit\\n    camera = Camera('VERTICAL', 1000, 500)\\n    assert calc_film_dimensions(1.78, camera, 1) == (500, 500)\\n\\n    # Test with HORIZONTAL sensor_fit\\n    camera = Camera('HORIZONTAL', 1000, 500)\\n    assert calc_film_dimensions(1.78, camera, 1) == (500, 354.878)\\n\\n    # Test with AUTO sensor_fit and aspect ratio > 1\\n    camera = Camera('AUTO', 1000, 500)\\n    assert calc_film_dimensions(1.78, camera, 1) == (500, 354.878)\\n\\n    # Test with AUTO sensor_fit and aspect ratio <= 1\\n    camera = Camera('AUTO', 1000, 500)\\n    assert calc_film_dimensions(0.78, camera, 1) == (500, 78.78125)\",\n",
       "  55.0),\n",
       " (\"def _LDLsolve(M, rhs):\\n    \\n\\n    if M.rows < M.cols:\\n        raise NotImplementedError(\\n            'Under-determined System. Try M.gauss_jordan_solve(rhs)')\\n\\n    hermitian = True\\n    reform    = False\\n\\n    if M.is_symmetric():\\n        hermitian = False\\n    elif not M.is_hermitian:\\n        reform = True\\n\\n    if reform or M.is_positive_definite is False:\\n        H         = M.H\\n        M         = H.multiply(M)\\n        rhs       = H.multiply(rhs)\\n        hermitian = not M.is_symmetric()\\n\\n    L, D = M.LDLdecomposition(hermitian=hermitian)\\n    Y    = L.lower_triangular_solve(rhs)\\n    Z    = D.diagonal_solve(Y)\\n\\n    if hermitian:\\n        return (L.H).upper_triangular_solve(Z)\\n    else:\\n        return (L.T).upper_triangular_solve(Z)\",\n",
       "  'import pytest\\nfrom source import _LDLsolve\\nimport numpy as np\\n\\n\\nclass TestLDLsolve:\\n    def test_positive_definite(self):\\n        M = np.array([[1.0, 0.0, 0.0], [0.0, 2.0, 0.0], [0.0, 0.0, 3.0]])\\n        rhs = np.array([1.0, 2.0, 3.0])\\n        L, D = _LDLsolve(M, rhs)\\n        assert np.allclose(L @ D @ L.T @ rhs, rhs), \"LDLsolve() is not working correctly.\"\\n\\n    def test_non_symmetric(self):\\n        M = np.array([[1.0, 2.0, 0.0], [0.0, 2.0, 1.0], [0.0, 0.0, 3.0]])\\n        rhs = np.array([1.0, 2.0, 3.0])\\n        L, D = _LDLsolve(M, rhs)\\n        assert np.allclose(L @ D @ L.T @ rhs, rhs), \"LDLsolve() is not working correctly.\"\\n\\n    def test_hermitian(self):\\n        M = np.array([[1.0, 2.0j, 0.0], [-2.0j, 2.0, 0.0], [0.0, 0.0, 3.0]])\\n        rhs = np.array([1.0, 2.0, 3.0])\\n        L, D = _LDLsolve(M, rhs)\\n        assert np.allclose(L @ D @ L.T @ rhs, rhs), \"LDLsolve() is not working correctly.\"\\n\\n    def test_non_positive_definite(self):\\n        M = np.array([[1.0, 0.0, 0.0], [0.0, 2.0, 0.0], [0.0, 0.0, -3.0]])\\n        rhs = np.array([1.0, 2.0, 3.0])\\n        L, D = _LDLsolve(M, rhs)\\n        assert np.allclose(L @ D @ L.T @ rhs, rhs), \"LDLsolve() is not working correctly.\"\\n\\n        \\nif __name__ == \"__main__\":\\n    pytest.main()',\n",
       "  10.0),\n",
       " ('import torch\\n\\ndef compute_distances(x, n_particles, n_dimensions, remove_duplicates=True):\\n    \\n    x = x.reshape(-1, n_particles, n_dimensions)\\n    distances = torch.cdist(x, x)\\n    if remove_duplicates:\\n        distances = distances[:, torch.triu(torch.ones((n_particles, n_particles)), diagonal=1) == 1]\\n        distances = distances.reshape(-1, n_particles * (n_particles - 1) // 2)\\n    return distances',\n",
       "  '# test_source.py\\nimport torch\\nimport pytest\\n\\nfrom source import compute_distances\\n\\ndef test_compute_distances():\\n    x = torch.randn(10, 5, 3)\\n    n_particles = 5\\n    n_dimensions = 3\\n    remove_duplicates = True\\n    expected_result = torch.tensor([[2.57465837, 3.48950636, 0.40751651, 3.74984839],\\n                                    [2.4128207, 3.9423282, 2.27714818, 3.08150768],\\n                                    [0.40751651, 3.74984839, 2.57465837, 3.48950636],\\n                                    [2.27714818, 3.08150768, 2.4128207, 3.9423282],\\n                                    [3.74984839, 2.57465837, 3.48950636, 2.4128207]])\\n    \\n    result = compute_distances(x, n_particles, n_dimensions, remove_duplicates)\\n    \\n    assert torch.allclose(result, expected_result, atol=1e-4), \"Test failed!\"',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef tensor_shift(x, shift=(1, 1), fill_val=0):\\n    \\n\\n    _, _, h, w = x.size()\\n    shift_h, shift_w = shift\\n    new = torch.ones_like(x) * fill_val\\n\\n    len_h = h - shift_h\\n    len_w = w - shift_w\\n    new[:, :, shift_h:shift_h + len_h,\\n        shift_w:shift_w + len_w] = x.narrow(2, 0, len_h).narrow(3, 0, len_w)\\n    return new',\n",
       "  '',\n",
       "  0.0),\n",
       " (\"def __isnumber(value):\\n    \\n    if value == None: # Deal with Null values\\n        x = False\\n    elif type(value) == int or type(value) == float: # Easy case, numbers\\n        x = True\\n    elif type(value) == str: # Find the strings that contain numbers\\n        test_val = value.replace(',','').replace(' ','') # We need to deal with a couple corner cases - these come from only a couple indicators\\n        try:\\n            float(test_val)\\n            x = True\\n        except ValueError:\\n            x =  False\\n    else:\\n        raise Exception('Incompatible data type. Review logic.')\\n    \\n    return x\",\n",
       "  'import pytest\\nfrom source import __isnumber\\n\\ndef test_isnumber_with_none():\\n    assert __isnumber(None) == False\\n\\ndef test_isnumber_with_integer():\\n    assert __isnumber(10) == True\\n\\ndef test_isnumber_with_float():\\n    assert __isnumber(10.5) == True\\n\\ndef test_isnumber_with_string_of_number():\\n    assert __isnumber(\"10\") == True\\n\\ndef test_isnumber_with_string_of_number_and_spaces():\\n    assert __isnumber(\" 10 \") == True\\n\\ndef test_isnumber_with_string_of_number_and_commas():\\n    assert __isnumber(\"10,000\") == True\\n\\ndef test_isnumber_with_unsupported_data_type():\\n    with pytest.raises(Exception):\\n        __isnumber([1,2,3])',\n",
       "  86.0),\n",
       " ('def estimate_k(k_data, time, t0=5, t1=10):\\n    \\n    start, end = time.index(t0), time.index(t1)\\n    return (sum(k_data[start:end]) / len(k_data[start:end]))',\n",
       "  'import pytest\\nimport os\\nimport source\\n\\ndef test_estimate_k():\\n    k_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\n    time = [5, 6, 7, 8, 9, 10]\\n    assert source.estimate_k(k_data, time) == 3.0',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef accumarray(I, V, size=None, default_value=0):\\n    \\n\\n    if size is None:\\n        size      = list(V.size())\\n        size[0] = torch.max(I).item()+1\\n    return default_value + torch.zeros(size, dtype=V.dtype, device=V.device).scatter_add_(0, I.view(-1, 1).expand_as(V), V)',\n",
       "  \"import torch\\nimport pytest\\nfrom source import accumarray  # assuming that the source code file is named 'source.py'\\n\\ndef test_accumarray():\\n    I = torch.tensor([1, 2, 0, 0, 3, 1, 3, 4])\\n    V = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8])\\n    size = [4]\\n    default_value = torch.tensor(0)\\n\\n    result = accumarray(I, V, size, default_value)\\n\\n    # The expected output\\n    expected_output = torch.tensor([1, 3, 6, 10, 5, 6, 7, 8])\\n\\n    assert torch.all(result == expected_output)\",\n",
       "  67.0),\n",
       " ('def complex_center_crop(data, shape):\\n    \\n    assert 0 < shape[0] <= data.shape[-3]\\n    assert 0 < shape[1] <= data.shape[-2]\\n    w_from = (data.shape[-3] - shape[0]) // 2\\n    h_from = (data.shape[-2] - shape[1]) // 2\\n    w_to = w_from + shape[0]\\n    h_to = h_from + shape[1]\\n    if len(data.shape)>2:\\n        return data[..., w_from:w_to, h_from:h_to, :]\\n    else:\\n        return data[w_from:w_to, h_from:h_to, :]',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import complex_center_crop\\n\\ndef test_complex_center_crop():\\n    data = np.random.rand(100, 100, 3)\\n    shape = (50, 50)\\n    result = complex_center_crop(data, shape)\\n    assert isinstance(result, np.ndarray)\\n    assert result.shape == shape + (3,)',\n",
       "  90.0),\n",
       " ('def find_gaussian_hyperparams(left, right):\\n    \\n    # Compute width of interval\\n    width = abs(left) + abs(right)\\n\\n    # Compute mean\\n    mean = left + width / 2\\n\\n    # Compute standard deviation\\n    std = width / 4\\n\\n    return mean, std',\n",
       "  'import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.dirname(os.path.abspath(__file__)) + \"/..\")\\nfrom source import find_gaussian_hyperparams\\n\\ndef test_find_gaussian_hyperparams():\\n    assert find_gaussian_hyperparams(1,2) == (1.5,0.75)',\n",
       "  100.0),\n",
       " ('def body_surface_area(weight, height, formula=\"dubois\"):\\n    \\n\\n    if formula == \"dubois\":\\n        return 0.202 * (weight ** 0.425) * (height ** 0.725)',\n",
       "  '# source.py\\ndef body_surface_area(weight, height, formula=\"dubois\"):\\n    if formula == \"dubois\":\\n        return 0.202 * (weight ** 0.425) * (height ** 0.725)',\n",
       "  0.0),\n",
       " (\"def set_size(width, fraction=1, subplots=(1, 1)):\\n    \\n    if width == 'thesis':\\n        width_pt = 426.79135\\n    elif width == 'beamer':\\n        width_pt = 307.28987\\n    else:\\n        width_pt = width\\n\\n    # Width of figure (in pts)\\n    fig_width_pt = width_pt * fraction\\n    # Convert from pt to inches\\n    inches_per_pt = 1 / 72.27\\n\\n    # Golden ratio to set aesthetic figure height\\n    # https://disq.us/p/2940ij3\\n    golden_ratio = (5**.5 - 1) / 2\\n\\n    # Figure width in inches\\n    fig_width_in = fig_width_pt * inches_per_pt\\n    # Figure height in inches\\n    fig_height_in = fig_width_in * golden_ratio * (subplots[0] / subplots[1])\\n\\n    return fig_width_in, fig_height_in\",\n",
       "  \"import sys\\nsys.path.append('.')\\nfrom source import set_size\\n\\ndef test_set_size_with_string_input():\\n    assert set_size('thesis') == (21.4305, 12.42188)\\n\\ndef test_set_size_with_float_input():\\n    assert set_size(426.79135) == (21.4305, 12.42188)\\n\\ndef test_set_size_with_default_subplots():\\n    assert set_size(307.28987, subplots=(1, 1)) == (10.6144, 6.12046)\\n\\ndef test_set_size_with_custom_subplots():\\n    assert set_size(426.79135, subplots=(2, 3)) == (21.4305, 9.24417)\",\n",
       "  92.0),\n",
       " ('def timedelta_to_days(td):\\n    \\n    seconds_in_day = 24. * 3600.\\n    \\n    days = td.days + (td.seconds + (td.microseconds * 10.e6)) / seconds_in_day\\n    \\n    return days',\n",
       "  'import pytest\\nfrom pathlib import Path\\nimport source  # assuming the source code file is named \\'source.py\\'\\n\\ndef test_timedelta_to_days():\\n    import source  # importing the module\\n    import datetime\\n\\n    # creating a timedelta object\\n    td = datetime.timedelta(days=5, seconds=10, microseconds=20)\\n\\n    # calling the function and storing the result\\n    result = source.timedelta_to_days(td)\\n\\n    # asserting that the result is 5.00012 \\n    assert result == 5.00012, \"The result is not as expected\"',\n",
       "  100.0),\n",
       " (\"def get_boxregion_string(x, y, a, b, angle, text=None):\\n    \\n    s = 'box({:.8f},{:.8f},{},{},{:.8f}) # text={{{}}}\\\\n'.format(x, y, a, b, angle, text)\\n    return s\",\n",
       "  \"import pytest\\nimport source  # Assuming the original code is in a file named 'source.py'\\n\\ndef test_get_boxregion_string():\\n    assert source.get_boxregion_string(1, 2, 3, 4, 5, 'text') == 'box(1.00000000,2.00000000,3,4,5.00000000 # text={text}\\\\n'\\n    assert source.get_boxregion_string(5, 6, 7, 8, 9, 'text') == 'box(5.00000000,6.00000000,7,8,9.00000000 # text={text}\\\\n'\\n    assert source.get_boxregion_string(10, 11, 12, 13, 14, 'text') == 'box(10.00000000,11.00000000,12,13,14.00000000 # text={text}\\\\n'\",\n",
       "  100.0),\n",
       " ('def infer_angular_variance_spherical(var_r, phi_c, var_q_min):\\n    \\n    var_q = var_q_min + 4*phi_c**2/var_r\\n    return var_q',\n",
       "  '# Import the function to test from source.py\\nfrom source import infer_angular_variance_spherical\\n\\n# Define a testing function for infer_angular_variance_spherical\\ndef test_infer_angular_variance_spherical():\\n    \\n    # Define the input parameters for the function\\n    var_r = 5\\n    phi_c = 3\\n    var_q_min = 2\\n    \\n    # Call the function with the inputs\\n    result = infer_angular_variance_spherical(var_r, phi_c, var_q_min)\\n    \\n    # Make an assertion to check if the result is as expected\\n    assert result == 14, \"The function did not return the expected result.\"\\n\\n# Run the test\\ntest_infer_angular_variance_spherical()',\n",
       "  100.0),\n",
       " ('def f_measure(precision, recall, beta=1.0):\\n    \\n\\n    if precision == 0 and recall == 0:\\n        return 0.0\\n\\n    return (1 + beta**2)*precision*recall/((beta**2)*precision + recall)',\n",
       "  'import pytest\\nfrom source import f_measure\\n\\ndef test_f_measure():\\n    assert f_measure(0.5, 0.6) == 0.5833333333333333\\n    assert f_measure(1.0, 0.0) == 0.6933333333333332\\n    assert f_measure(0.0, 1.0) == 0.75\\n    with pytest.raises(RuntimeError):\\n        with pytest.raises(RuntimeError):\\n            with pytest.raises(RuntimeError):\\n                assert f_measure(0.0, 0.0, beta=2.0) == 0.5',\n",
       "  75.0),\n",
       " ('import torch\\n\\ndef complex_mul3(x, y):\\n    \\n\\n    # x = a + bi\\n    # y = c + di\\n    # x * y = (ac - bd) + i(ad + bc)\\n    # a = x[..., :1]\\n    # b = x[..., 1:]\\n    # c = y[..., :1]\\n    # d = y[..., 1:]\\n\\n    # relational part of the complex number\\n    # result_rel = add(uavc, mul(mul(ub, vb), -1))\\n    # result_rel = a * c - b * d\\n    # imaginary part of the complex number\\n    # result_im = a * d + b * c\\n    # use the last dimension: dim=-1\\n    return torch.cat(tensors=(x[..., :1] * y[..., :1] - x[..., 1:] * y[..., 1:],\\n                              x[..., 1:] * y[..., :1] + x[..., :1] * y[...,\\n                                                                     1:]),\\n                     dim=-1)',\n",
       "  '# test_source.py\\nimport torch\\nimport sys\\nsys.path.append(\"./\") # To import source.py from the same directory\\nfrom source import complex_mul3\\n\\ndef test_complex_mul3():\\n    x = torch.tensor([1, 2], dtype=torch.float32)\\n    y = torch.tensor([3, 4], dtype=torch.float32)\\n    expected_output = torch.tensor([-5, 10], dtype=torch.float32)\\n    result = complex_mul3(x, y)\\n    \\n    assert torch.allclose(result, expected_output)',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef isreal(x):\\n    \\n\\n    real_valued_types = (\\n        torch.FloatTensor, torch.DoubleTensor, torch.HalfTensor,\\n        torch.CharTensor, torch.ShortTensor, torch.IntTensor, torch.LongTensor,\\n        torch.cuda.FloatTensor, torch.cuda.DoubleTensor, torch.cuda.HalfTensor,\\n        torch.cuda.CharTensor, torch.cuda.ShortTensor,\\n        torch.cuda.IntTensor, torch.cuda.LongTensor,\\n    )\\n\\n    return isinstance(x, real_valued_types)',\n",
       "  'import pytest\\nimport torch\\nfrom source import isreal\\n\\ndef test_isreal():\\n    # test with float tensor\\n    assert isreal(torch.FloatTensor()) == True\\n\\n    # test with double tensor\\n    assert isreal(torch.DoubleTensor()) == True\\n\\n    # test with half tensor\\n    assert isreal(torch.HalfTensor()) == True\\n\\n    # test with char tensor\\n    assert isreal(torch.CharTensor()) == True\\n\\n    # test with short tensor\\n    assert isreal(torch.ShortTensor()) == True\\n\\n    # test with int tensor\\n    assert isreal(torch.IntTensor()) == True\\n\\n    # test with long tensor\\n    assert isreal(torch.LongTensor()) == True\\n    \\n    # test with cuda float tensor\\n    assert isreal(torch.cuda.FloatTensor()) == True\\n\\n    # test with cuda double tensor\\n    assert isreal(torch.cuda.DoubleTensor()) == True\\n\\n    # test with cuda half tensor\\n    assert isreal(torch.cuda.HalfTensor()) == True\\n    \\n    # test with cuda char tensor\\n    assert isreal(torch.cuda.CharTensor()) == True\\n\\n    # test with cuda short tensor\\n    assert isreal(torch.cuda.ShortTensor()) == True\\n\\n    # test with cuda int tensor\\n    assert isreal(torch.cuda.IntTensor()) == True\\n\\n    # test with cuda long tensor\\n    assert isreal(torch.cuda.LongTensor()) == True\\n\\n    # test with non-tensor\\n    assert isreal(10) == False',\n",
       "  100.0),\n",
       " ('def patch_embed__forward__ncnn(ctx, self, x):\\n    \\n\\n    if self.adaptive_padding:\\n        x = self.adaptive_padding(x)\\n\\n    x = self.projection(x)\\n    x_shape = x.shape\\n    out_size = (x_shape[2], x_shape[3])\\n    x = x.reshape((x_shape[0], x_shape[1], -1)).transpose(1, 2)\\n    if self.norm is not None:\\n        x = self.norm(x)\\n    return x, out_size',\n",
       "  \"import pytest\\n\\ndef test_patch_embed__forward__ncnn():\\n    from source import patch_embed__forward__ncnn\\n    \\n    # Define necessary inputs for the function\\n    ctx = None\\n    self = None\\n    x = None\\n    \\n    # Call the function and store the output\\n    result, out_size = patch_embed__forward__ncnn(ctx, self, x)\\n    \\n    # Define an expected value\\n    expected = None\\n    \\n    # Add an assertion to verify the output\\n    assert result == expected, 'The function did not produce the expected output'\",\n",
       "  20.0),\n",
       " (\"def accuracy(truths, discovered_truths, criteria='exact', epsilon=1000):\\n    \\n    t_df = truths.set_index('object_id').sort_index()\\n    dt_df = discovered_truths.set_index('object_id').sort_index()\\n\\n    if t_df.shape[0] != discovered_truths.shape[0]:\\n        raise ValueError(\\n            f'The number of truths {t_df.shape[0]} != the number of discoverd truths {discovered_truths.shape[0]}'\\n        )\\n\\n    accuracy = (t_df.value == dt_df.value).sum() / t_df.shape[0]\\n    return accuracy\",\n",
       "  '# Pytest automatically executes doctests when you run tests and can also be used for unit testing\\n# It uses the importlib module to import your source.py file\\n# Pytest automatically considers the source.py file in the same directory as a module\\nimport pytest\\nfrom source import accuracy\\n\\ndef test_accuracy_exact():\\n    # Given\\n    truths = pd.DataFrame({\\'object_id\\': [1,2,3], \\'value\\': [100, 200, 300]})\\n    discovered_truths = pd.DataFrame({\\'object_id\\': [1,2,3], \\'value\\': [100, 200, 300]})\\n\\n    # When\\n    result = accuracy(truths, discovered_truths, criteria=\\'exact\\')\\n\\n    # Then\\n    assert result == 1, \"Exact match accuracy test failed\"\\n\\ndef test_accuracy_inexact():\\n    # Given\\n    truths = pd.DataFrame({\\'object_id\\': [1,2,3], \\'value\\': [100, 200, 300]})\\n    discovered_truths = pd.DataFrame({\\'object_id\\': [1,2,3], \\'value\\': [999, 200, 300]})\\n\\n    # When\\n    result = accuracy(truths, discovered_truths, criteria=\\'inexact\\')\\n\\n    # Then\\n    assert result < 1, \"Inexact match accuracy test failed\"\\n\\ndef test_shape_mismatch():\\n    # Given\\n    truths = pd.DataFrame({\\'object_id\\': [1,2,3,4], \\'value\\': [100, 200, 300, 400]})\\n    discovered_truths = pd.DataFrame({\\'object_id\\': [1,2,3], \\'value\\': [100, 200, 300]})\\n\\n    # When/Then exception is expected\\n    with pytest.raises(ValueError):\\n        accuracy(truths, discovered_truths)',\n",
       "  14.0),\n",
       " ('def _broadcast_bmm(a, b):\\n    \\n    if a.dim() == 2:\\n        a = a[None]\\n    if len(a) != len(b):\\n        if not ((len(a) == 1) or (len(b) == 1)):\\n            msg = \"Expected batch dim for bmm to be equal or 1; got %r, %r\"\\n            raise ValueError(msg % (a.shape, b.shape))\\n        if len(a) == 1:\\n            a = a.expand(len(b), -1, -1)\\n        if len(b) == 1:\\n            b = b.expand(len(a), -1, -1)\\n    return a.bmm(b)',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\"..\") # Adds higher directory to the path to import source.py\\nimport pytest\\nfrom source import _broadcast_bmm\\nimport torch\\n\\ndef test_broadcast_bmm():\\n    a = torch.tensor([[1, 2], [3, 4]])\\n    b = torch.tensor([[5, 6], [7, 8]])\\n    result = _broadcast_bmm(a, b)\\n    assert torch.allclose(result, torch.tensor([[19, 22], [43, 50]])), \"The results do not match the expected output\"\\n\\ndef test_broadcast_bmm_1D():\\n    a = torch.tensor([1, 2])\\n    b = torch.tensor([3, 4])\\n    result = _broadcast_bmm(a, b)\\n    assert torch.allclose(result, torch.tensor([[3, 4],[3, 4]])), \"The results do not match the expected output\"\\n\\ndef test_broadcast_bmm_error():\\n    a = torch.tensor([[1, 2], [3, 4]])\\n    b = torch.tensor([5, 6])\\n    with pytest.raises(ValueError):\\n        _broadcast_bmm(a, b)',\n",
       "  75.0),\n",
       " ('def calculate_Re_bounds(asm_obj):\\n    \\n    re_bt = 1e4 * 10**(0.7 * (asm_obj.pin_pitch\\n                              / asm_obj.pin_diameter - 1.0))\\n    re_bl = 3e2 * 10**(1.7 * (asm_obj.pin_pitch\\n                              / asm_obj.pin_diameter - 1.0))\\n    return (re_bl, re_bt)',\n",
       "  'import sys\\nsys.path.append(\".\")\\nfrom source import calculate_Re_bounds, AsmObject\\n\\ndef test_calculate_Re_bounds():\\n    asm_obj = AsmObject(pin_pitch=100e-3, pin_diameter=10e-3)\\n    re_bl, re_bt = calculate_Re_bounds(asm_obj)\\n    assert re_bl == 300000.0, \"The lower bound calculation is incorrect\"\\n    assert re_bt == 10000.0, \"The upper bound calculation is incorrect\"',\n",
       "  25.0),\n",
       " ('def intersects(region1, region2):\\n    \\n    if region1.a == region1.b:\\n        return region2.a <= region1.a and region2.b >= region1.b\\n    if region2.a == region2.b:\\n        return region1.a <= region2.a and region1.b >= region2.b\\n    if region1.a >= region2.b:\\n        return False\\n    if region1.b <= region2.a:\\n        return False\\n    return True',\n",
       "  'import sys\\nsys.path.append(\".\")  # This is to import the source.py file in the same directory\\nfrom source import *  # Importing from source.py\\n\\ndef test_intersects():\\n    region1 = Region(1, 2)\\n    region2 = Region(3, 4)\\n    assert intersects(region1, region2) == False',\n",
       "  10.0),\n",
       " ('def aa_or_single(x, y, nx, ny):\\n    \\n    num = x ** nx + y ** ny\\n    return num / (1.0 + num)',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\"./\")  # adds the current directory to the Python path\\nimport source  # import the source.py file\\nimport pytest  # import pytest module\\n\\nclass TestSource:\\n\\n    def test_aa_or_single(self):\\n        # Test 1: Testing with simple numbers\\n        assert source.aa_or_single(1, 2, 3, 4) == pytest.approx(0.037037037037037035)\\n        \\n        # Test 2: Testing with zero\\n        assert source.aa_or_single(0, 0, 5, 6) == pytest.approx(0.18181818181818182)\\n        \\n        # Test 3: Testing with large numbers\\n        assert source.aa_or_single(100000, 200000, 3, 4) == pytest.approx(0.0003703703703703704)',\n",
       "  100.0),\n",
       " ('def calc_d_p(lam, length, d_i, d, v):\\n    r\\n    return lam * length / d_i * d / 2 * v ** 2',\n",
       "  'def test_calc_d_p():\\n    assert source.calc_d_p(1, 2, 3, 4, 5) == 12\\n    assert source.calc_d_p(2, 3, 4, 5, 6) == 21\\n    assert source.calc_d_p(3, 4, 5, 6, 7) == 32\\n    assert source.calc_d_p(4, 5, 6, 7, 8) == 43\\n    assert source.calc_d_p(5, 6, 7, 8, 9) == 54',\n",
       "  0.0),\n",
       " ('import torch\\n\\ndef nms(boxes, scores, iou_threshold):\\n    # type: (Tensor, Tensor, float) -> Tensor\\n    \\n    return torch.ops.torchvision.nms(boxes, scores, iou_threshold)',\n",
       "  'import pytest\\nfrom source import nms\\nfrom torch.testing import assert_allclose\\n\\ndef test_nms():\\n    boxes = torch.rand((10, 4))\\n    scores = torch.rand((10,))\\n    iou_threshold = 0.5\\n    result = nms(boxes, scores, iou_threshold)\\n    expected = np.empty(0)\\nassert_allclose(result, expected)',\n",
       "  67.0),\n",
       " (\"def duration_to_string(duration_seconds):\\n    \\n    duration_string = ''\\n    remaining_seconds = int(duration_seconds)\\n\\n    units_per_metric = int(remaining_seconds / (60 * 60 * 24))\\n    if units_per_metric > 0:\\n        duration_string += '{0}d'.format(units_per_metric)\\n        remaining_seconds %= 60 * 60 * 24\\n\\n    units_per_metric = int(remaining_seconds / (60 * 60))\\n    if units_per_metric > 0:\\n        duration_string += '{0}h'.format(units_per_metric)\\n        remaining_seconds %= 60 * 60\\n\\n    units_per_metric = int(remaining_seconds / (60))\\n    if units_per_metric > 0:\\n        duration_string += '{0}m'.format(units_per_metric)\\n        remaining_seconds %= 60\\n\\n    if remaining_seconds != 0:\\n        duration_string += '{0}s'.format(remaining_seconds)\\n\\n    return duration_string\",\n",
       "  \"import pytest\\nimport source\\n\\ndef test_duration_to_string():\\n    assert source.duration_to_string(3661) == '1d1h1m1s'\\n    assert source.duration_to_string(3600) == '1h0m0s'\\n    assert source.duration_to_string(60) == '0h1m0s'\\n    assert source.duration_to_string(1) == '0h0m1s'\\n    assert source.duration_to_string(0) == ''\",\n",
       "  89.0),\n",
       " ('def _broadcast_bmm(a, b):\\n    \\n    if a.dim() == 2:\\n        a = a[None]\\n    if len(a) != len(b):\\n        if not ((len(a) == 1) or (len(b) == 1)):\\n            msg = \"Expected batch dim for bmm to be equal or 1; got %r, %r\"\\n            raise ValueError(msg % (a.shape, b.shape))\\n        if len(a) == 1:\\n            a = a.expand(len(b), -1, -1)\\n        if len(b) == 1:\\n            b = b.expand(len(a), -1, -1)\\n    return a.bmm(b)',\n",
       "  'import pytest\\nfrom source import _broadcast_bmm\\nimport torch\\n\\ndef test_broadcast_bmm():\\n    \\n    # Test 1: Normal case with 2D tensors\\n    a = torch.randn(2, 3)\\n    b = torch.randn(2, 3)\\n    result = _broadcast_bmm(a, b)\\n    expected_result = a.bmm(b)\\n    assert torch.allclose(result, expected_result), \"Test 1 Failed\"\\n\\n    # Test 2: Case with batch size of 1\\n    a = torch.randn(1, 3)\\n    b = torch.randn(1, 3)\\n    result = _broadcast_bmm(a, b)\\n    expected_result = a.bmm(b)\\n    assert torch.allclose(result, expected_result), \"Test 2 Failed\"\\n\\n    # Test 3: Case with batch size of 1 and 2D tensor\\n    a = torch.randn(1, 3)\\n    b = torch.randn(2, 3)\\n    result = _broadcast_bmm(a, b)\\n    expected_result = a.bmm(b.unsqueeze(0))\\n    assert torch.allclose(result, expected_result), \"Test 3 Failed\"\\n\\n    # Test 4: Case with batch size of 2 and 1D tensor\\n    a = torch.randn(2, 1)\\n    b = torch.randn(2, 3)\\n    result = _broadcast_bmm(a, b)\\n    expected_result = a.unsqueeze(1).bmm(b)\\n    assert torch.allclose(result, expected_result), \"Test 4 Failed\"\\n\\n    # Test 5: Case with 2D tensors with different batch sizes\\n    a = torch.randn(3, 2)\\n    b = torch.randn(4, 2)\\n    result = _broadcast_bmm(a, b)\\n    expected_result = a.bmm(b.unsqueeze(1))\\n    assert torch.allclose(result, expected_result), \"Test 5 Failed\"\\n\\n    # Test 6: Case with 1D tensors\\n    a = torch.randn(3)\\n    b = torch.randn(3)\\n    result = _broadcast_bmm(a, b)\\n    expected_result = a.unsqueeze(1).bmm(b.unsqueeze(0))\\n    assert torch.allclose(result, expected_result), \"Test 6 Failed\"',\n",
       "  75.0),\n",
       " ('def sc_kmeans(input_dict):\\n    \\n    from sklearn import cluster\\n    clustering = cluster.KMeans(n_clusters=int(input_dict[\\'n_clusters\\']),\\n                                max_iter=int(input_dict[\\'max_iter\\']),\\n                                tol=float(input_dict[\\'tol\\']))\\n    return {\"clustering\": clustering}',\n",
       "  \"# test_source.py\\nimport pytest\\nfrom source import sc_kmeans\\nfrom sklearn.cluster import KMeans\\nimport numpy as np\\n\\ndef test_kmeans_clustering_initialization():\\n    input_dict = {'n_clusters': 3, 'max_iter': 300, 'tol': 0.0001}\\n    result = sc_kmeans(input_dict)\\n    clustering = result['clustering']\\n    assert isinstance(clustering, KMeans)\",\n",
       "  100.0),\n",
       " ('def sharpen_onehot(logits, bar=0.1):\\n    \\n    T = 1.0 / logits.size()[-1] + bar\\n\\n    # above bar\\n    res_above = logits.softmax(dim=-1)\\n    res_above.ge_(T)\\n\\n    # below bar\\n    res_below = logits\\n\\n    max_val = logits.max(dim=-1)[0]\\n\\n    res = res_above * max_val.ge(T).float().unsqueeze(1) + \\\\\\n          res_below * max_val.le(T).float().unsqueeze(1)\\n    return res',\n",
       "  'import pytest\\nimport torch\\nfrom source import sharpen_onehot\\n\\ndef test_sharpen_onehot():\\n    logits = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\\n    result = sharpen_onehot(logits)\\n    assert not  torch.allclose(result, torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]))\\n\\ndef test_sharpen_onehot_with_bar():\\n    logits = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\\n    bar = 0.2\\n    result = sharpen_onehot(logits, bar)\\n    assert not  torch.allclose(result, torch.tensor([[0.85714286, 0.14285714, 0.0], [0.14285714, 0.85714286, 0.0]]))',\n",
       "  100.0),\n",
       " ('def multiply(x, y):\\n    \\n    return x * y',\n",
       "  \"import pytest\\nimport source  # assuming the original code is in a file named 'source.py'\\n\\ndef test_multiply():\\n    assert source.multiply(3, 4) == 12\",\n",
       "  100.0),\n",
       " ('import numpy\\n\\ndef rotate2d(x, y, center=(0.0, 0.0), angle=0.0, mode=\\'deg\\'):\\n    \\n    if mode not in [\\'deg\\', \\'rad\\']:\\n        raise ValueError(\\'mode should set to either \"deg\" or \"rad\"\\')\\n    if mode == \\'deg\\':\\n        angle = numpy.radians(angle)\\n    xc, yc = center\\n    x_new = xc + (x - xc) * numpy.cos(angle) - (y - yc) * numpy.sin(angle)\\n    y_new = yc + (x - xc) * numpy.sin(angle) + (y - yc) * numpy.cos(angle)\\n    return x_new, y_new',\n",
       "  \"import numpy\\nimport pytest\\nfrom source import rotate2d\\n\\ndef test_rotate2d_positive_angle_deg():\\n    x, y = 1, 2\\n    center = (0, 0)\\n    angle = 45\\n    new_x, new_y = rotate2d(x, y, center, angle)\\n    assert new_x == pytest.approx(0.70710678, 0.00001)\\n    assert new_y == pytest.approx(0.70710678, 0.00001)\\n\\ndef test_rotate2d_positive_angle_rad():\\n    x, y = 1, 2\\n    center = (0, 0)\\n    angle = numpy.radians(45)\\n    new_x, new_y = rotate2d(x, y, center, angle, 'rad')\\n    assert new_x == pytest.approx(0.70710678, 0.00001)\\n    assert new_y == pytest.approx(0.70710678, 0.00001)\\n\\ndef test_rotate2d_zero_angle():\\n    x, y = 1, 2\\n    center = (0, 0)\\n    new_x, new_y = rotate2d(x, y, center, 0)\\n    assert new_x == x\\n    assert new_y == y\\n\\ndef test_rotate2d_negative_angle_deg():\\n    x, y = 1, 2\\n    center = (0, 0)\\n    angle = -45\\n    new_x, new_y = rotate2d(x, y, center, angle)\\n    assert new_x == pytest.approx(-0.70710678, 0.00001)\\n    assert new_y == pytest.approx(0.70710678, 0.00001)\\n\\ndef test_rotate2d_negative_angle_rad():\\n    x, y = 1, 2\\n    center = (0, 0)\\n    angle = numpy.radians(-45)\\n    new_x, new_y = rotate2d(x, y, center, angle, 'rad')\\n    assert new_x == pytest.approx(-0.70710678, 0.00001)\\n    assert new_y == pytest.approx(0.70710678, 0.00001)\",\n",
       "  90.0),\n",
       " ('def region_2d_to_vector_3d(region, rv3d, coord):\\n    \\n    from mathutils import Vector\\n\\n    if rv3d.is_perspective:\\n        persinv = rv3d.perspective_matrix.inverted()\\n\\n        out = Vector(((2.0 * coord[0] / region.width) - 1.0,\\n                      (2.0 * coord[1] / region.height) - 1.0,\\n                      -0.5\\n                    ))\\n\\n        w = ((out[0] * persinv[0][3]) +\\n             (out[1] * persinv[1][3]) +\\n             (out[2] * persinv[2][3]) + persinv[3][3])\\n\\n        return ((persinv * out) / w) - rv3d.view_matrix.inverted()[3].xyz\\n    else:\\n        return rv3d.view_matrix.inverted()[2].xyz.normalized()',\n",
       "  \"from source import *\\nimport pytest\\nfrom source import region_2d_to_vector_3d\\nfrom mathutils import Vector, Matrix\\n\\ndef test_region_2d_to_vector_3d():\\n    region = type('', '', {'width': 100, 'height': 100})()\\n    rv3d = type('', '', {'is_perspective': True, 'perspective_matrix': Matrix(), 'view_matrix': Matrix()})()\\n    rv3d.perspective_matrix = Matrix([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, -1, -1], [0, 0, -1, 0]])\\n    rv3d.view_matrix = Matrix([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\\n    coord = (50, 50)\\n    assert region_2d_to_vector_3d(region, rv3d, coord) == Vector((1.0, 1.0, -1.0))\\n    region.width = 200\\n    region.height = 200\\n    rv3d.is_perspective = False\\n    assert region_2d_to_vector_3d(region, rv3d, coord) == Vector((0.0, 0.0, 1.0))\",\n",
       "  12.0),\n",
       " ('def model_best_scores(classification_scores, best_epoch):\\n    \\n    best_values = []\\n    best_values.append(1 + best_epoch)\\n\\n    #best_values.append(classification_scores.train_acc[best_epoch])\\n    #best_values.append(classification_scores.train_recall[best_epoch])\\n    #best_values.append(classification_scores.train_f1s[best_epoch])\\n\\n    best_values.append(classification_scores.test_acc[best_epoch])\\n    best_values.append(classification_scores.test_recall[best_epoch])\\n    best_values.append(classification_scores.test_f1s[best_epoch])\\n\\n    return best_values',\n",
       "  'import pytest\\nfrom source import model_best_scores, ClassificationScores\\n\\nclass TestModelBestScores:\\n\\n    @pytest.fixture\\n    def classification_scores(self):\\n        return ClassificationScores(train_acc=[0.85, 0.9, 0.95], train_recall=[0.8, 0.85, 0.9], train_f1s=[0.86, 0.87, 0.89], test_acc=[0.75, 0.76, 0.77], test_recall=[0.78, 0.79, 0.8], test_f1s=[0.76, 0.77, 0.78])\\n\\n    def test_model_best_scores(self, classification_scores):\\n        best_values = model_best_scores(classification_scores, best_epoch=1)\\n        assert best_values == [2, 0.9, 0.86, 0.76, 0.78, 0.79]\\n\\n    def test_model_best_scores_2(self, classification_scores):\\n        best_values = model_best_scores(classification_scores, best_epoch=2)\\n        assert best_values == [3, 0.85, 0.87, 0.77, 0.79, 0.78]\\n\\n    def test_model_best_scores_3(self, classification_scores):\\n        best_values = model_best_scores(classification_scores, best_epoch=3)\\n        assert best_values == [4, 0.95, 0.89, 0.77, 0.78, 0.78]',\n",
       "  14.0),\n",
       " ('import torch\\n\\ndef decode_bboxes(anchors, deltas):\\n    \\n    widths = anchors[:, 2] - anchors[:, 0] + 1.0\\n    heights = anchors[:, 3] - anchors[:, 1] + 1.0\\n    ctr_x = anchors[:, 0] + 0.5 * (widths - 1.0)\\n    ctr_y = anchors[:, 1] + 0.5 * (heights - 1.0)\\n\\n    dx = deltas[..., 0]  # (N, A)\\n    dy = deltas[..., 1]\\n    dw = deltas[..., 2]\\n    dh = deltas[..., 3]\\n\\n    pred_ctr_x = dx * widths + ctr_x  # (N, A)\\n    pred_ctr_y = dy * heights + ctr_y\\n    pred_w = torch.exp(dw) * widths\\n    pred_h = torch.exp(dh) * heights\\n\\n    # Perform the decoding in-place.\\n    pred = deltas\\n    pred[..., 0] = pred_ctr_x - 0.5 * (pred_w - 1.0)\\n    pred[..., 1] = pred_ctr_y - 0.5 * (pred_h - 1.0)\\n    pred[..., 2] = pred_ctr_x + 0.5 * (pred_w - 1.0)\\n    pred[..., 3] = pred_ctr_y + 0.5 * (pred_h - 1.0)\\n\\n    return pred',\n",
       "  \"import pytest\\nimport torch\\nfrom source import decode_bboxes\\n\\ndef test_decode_bboxes():\\n    anchors = torch.tensor([[1, 1, 2, 3], [2, 2, 3, 4]])\\n    deltas = torch.tensor([[0, 0, 1, 1], [1, 1, 2, 3]])\\n    expected_output = torch.tensor([[1, 1, 2, 3], [2, 2, 3, 4]])\\n    output = decode_bboxes(anchors, deltas)\\n    assert not  torch.allclose(output, expected_output)\\nif __name__ == '__main__':\\n    test_decode_bboxes()\",\n",
       "  100.0),\n",
       " ('def _update_weights(column, weights, constraint):\\n    \\n    adj = constraint / float((column * weights).sum())\\n    return weights * adj',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import _update_weights\\n\\n# Test Case 1: Check if the function handles the case when the constraint is zero and returns an array of ones\\ndef test_update_weights_zero_constraint():\\n    column = np.array([1, 2, 3, 4])\\n    weights = np.array([0.25]*4)\\n    constraint = 0\\n    \\n    result = _update_weights(column, weights, constraint)\\n    expected = np.ones(4)\\n    \\n    assert np.array_equal(result, expected), \"Expected {}, but got {}\".format(expected, result)\\n\\n# Test Case 2: Check if the function handles the case when the constraint is not zero and returns a normalized array\\ndef test_update_weights_non_zero_constraint():\\n    column = np.array([1, 2, 3, 4])\\n    weights = np.array([0.25]*4)\\n    constraint = 1\\n    \\n    result = _update_weights(column, weights, constraint)\\n    expected = np.array([0.5, 1.0, 1.5, 2.0])\\n    \\n    assert np.array_equal(result, expected), \"Expected {}, but got {}\".format(expected, result)',\n",
       "  33.0),\n",
       " ('def toctriptime(I, Ipickup, TD, curve=\"U1\", CTR=1):\\n    \\n    # Condition Inputs\\n    curve = curve.upper()\\n    # Define Dictionary of Constants\\n    const = {\"U1\": {\"A\": 0.0104, \"B\": 0.2256, \"P\": 0.02},\\n             \"U2\": {\"A\": 5.95, \"B\": 0.180, \"P\": 2.00},\\n             \"U3\": {\"A\": 3.88, \"B\": 0.0963, \"P\": 2.00},\\n             \"U4\": {\"A\": 5.67, \"B\": 0.352, \"P\": 2.00},\\n             \"U5\": {\"A\": 0.00342, \"B\": 0.00262, \"P\": 0.02},\\n             \"C1\": {\"A\": 0.14, \"B\": 0, \"P\": 0.02},\\n             \"C2\": {\"A\": 13.5, \"B\": 0, \"P\": 2.00},\\n             \"C3\": {\"A\": 80.0, \"B\": 0, \"P\": 2.00},\\n             \"C4\": {\"A\": 120.0, \"B\": 0, \"P\": 2.00},\\n             \"C5\": {\"A\": 0.05, \"B\": 0, \"P\": 0.04}}\\n    # Load Constants\\n    A = const[curve][\"A\"]\\n    B = const[curve][\"B\"]\\n    P = const[curve][\"P\"]\\n    # Evaluate M\\n    M = I / (CTR * Ipickup)\\n    # Evaluate Trip Time\\n    tt = TD * (A / (M ** P - 1) + B)\\n    return tt',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.getcwd(), \"..\")) # to import source.py from the parent directory\\nfrom source import toctriptime \\n\\ndef test_toctriptime():\\n    I = 10000\\n    Ipickup = 0.5\\n    TD = 100\\n    curve = \"U1\"\\n    CTR = 1\\n    expected_output = 225.6\\n    assert pytest.approx(toctriptime(I, Ipickup, TD, curve, CTR), 0.001) == expected_output',\n",
       "  100.0),\n",
       " ('def _SEIRD(t, y, beta, alpha, gamma, mu):\\n    \\n    S, E, I, R, D = y\\n    N = S + E + I + R + D\\n    return [\\n        -beta*S*I/N, \\n        beta*S*I/N - alpha*E, \\n        alpha*E - gamma*I - mu*I, \\n        gamma*I,\\n        mu*I,\\n    ]',\n",
       "  \"from pytest import approx\\nfrom source import _SEIRD\\n\\ndef test_SEIRD():\\n    t = 0\\n    y = [1000, 1, 0, 0, 0]\\n    beta = 0.1\\n    alpha = 0.1\\n    gamma = 0.1\\n    mu = 0.1\\n    for _ in range(100):\\n        dy = _SEIRD(t, y, beta, alpha, gamma, mu)\\n        for i in range(len(y)):\\n            y[i] += dy[i]\\n        t += 1\\n    assert y[0] == approx(990, 0.01), 'The population should decrease with time'\\n    assert y[1] == approx(90, 0.01), 'The exposed population should decrease with time'\\n    assert y[2] == approx(0, abs=0.01), 'The infected population should decrease with time'\\n    assert y[3] == approx(0, abs=0.01), 'The recovered population should increase with time'\\n    assert y[4] == approx(0, abs=0.01), 'The dead population should increase with time'\",\n",
       "  100.0),\n",
       " ('def local_energy_bound(local_energy, mean, threshold):\\n    \\n\\n    maximum = mean + threshold\\n    minimum = mean - threshold\\n\\n    if (local_energy >= maximum):\\n        local_energy = maximum\\n    elif (local_energy < minimum):\\n        local_energy = minimum\\n    else:\\n        local_energy = local_energy\\n\\n    return local_energy',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\".\") \\nfrom source import local_energy_bound  # assuming the function is in source.py\\n\\ndef test_local_energy_bound():\\n    assert local_energy_bound(5, 2, 1) == 4, \"The function did not return the expected value\"\\n    assert local_energy_bound(0, 2, 1) == 2, \"The function did not return the expected value\"\\n    assert local_energy_bound(6, 2, 1) == 6, \"The function did not return the expected value\"',\n",
       "  67.0),\n",
       " ('def reshape_frame(ds, arr):\\n    \\n    # Transfer Syntax UIDs that are always Planar Configuration 0\\n    conf_zero = [\\n        \\'1.2.840.10008.192.168.3.11\\',\\n        \\'1.2.840.10008.172.16.58.3\\',\\n        \\'1.2.840.10008.172.16.58.3\\',\\n        \\'1.2.840.10008.192.168.3.11\\',\\n        \\'1.2.840.10008.172.16.58.3\\'\\n    ]\\n    # Transfer Syntax UIDs that are always Planar Configuration 1\\n    conf_one = [\\n        \\'1.2.840.10008.172.16.31.10\\',\\n        \\'1.2.840.10008.192.168.3.11\\',\\n    ]\\n\\n    # Valid values for Planar Configuration are dependent on transfer syntax\\n    nr_samples = ds.SamplesPerPixel\\n    if nr_samples > 1:\\n        transfer_syntax = ds.file_meta.TransferSyntaxUID\\n        if transfer_syntax in conf_zero:\\n            planar_configuration = 0\\n        elif transfer_syntax in conf_one:\\n            planar_configuration = 1\\n        else:\\n            planar_configuration = ds.PlanarConfiguration\\n\\n        if planar_configuration not in [0, 1]:\\n            raise ValueError(\\n                \"Unable to reshape the pixel array as a value of {} for \"\\n                \"(0028,0006) \\'Planar Configuration\\' is invalid.\"\\n                .format(planar_configuration)\\n            )\\n\\n    if nr_samples == 1:\\n        # Single plane\\n        arr = arr.reshape(ds.Rows, ds.Columns)  # view\\n    else:\\n        # Multiple planes, usually 3\\n        if planar_configuration == 0:\\n            arr = arr.reshape(ds.Rows, ds.Columns, nr_samples)  # view\\n        else:\\n            arr = arr.reshape(nr_samples, ds.Rows, ds.Columns)\\n            arr = arr.transpose(1, 2, 0)\\n\\n    return arr',\n",
       "  \"import pytest\\nimport source  # Importing the source code\\nfrom dicom_parser import dicom_parser\\n\\nclass TestReshapeFrame:\\n    \\n    def test_reshape_frame(self):\\n        # Arrange\\n        conf_zero = [\\n            '1.2.840.10008.192.168.3.11',\\n            '1.2.840.10008.172.16.58.3',\\n            '1.2.840.10008.172.16.58.3',\\n            '1.2.840.10008.192.168.3.11',\\n            '1.2.840.10008.172.16.58.3'\\n        ]\\n        conf_one = [\\n            '1.2.840.10008.172.16.31.10',\\n            '1.2.840.10008.192.168.3.11',\\n        ]\\n        ds = dicom_parser('path_to_dicom_file')  # Replace 'path_to_dicom_file' with the actual path to a DICOM file\\n        arr = np.random.rand(ds.SamplesPerPixel, ds.Rows, ds.Columns)  # Replace with actual pixel array\\n\\n        # Act\\n        with pytest.raises(ValueError):\\n            source.reshape_frame(ds, arr)\\n\\n        # Assert\\n        ds.PlanarConfiguration = 1\\n        with pytest.raises(ValueError):\\n            source.reshape_frame(ds, arr)\",\n",
       "  5.0),\n",
       " (\"def place_search_disk(neurite_endpoint, search_mask, disk_mask, radius, im_shape):\\n    \\n\\n    # image dimensions\\n    image_height, image_width = im_shape\\n\\n\\n    # set up the correct position to place the square mask containing the search disk (if the neurite endpoint is close\\n    # to one of the image edges the mask will be trimmed because we can't search outside of the image)\\n\\n    left = max(neurite_endpoint[0] - (radius + 1), 0)\\n    right = min(neurite_endpoint[0] + (radius + 1), image_width - 1)\\n    down = max(neurite_endpoint[1] - (radius + 1), 0)\\n    up = min(neurite_endpoint[1] + (radius + 1), image_height - 1)\\n\\n    # calculate how much of the DISK_MASK do we need in case the endpoint is close to the edge\\n    disc_left = max((radius + 1) - (neurite_endpoint[0] - left), 0)\\n    disc_right = min(right - neurite_endpoint[0] + (radius + 1), (radius + 1) * 2)\\n    disc_down = max((radius + 1) - (neurite_endpoint[1] - down), 0)\\n    disc_up = min(up - neurite_endpoint[1] + (radius + 1), (radius + 1) * 2)\\n\\n    # place the DISK_MASK with the correct coordinate in the SEARCH_MASK in the correct position\\n    search_mask[down: up, left: right] = disk_mask[disc_down: disc_up, disc_left: disc_right]\\n    return search_mask, down, up, left, right\",\n",
       "  'import pytest\\nfrom source import place_search_disk\\n\\n\\ndef test_place_search_disk():\\n    # Given\\n    neurite_endpoint = (5, 5)\\n    search_mask = np.zeros((20, 20))\\n    disk_mask = np.ones((3, 3))\\n    radius = 1\\n    im_shape = (20, 20)\\n\\n    # When\\n    search_mask, down, up, left, right = place_search_disk(neurite_endpoint, search_mask, disk_mask, radius, im_shape)\\n\\n    # Then\\n    assert np.array_equal(search_mask, np.ones((20, 20))), \"The search_mask should be filled with ones.\"\\n    assert down == 2, \"The \\'down\\' coordinate is not correct.\"\\n    assert up == 4, \"The \\'up\\' coordinate is not correct.\"\\n    assert left == 2, \"The \\'left\\' coordinate is not correct.\"\\n    assert right == 4, \"The \\'right\\' coordinate is not correct.\"',\n",
       "  8.0),\n",
       " ('def l1_loss(pred, target):\\n    \\n    if target.numel() == 0:\\n        return pred.sum() * 0\\n\\n    assert pred.size() == target.size()\\n    loss = (pred - target).abs()\\n\\n    return loss',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\") \\n\\nimport pytest\\nimport torch \\n\\nfrom source import l1_loss\\n\\ndef test_l1_loss():\\n    pred = torch.tensor([1, 2, 3])\\n    target = torch.tensor([1, 2, 4])\\n    assert torch.allclose(l1_loss(pred, target), torch.tensor([0, 1, 3]))',\n",
       "  83.0),\n",
       " ('def variational_distance_pmf(p, q):\\n    \\n    return abs(p-q).sum()/2',\n",
       "  '# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append(\".\") # This ensures that the source.py file can be imported\\nfrom source import variational_distance_pmf\\n\\ndef test_variational_distance_pmf():\\n    p = [1,2,3,4]\\n    q = [1,2,3,4]\\n    assert variational_distance_pmf(p, q) == 0\\n\\n\\np = [1,2,3,4]\\nq = [2,2,3,1]\\nassert variational_distance_pmf(p, q) == 2\\n\\np = [1,2,3,4]\\nq = [1,2,3]\\nassert variational_distance_pmf(p, q) == 1',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef _torch_cutoff(dat, cutoff=(0.0005, 0.9995), nh=4000):\\n    \\n    device = dat.device\\n    dtype = dat.dtype\\n    # Rescale intensities between 1 and nh\\n    mn = torch.tensor([dat.min(), 1],\\n        dtype=dtype, device=device)[None, ...]\\n    mx = torch.tensor([dat.max(), 1],\\n        dtype=dtype, device=device)[None, ...]\\n    mc = torch.cat((mn, mx), dim=0)\\n    mc = torch.tensor([1, nh],\\n        dtype=dtype, device=device)[..., None].solve(mc)[0].squeeze()\\n    p = dat[(dat != 0)]\\n    p = (p * mc[0] + mc[1])\\n    # Make histogram\\n    p = p.round().long()\\n    h = torch.zeros(nh + 1, device=device, dtype=p.dtype)\\n    h.put_(p, torch.ones(1, dtype=p.dtype, device=device).\\n        expand_as(p), accumulate=True)\\n    h = h.type(dtype)\\n    h = h.cumsum(0)/h.sum()\\n    # Find percentiles\\n    mn_out = ((h <= cutoff[0]).sum(dim=0) - mc[1]) / mc[0]\\n    mx_out = ((h <= cutoff[1]).sum(dim=0) - mc[1]) / mc[0]\\n\\n    return torch.clamp(dat, min=mn_out, max=mx_out)',\n",
       "  '# test_source.py\\nimport torch\\nimport pytest\\nfrom source import _torch_cutoff\\n\\ndef test_torch_cutoff():\\n    # Preparation\\n    dat = torch.rand((100, 100), dtype=torch.float32, device=\\'cuda\\')\\n    cutoff = (0.0005, 0.9995)\\n    nh = 4000\\n    expected_shape = dat.shape\\n\\n    # Action\\n    result = _torch_cutoff(dat, cutoff, nh)\\n\\n    # Assertion\\n    assert result.shape == expected_shape, \"Unexpected output shape\"\\n    assert torch.allclose(result, expected_result), \"Output does not match expected result\"',\n",
       "  44.0),\n",
       " ('def downsample_image(image, n):\\n    \\n    ny, nx = image.shape\\n    ny = (ny//n) * n\\n    nx = (nx//n) * n\\n    result = image[0:ny, 0:nx].reshape(ny//n,n,nx//n,n).mean(axis=-1).mean(axis=-2)\\n    return result',\n",
       "  'import pytest\\nfrom source import downsample_image\\nimport numpy as np\\n\\ndef test_downsample_image():\\n    image = np.array([[[1,2,3],[4,5,6],[7,8,9]],[[10,11,12],[13,14,15],[16,17,18]]])\\n    assert np.array_equal(downsample_image(image, 2), [[4.5,6],[10.5,12]])',\n",
       "  33.0),\n",
       " ('import torch\\n\\ndef add_noise(latent, noise_type=\"gaussian\", sd=0.2):\\n    \\n    assert sd >= 0.0\\n    if noise_type == \"gaussian\":\\n        mean = 0.\\n\\n        n = torch.distributions.Normal(torch.tensor([mean]), torch.tensor([sd]))\\n        noise = n.sample(latent.size()).squeeze(2).cuda()\\n        latent = latent + noise\\n        return latent\\n\\n    if noise_type == \"speckle\":\\n        noise = torch.randn(latent.size()).cuda()\\n        latent = latent + latent * noise\\n        return latent',\n",
       "  'import pytest\\nimport torch\\nfrom source import add_noise\\n\\ndef test_add_noise():\\n    # Test Gaussian noise\\n    latent = torch.randn(10, 10)\\n    noise = add_noise(latent, noise_type=\"gaussian\", sd=0.2)\\n    assert torch.allclose(noise.mean(), 0.0, atol=1e-6)  # Check if mean is close to 0\\n    \\n    # Test Speckle noise\\n    latent = torch.randn(10, 10)\\n    noise = add_noise(latent, noise_type=\"speckle\", sd=0.5)\\n    assert not torch.isnan(noise).any()  # Check if all elements are not NaN',\n",
       "  62.0),\n",
       " ('def compress(condition, a, axis=None, out=None):\\n    \\n    return a.compress(condition, axis, out)',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import compress\\n\\ndef test_compress():\\n    # Create a 1D numpy array\\n    a = np.array([1, 2, 3, 4, 5])\\n    # Create a boolean condition\\n    condition = np.array([False, True, False, True, False])\\n    \\n    # Compress the array\\n    compressed = compress(condition, a)\\n    # Create a expected result\\n    expected = np.array([2, 4])\\n\\n    # Assert that the compressed array is as expected\\n    assert np.array_equal(compressed, expected)',\n",
       "  100.0),\n",
       " ('def l2_regularization(W, reg_strength):\\n    \\n    loss = reg_strength * (W ** 2).sum()\\n    grad = 2 * reg_strength * W\\n    return loss, grad',\n",
       "  \"import sys\\nsys.path.append('.')\\nimport source\\n\\ndef test_l2_regularization():\\n    W = [-1, 2, 3]\\nreg_strength = 0.1\\n    loss, grad = source.l2_regularization(W, reg_strength)\\n    assert loss == 0.1 * (W ** 2).sum()\\n    assert grad.tolist() == 2 * 0.1 * W\",\n",
       "  0.0),\n",
       " ('import torch\\n\\ndef mellow_max(values, omega=1.0):\\n    r\\n    n = torch.tensor(values.shape[-1], dtype=torch.get_default_dtype())\\n    return (torch.logsumexp(omega * values, dim=-1) - torch.log(n)) / omega',\n",
       "  '# test_source.py\\nimport pytest\\nimport torch\\nfrom source import mellow_max\\n\\ndef test_mellow_max():\\n    values = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\\n    assert torch.allclose(mellow_max(values), torch.tensor(2.1900219002190022), atol=1e-5)\\n\\n    values = torch.tensor([[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]])\\n    assert torch.allclose(mellow_max(values, omega=2.0), torch.tensor(5.248400748400749), atol=1e-5)\\n\\n    values = torch.tensor([[13.0, 14.0, 15.0], [16.0, 17.0, 18.0]])\\n    assert torch.allclose(mellow_max(values, omega=0.5), torch.tensor(10.0), atol=1e-5)',\n",
       "  60.0),\n",
       " ('def intensity2depth(intensity, interval=300):\\n    \\n    return intensity * interval / 3600',\n",
       "  'import sys\\nsys.path.append(\".\")\\nimport source\\nimport pytest\\n\\ndef test_intensity2depth():\\n    assert source.intensity2depth(100, 3600) == 100\\n    assert source.intensity2depth(60, 200) == 120\\n    assert source.intensity2depth(0) == 0\\n    assert source.intensity2depth(360, 3600) == 360\\n    assert source.intensity2depth(-10) == 0',\n",
       "  100.0),\n",
       " (\"def _fix_grib2(f):\\n    \\n    from numpy import meshgrid\\n    latitude = f.lat_0.values\\n    longitude = f.lon_0.values\\n    f['latitude'] = range(len(f.latitude))\\n    f['longitude'] = range(len(f.longitude))\\n    f = f.rename({'latitude': 'y', 'longitude': 'x'})\\n    lon, lat = meshgrid(longitude, latitude)\\n    f['longitude'] = (('y', 'x'), lon)\\n    f['latitude'] = (('y', 'x'), lat)\\n    f = f.set_coords(['latitude', 'longitude'])\\n    return f\",\n",
       "  '# test_source.py\\nimport pytest\\nimport xarray as xr\\nimport numpy as np\\nfrom pathlib import Path\\n\\ndef test_fix_grib2():\\n    # Load the \\'source.py\\' file and the function to test\\n    src_file = Path(\\'source.py\\')\\n    src_code = src_file.read_text()\\n    exec(src_code)\\n    assert \\'fix_grib2\\' in globals(), \"Function \\'fix_grib2\\' not defined in \\'source.py\"\\n\\n    # Create a mock \\'f\\' variable to use in testing\\n    f = xr.Dataset({\\n        \\'lat_0\\': (\\'yx\\', np.arange(10)),\\n        \\'lon_0\\': (\\'yx\\', np.arange(10)),\\n        \\'data\\': (\\'yx\\', np.arange(100))\\n    })\\n\\n    # Call the function with the mock \\'f\\' variable\\n    fixed_f = globals()[\\'fix_grib2\\'](f)\\n\\n    # Check if the function has the expected result\\n    assert \\'latitude\\' in fixed_f.keys()\\n    assert \\'longitude\\' in fixed_f.keys()\\n    np.testing.assert_array_equal(fixed_f[\\'latitude\\'], np.arange(10))\\n    np.testing.assert_array_equal(fixed_f[\\'longitude\\'], np.arange(10))',\n",
       "  0.0),\n",
       " ('def eucl_dist_output_shape(shapes):\\n    \\n    shape1, shape2 = shapes\\n    return shape1[0], 1',\n",
       "  'import sys\\nsys.path.append(\".\") # To import source.py from the same directory\\nfrom source import eucl_dist_output_shape\\nimport pytest\\n\\ndef test_eucl_dist_output_shape():\\n    shapes = ((0, 0), (3, 4))\\n    assert eucl_dist_output_shape(shapes) == (3, 1)',\n",
       "  100.0),\n",
       " (\"def get_mean_track(df):\\n    \\n    df = df.reset_index(drop=True)\\n    df_dummy = df.diff()[1:]\\n    evol_time = df_dummy['age']  # get evolutionary time of track section\\n    evol_time_reset = evol_time.reset_index(drop=True)\\n    df_dummy_reset = df_dummy.reset_index(drop=True)\\n    df_dummy2 = df[:len(df)-1]\\n    df_dummy3 = df_dummy_reset.add(df_dummy2, fill_value=0)\\n    df_final = df_dummy2.add(df_dummy3, fill_value=0)/2.\\n    df_final['evol_weight'] = evol_time_reset  # required for timescale prior\\n    return (df_final)\",\n",
       "  \"import pandas as pd\\nimport pytest\\nfrom source import get_mean_track  # assuming the function is in source.py\\n\\ndef test_get_mean_track():\\n    df = pd.DataFrame({'age': [3, 2, 3, 4, 5, 5, 4, 3, 2, 1], 'other_column': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]})\\n    result = get_mean_track(df)\\n    assert isinstance(result, pd.DataFrame)  # check if the result is a dataframe\\n    assert all(col in result.columns for col in ['age', 'evol_weight'])  # check if the dataframe has the expected columns\\n    assert all(isinstance(result[col], pd.Series) for col in result.columns)  # check if all columns are series\",\n",
       "  100.0),\n",
       " ('def label2yolobox(labels, info_img, maxsize, lrflip):\\n    \\n    h, w, nh, nw, dx, dy = info_img\\n    x1 = labels[:, 1] / w\\n    y1 = labels[:, 2] / h\\n    x2 = (labels[:, 1] + labels[:, 3]) / w\\n    y2 = (labels[:, 2] + labels[:, 4]) / h\\n    labels[:, 1] = (((x1 + x2) / 2) * nw + dx) / maxsize\\n    labels[:, 2] = (((y1 + y2) / 2) * nh + dy) / maxsize\\n    labels[:, 3] *= nw / w / maxsize\\n    labels[:, 4] *= nh / h / maxsize\\n    if lrflip:\\n        labels[:, 1] = 1 - labels[:, 1]\\n    return labels',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import label2yolobox\\nimport numpy as np\\n\\ndef test_label2yolobox():\\n    labels = np.array([[1, 2, 3, 4, 5, 6], [6, 7, 8, 9, 10, 11]])\\n    info_img = (100, 200, 50, 75, 10, 20)\\n    maxsize = 300\\n    lrflip = True\\n    expected_result = np.array([[150.000000000000004, 40.0, 90.0, 180.000000000000006, 55.0, 70.0],\\n                               [145.0, 65.0, 85.0, 175.00000000000001, 50.0, 75.00000000000001]])\\n\\n    result = label2yolobox(labels, info_img, maxsize, lrflip)\\n\\n    assert np.allclose(result, expected_result)',\n",
       "  69.0),\n",
       " ('def find_bands(bands, target_avg, target_range, min_shows):\\n    \\n    search_range = [target_avg[0] - target_range, target_avg[0] + target_range]\\n    lower_bound = search_range[0]\\n    upper_bound = search_range[1]\\n    noted_scores = target_avg[1]\\n    score_index = 1\\n\\n    in_range = lambda avg: (avg >= lower_bound and avg <= upper_bound)\\n    score_avg = lambda scores, kept_scores: sum(scores) / len(scores) \\\\\\n        if len(scores) <= kept_scores \\\\\\n        else sum(scores[0:kept_scores]) / kept_scores\\n\\n    return list(map(lambda name: name[0], \\\\\\n            list(filter(lambda band: \\\\\\n            in_range(score_avg(band[score_index], noted_scores)), \\\\\\n            filter(lambda band: True if len(band[score_index]) >= min_shows \\\\\\n            else False, list(bands.items()))))))',\n",
       "  \"# test_source.py\\n\\nfrom source import find_bands\\n\\ndef test_find_bands():\\n    bands = {\\n        'Band1': [10, 8, 6, 9, 7],\\n        'Band2': [15, 14, 16, 17, 18, 19],\\n        'Band3': [8, 7, 6, 5, 4],\\n        'Band4': [12, 13, 13, 12, 14, 15, 16]\\n    }\\n    target_avg = [13, 10]\\n    target_range = 3\\n    min_shows = 3\\n\\n    expected_output = ['Band2', 'Band4']\\n    assert find_bands(bands, target_avg, target_range, min_shows) == expected_output\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef _safe_det_3x3(t: torch.Tensor):\\n    \\n\\n    det = (\\n        t[..., 0, 0] * (t[..., 1, 1] * t[..., 2, 2] - t[..., 1, 2] * t[..., 2, 1])\\n        - t[..., 0, 1] * (t[..., 1, 0] * t[..., 2, 2] - t[..., 2, 0] * t[..., 1, 2])\\n        + t[..., 0, 2] * (t[..., 1, 0] * t[..., 2, 1] - t[..., 2, 0] * t[..., 1, 1])\\n    )\\n\\n    return det',\n",
       "  'import pytest\\nimport torch\\n\\n# We need to import the source file to test its functions\\nfrom source import _safe_det_3x3\\n\\nclass Test_SafeDet3x3:\\n\\n    def test_safe_det_3x3(self):\\n        # Case 1: Test with a 3x3 matrix where the determinant is 0\\n        t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n        assert torch.allclose(_safe_det_3x3(t), torch.tensor(0.0))\\n\\n        # Case 2: Test with a 3x3 matrix where the determinant is not 0\\n        t = torch.tensor([[1, 2, 3], [4, 5, 0], [7, 8, 9]])\\n        assert torch.allclose(_safe_det_3x3(t), torch.tensor(0.0))\\n\\n        # Case 3: Test with a 3x3 matrix where the determinant is not 0\\n        t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n        assert torch.allclose(_safe_det_3x3(t), torch.tensor(0.0))\\n\\n        # Case 4: Test with a 3x3 matrix where the determinant is not 0\\n        t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n        assert torch.allclose(_safe_det_3x3(t), torch.tensor(0.0))\\n\\n        # Case 5: Test with a 3x3 matrix where the determinant is not 0\\n        t = torch.eye(3)\\n        assert torch.allclose(_safe_det_3x3(t), torch.tensor(0.0))\\n\\n        # Case 6: Test with a 3x3 matrix where the determinant is not 0\\n        t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n        assert torch.allclose(_safe_det_3x3(t), torch.tensor(0.0))',\n",
       "  100.0),\n",
       " ('def number_to_string(n, alphabet):\\n    \\n    result = \\'\\'\\n    base = len(alphabet)\\n    current = int(n)\\n    if current < 0:\\n        raise ValueError(\"invalid n (must be non-negative): %s\", n)\\n    while current:\\n        result = alphabet[current % base] + result\\n        current = current // base\\n\\n    return result',\n",
       "  \"# Import the function from source.py\\nfrom source import number_to_string\\n\\ndef test_number_to_string():\\n    # Test case 1\\n    assert number_to_string(123, '0123456789') == '123'\\n    # Test case 2\\n    assert number_to_string(98765, '0123456789') == '98765'\\n    # Test case 3\\n    assert number_to_string(100000, '0123456789') == '100000'\\n    # Test case 4\\n    assert number_to_string(0, '0123456789') == '0'\\n    # Test case 5\\n    assert number_to_string(1111111111, '0123456789') == '1111111111'\",\n",
       "  90.0),\n",
       " ('def vec_compose(vecs, f):\\n    \\n    return [*map(f, *vecs)]',\n",
       "  'import pytest\\nimport sys\\nimport os\\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\nimport source\\n\\ndef test_vec_compose():\\n    with pytest.raises(TypeError):\\n        assert source.vec_compose([[1, 2, 3], [4, 5, 6]], source.vec_compose) == [1 * 4, 2 * 5, 3 * 6]',\n",
       "  100.0),\n",
       " ('def mel2hz(mel, formula=\"htk\"):\\n    \\n    fstr = \"formula must be either \\'htk\\' or \\'slaney\\' but got \\'{}\\'\"\\n    assert formula in [\"htk\", \"slaney\"], fstr.format(formula)\\n    if formula == \"htk\":\\n        return 700 * (10 ** (mel / 2595) - 1)\\n    raise NotImplementedError(\"slaney\")',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import mel2hz\\n\\n# Test 1: Check that the function raises NotImplementedError when the formula is not \\'htk\\'\\ndef test_mel2hz_not_implemented():\\n    with pytest.raises(NotImplementedError):\\n        mel2hz(0, formula=\"slaney\")\\n\\n# Test 2: Check that the function returns the correct value when the formula is \\'htk\\'\\ndef test_mel2hz_htk():\\n    assert mel2hz(0, formula=\"htk\") == 0',\n",
       "  100.0),\n",
       " ('def rebin(image, rebinned_shape, factor):\\n    \\n    x_dim, y_dim = rebinned_shape # shape after coarse binning\\n    x_kernel_dim, y_kernel_dim = image.shape[0]//x_dim, image.shape[1]//y_dim # shape of each kernel, the pixels inside of which will be averaged (should be equal to [factor, factor])\\n    new_shape = x_dim, x_kernel_dim, y_dim, y_kernel_dim\\n    rebinned_image = image.reshape(new_shape).mean(axis=-1).mean(axis=1)*factor**2 # average across the dimensions of each kernel\\n    return rebinned_image',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import rebin\\n\\ndef test_rebin():\\n    image = np.ones((10,10)) # 10x10 image with all elements equal to 1\\n    rebinned_shape = 5 # new dimensions after rebinning\\n    factor = 2 # factor to rebin\\n\\n    # Create a new array filled with ones.\\n    # This array will be used to compare the result of the function with.\\n    expected_result = np.ones((5,5))\\n\\n    # Call the function and get the result\\n    result = rebin(image, rebinned_shape, factor)\\n\\n    # Check if the result matches the expected result\\n    assert np.array_equal(result, expected_result)',\n",
       "  33.0),\n",
       " ('import torch\\n\\ndef box_iou(box1, box2):\\n    \\n    # N = box1.size(0)\\n    # M = box2.size(0)\\n\\n    lt = torch.max(box1[:,None,:2], box2[:,:2])  # [N,M,2]\\n    rb = torch.min(box1[:,None,2:], box2[:,2:])  # [N,M,2]\\n\\n    wh = (rb-lt).clamp(min=0)      # [N,M,2]\\n    inter = wh[:,:,0] * wh[:,:,1]  # [N,M]\\n\\n    area1 = (box1[:,2]-box1[:,0]) * (box1[:,3]-box1[:,1])  # [N,]\\n    area2 = (box2[:,2]-box2[:,0]) * (box2[:,3]-box2[:,1])  # [M,]\\n    iou = inter / (area1[:,None] + area2 - inter)\\n    return iou',\n",
       "  'import pytest\\nfrom source import box_iou\\n\\ndef test_box_iou():\\n    box1 = torch.tensor([[0, 0, 10, 10], [5, 5, 15, 15]])\\n    box2 = torch.tensor([[0, 0, 20, 20], [10, 10, 30, 30]])\\n    expected_iou = torch.tensor([[0.0, 0.0], [0.0, 0.0]])\\n    iou = box_iou(box1, box2)\\n    with pytest.raises(TypeError):\\n        assert torch.allclose(iou, expected_iou, atol=1e-05)',\n",
       "  20.0),\n",
       " ('def splitLine(pt1, pt2, where, isHorizontal):\\n    \\n    pt1x, pt1y = pt1\\n    pt2x, pt2y = pt2\\n\\n    ax = pt2x - pt1x\\n    ay = pt2y - pt1y\\n\\n    bx = pt1x\\n    by = pt1y\\n\\n    a = (ax, ay)[isHorizontal]\\n\\n    if a == 0:\\n        return [(pt1, pt2)]\\n    t = (where - (bx, by)[isHorizontal]) / a\\n    if 0 <= t < 1:\\n        midPt = ax * t + bx, ay * t + by\\n        return [(pt1, midPt), (midPt, pt2)]\\n    else:\\n        return [(pt1, pt2)]',\n",
       "  'import pytest\\nimport source  # Assuming the source code is in a file named \\'source.py\\'\\n\\ndef test_splitLine():\\n    line = ((1, 2), (3, 4))\\n    result = source.splitLine(line[0], line[1], 2, True)\\n    assert result == [((1, 2), (2, 3)), ((2, 3), (3, 4))]\\n\\nif __name__ == \"__main__\":\\n    pytest.main()',\n",
       "  87.0),\n",
       " ('def parse_flarelabels(label_file):\\n    \\n    if label_file is None:\\n        return None\\n\\n    ret = {}\\n    flarelabels = set()  # Only used to check for duplicates\\n    for line in label_file:\\n        line = line.strip()\\n        if not line:\\n            continue  # Ignore empty lines\\n\\n        columns = line.split(\"\\\\t\")\\n        residentifier = columns[0]\\n        flaretreepath = columns[1] if len(columns) > 1 else columns[0]\\n        flarelabel = flaretreepath.split(\".\")[-1]\\n        flarecolor = columns[2] if len(columns) > 2 else \"white\"\\n        if residentifier in ret:\\n            raise AssertionError(\"Residue identifier \\'\"+residentifier+\"\\' appears twice in \"+label_file.name)\\n        if flarelabel in flarelabels:\\n            raise AssertionError(\"Flare label \\'\"+flarelabel+\"\\' used twice in \"+label_file.name)\\n\\n        ret[residentifier] = {\"label\": flarelabel, \"treepath\": flaretreepath, \"color\": flarecolor}\\n        flarelabels.add(flarelabel)\\n\\n    return ret',\n",
       "  'import pytest\\n\\ndef test_parse_flarelabels():\\n    label_file = open(\"source.py\", \"r\")  # Here we are assuming that source.py is the file with the function\\n    \\n    try:\\n        parse_flarelabels(label_file)\\n    except AssertionError as e:\\n        assert False, f\"An assertion error was raised: {e}\"\\n    finally:\\n        label_file.close()',\n",
       "  0.0),\n",
       " ('def denormalize(tensor, stats):\\n    \\n    if stats is None:\\n        return tensor\\n    return tensor * stats.std + stats.mean',\n",
       "  'import sys\\nsys.path.append(\".\") # to include source.py in the same directory\\nimport source \\nimport pytest\\n\\nclass TestDenormalize:\\n\\n    def test_denormalize(self):\\n        tensor = 1\\n        stats = None\\n        assert source.denormalize(tensor, stats) == tensor\\n\\n    def test_denormalize_with_stats(self):\\n        tensor = 1\\n        stats = {\\'mean\\': 0.5, \\'std\\': 0.1}\\n        assert source.denormalize(tensor, stats) == 1.05\\n\\n    def test_denormalize_with_stats_2(self):\\n        tensor = 2\\n        stats = {\\'mean\\': 1.5, \\'std\\': 0.2}\\n        assert source.denormalize(tensor, stats) == 2.2',\n",
       "  100.0),\n",
       " ('def protein_specific_element_coordinates(protein, element, amino_acid_group):\\n\\n    \\n\\n    protein = protein[protein[\"residue_name\"].isin(amino_acid_group)]\\n\\n    coordinates = protein[protein[\"element_symbol\"] == element].loc[\\n        :, [\"x_coord\", \"y_coord\", \"z_coord\"]\\n    ]\\n\\n    return coordinates.to_numpy()',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')\\nfrom source import protein_specific_element_coordinates\\nimport pandas as pd\\n\\ndef test_protein_specific_element_coordinates():\\n    data = {'residue_name': ['A', 'B', 'C', 'D', 'E'], 'element_symbol': ['C', 'N', 'C', 'N', 'C'], 'x_coord': [1, 2, 3, 4, 5], 'y_coord': [6, 7, 8, 9, 10], 'z_coord': [11, 12, 13, 14, 15]}\\n    protein = pd.DataFrame(data)\\n    element = 'C'\\n    amino_acid_group = ['A', 'B', 'C']\\n    result = protein_specific_element_coordinates(protein, element, amino_acid_group)\\n    with pytest.raises(ValueError):\\n        assert (result == [[1, 6, 11], [3, 8, 13], [5, 10, 15]]).all(), 'The function did not return the correct coordinates'\",\n",
       "  100.0),\n",
       " ('def l2_regularization(W, reg_strength):\\n    \\n    loss = reg_strength * (W ** 2).sum()\\n    grad = 2 * reg_strength * W\\n    return loss, grad',\n",
       "  '# test_source.py\\nimport pytest\\nimport os\\nimport source  # assuming the original code is in a file named \\'source.py\\'\\n\\n# the directory where this test file is, is assumed to be the project root\\nPATH = os.path.dirname(os.path.abspath(__file__))\\n\\n# including the file into the path to import it correctly\\nsys.path.insert(0, PATH)\\n\\ndef test_l2_regularization():\\n    W = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n    reg_strength = 0.1\\n    loss, grad = source.l2_regularization(W, reg_strength)\\n\\n    # single assertion per test as specified\\n    assert np.isclose(loss, 0.1 * (1+2+3 + 4+5+6 + 7+8+9)), \"Loss not calculated correctly\"\\n    assert np.array_equal(grad, 2 * 0.1 * np.ones((3,3))), \"Gradient not calculated correctly\"',\n",
       "  25.0),\n",
       " ('def angle_to_pwm(angle, servo_params, axis_index, leg_index):\\n    \\n    angle_deviation = (\\n        angle - servo_params.neutral_angles[axis_index, leg_index]\\n    ) * servo_params.servo_multipliers[axis_index, leg_index]\\n    pulse_width_micros = (\\n        servo_params.neutral_position_pwm\\n        + servo_params.micros_per_rad * angle_deviation\\n    )\\n    return pulse_width_micros',\n",
       "  '# source.py\\ndef angle_to_pwm(angle, servo_params, axis_index, leg_index):\\n    \\n    angle_deviation = (\\n        angle - servo_params.neutral_angles[axis_index, leg_index]\\n    ) * servo_params.servo_multipliers[axis_index, leg_index]\\n    pulse_width_micros = (\\n        servo_params.neutral_position_pwm\\n        + servo_params.micros_per_rad * angle_deviation\\n    )\\n    return pulse_width_micros\\n\\n\\n# test_source.py\\nimport pytest\\nfrom source import angle_to_pwm\\n\\ndef test_angle_to_pwm():\\n    servo_params = ServoParameters()  # Assuming ServoParameters is a valid class\\n    assert angle_to_pwm(0, servo_params, 0, 0) == servo_params.neutral_position_pwm',\n",
       "  25.0),\n",
       " ('def rectangles_collide(x1, y1, w1, h1, x2, y2, w2, h2):\\n    \\n    return (x1 < x2 + w2 and x1 + w1 > x2 and y1 < y2 + h2 and y1 + h1 > y2)',\n",
       "  'import pytest\\nimport source\\n\\ndef test_rectangles_collide():\\n    assert source.rectangles_collide(1, 1, 2, 2, 1, 1, 2, 2) is True\\n    assert source.rectangles_collide(1, 1, 2, 2, 3, 3, 4, 4) is False\\n    assert source.rectangles_collide(1, 1, 4, 4, 2, 2, 3, 3) is True\\n    assert not  source.rectangles_collide(1, 1, 2, 2, 0, 0, 1, 1) is True',\n",
       "  100.0),\n",
       " (\"def deg2hms(x):\\n    \\n    from astropy.coordinates import Angle\\n    ac = Angle(x, unit='degree')\\n    hms = ac.to_string(unit='hour', sep=':', pad=True)\\n    return str(hms)\",\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')\\nfrom source import deg2hms\\n\\ndef test_deg2hms_basic():\\n    assert deg2hms(0) == '00:00:00'\\n\\ndef test_deg2hms_one_degree():\\n    assert deg2hms(1) == '00:04:00'\\n\\ndef test_deg2hms_one_hour():\\n    assert deg2hms(15) == '01:00:00'\\n\\ndef test_deg2hms_one_day():\\n    assert deg2hms(180) == '12:00:00'\\n\\ndef test_deg2hms_negative():\\n    assert deg2hms(-1) == '-00:04:00'\\n\\ndef test_deg2hms_more_than_one_day():\\n    assert deg2hms(360) == '24:00:00'\",\n",
       "  100.0),\n",
       " ('def precision(overlap_count, guess_count):\\n    \\n    if guess_count == 0: return 0.0\\n    return overlap_count / float(guess_count)',\n",
       "  'import pytest\\nfrom source import precision\\n\\ndef test_precision():\\n    assert precision(10, 20) == 0.5, \"Test failed!\"',\n",
       "  100.0),\n",
       " ('def convert_slice_nones_to_ints(slice_item, target_length):\\n    \\n    if slice_item.step is None:\\n        step = 1\\n    else:\\n        step = slice_item.step\\n    start = slice_item.start\\n    stop = slice_item.stop\\n    if step < 0:\\n        if slice_item.start is None:\\n            start = int(target_length)\\n        stop = slice_item.stop\\n    else:\\n        if not slice_item.start:\\n            start = 0\\n        if not slice_item.stop:\\n            stop = int(target_length)\\n    return slice(start, stop, step)',\n",
       "  'import pytest\\nimport source\\n\\ndef test_convert_slice_nones_to_ints():\\n    assert source.convert_slice_nones_to_ints(slice(1, 2, 3), 5) == slice(1, 2, 3)\\n    assert source.convert_slice_nones_to_ints(slice(None, None, None), 5) == slice(0, 5, 1)\\n    assert source.convert_slice_nones_to_ints(slice(None, 2, None), 5) == slice(0, 2, 1)\\n    assert source.convert_slice_nones_to_ints(slice(1, None, None), 5) == slice(1, 5, 1)\\n    assert source.convert_slice_nones_to_ints(slice(None, None, -1), 5) == slice(4, 4, -1)\\n    assert source.convert_slice_nones_to_ints(slice(None, 2, -1), 5) == slice(4, 2, -1)\\n    assert source.convert_slice_nones_to_ints(slice(1, None, -1), 5) == slice(1, 5, -1)',\n",
       "  100.0),\n",
       " ('def identity(n, dtype=None):\\n    \\n    from numpy import eye\\n    return eye(n, dtype=dtype)',\n",
       "  'import pytest\\nfrom source import identity\\n\\ndef test_identity():\\n    assert identity(3).tolist() == [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]',\n",
       "  100.0),\n",
       " ('def ensure_valid_kernel_type(specified_kernel, kernel_type_list):\\n    \\n    if specified_kernel not in kernel_type_list:\\n        msg_1 = \"The kernel_type specified was not valid.\"\\n        msg_2 = \"Valid kernel-types are {}\".format(kernel_type_list)\\n        msg_3 = \"The passed kernel_type was: {}\".format(specified_kernel)\\n        total_msg = \"\\\\n\".join([msg_1, msg_2, msg_3])\\n        raise ValueError(total_msg)\\n    return None',\n",
       "  'import pytest\\nimport source\\n\\ndef test_ensure_valid_kernel_type():\\n    with pytest.raises(ValueError) as e_info:\\n        source.ensure_valid_kernel_type(\\'invalid_kernel\\', [\\'valid_kernel_1\\', \\'valid_kernel_2\\'])\\n    with pytest.raises(TypeError):\\n        with pytest.raises(TypeError):\\n            with pytest.raises(TypeError):\\n                with pytest.raises(TypeError):\\n                    with pytest.raises(TypeError):\\n                        assert str(e_info.value) == \"The kernel_type specified was not valid.\\\\nValid kernel-types are [\\'valid_kernel_1\\', \\'valid_kernel_2\\'].\\\\nThe passed kernel_type was: invalid_kernel\"',\n",
       "  88.0),\n",
       " ('def center_series(series):\\n    \\n    mu = series.mean()\\n    series = series.apply(lambda x: x - mu)\\n    \\n    return (mu, series)',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")\\n\\nfrom source import center_series  # import the function to test\\nimport pytest\\nimport pandas as pd\\n\\ndef test_center_series():\\n    # create a simple series\\n    series = pd.Series([1, 2, 3, 4, 5])\\n    # call the function and get the result\\n    result = center_series(series)\\n    # assert that the result is as expected\\n    assert result[1].mean() == 0',\n",
       "  100.0),\n",
       " ('def l2_regularization(W, reg_strength):\\n    \\n    loss = reg_strength * (W ** 2).sum()\\n    grad = 2 * reg_strength * W\\n    return loss, grad',\n",
       "  '# test_l2_regularization.py\\n\\nimport sys\\nsys.path.append(\".\")\\nfrom source import l2_regularization\\nimport pytest\\n\\ndef test_l2_regularization():\\n    W = [1, 2, 3, 4]\\n    reg_strength = 0.1\\n    loss, grad = l2_regularization(W, reg_strength)\\n    assert loss == pytest.approx(0.2025404), \"The loss value is not correct\"\\n    assert grad == pytest.approx([0.2, 0.4, 0.6, 0.8]), \"The gradient is not correct\"',\n",
       "  50.0),\n",
       " ('import numpy\\n\\ndef unit_vectors(direction):\\n    \\n    directions_rad = numpy.deg2rad(direction)\\n    UnitX = -numpy.sin(0.5 * numpy.pi) * numpy.cos(directions_rad)\\n    UnitY = numpy.sin(0.5 * numpy.pi) * numpy.sin(directions_rad)\\n\\n    UnitX[numpy.isclose(direction, -1)] = 0\\n    UnitY[numpy.isclose(direction, -1)] = 0\\n\\n    return UnitX, UnitY',\n",
       "  'import numpy as np\\nimport pytest\\n\\nfrom source import unit_vectors\\n\\ndef test_unit_vectors():\\n    direction = np.array([0, 30, -1, 180])\\n    expected_x = np.array([0, 0.5, 0, -0.5])\\n    expected_y = np.array([0.8660254037844386, 0.5, 0, -0.5])\\n\\n    assert np.array_equal(unit_vectors(direction), (expected_x, expected_y)), \\'Expected and actual unit vectors do not match\\'\\n\\nif __name__ == \"__main__\":\\n    test_unit_vectors()',\n",
       "  100.0),\n",
       " ('def extended_euclidean_algorithm(a, b):\\n    \\n\\n    r1 = a\\n    x1 = 1\\n    y1 = 0\\n\\n    r2 = b\\n    x2 = 0\\n    y2 = 1\\n\\n    while r2 != 0:\\n        # r1 = qr2 + r3\\n        r3 = r1 % r2\\n        q = r1 // r2\\n\\n        x3 = x1 - q * x2\\n        y3 = y1 - q * y2\\n\\n        r1 = r2\\n        r2 = r3\\n\\n        x1 = x2\\n        y1 = y2\\n        x2 = x3\\n        y2 = y3\\n\\n    return r1, x1, y1',\n",
       "  '# -*- coding: utf-8 -*-\\nimport pytest\\nimport sys\\nsys.path.append(\"..\") # to import source.py\\nfrom source import extended_euclidean_algorithm\\n\\ndef test_extended_euclidean_algorithm():\\n    # Test with known inputs\\n    assert extended_euclidean_algorithm(35, 15) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(101, 103) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(43, 19) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(121, 5) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(27, 81) == (1, 1, 0)\\n    assert extended_euclidean_algorithm(4, 5) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(122, 10) == (0, 1, -1)\\n    assert extended_euclidean_algorithm(10, 10) == (0, 1, -1)\\n    assert extended_euclidean_algorithm(1, 1) == (0, 1, -1)\\n    assert extended_euclidean_algorithm(7, 3) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(3, 3) == (0, 1, -1)\\n    assert extended_euclidean_algorithm(11, 7) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(14, 4) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(3, 11) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(2, 1) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(6, 9) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(13, 9) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(20, 14) == (0, 1, -1)\\n    assert extended_euclidean_algorithm(7, 4) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(12, 5) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(18, 9) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(18, 8) == (0, 1, -1)\\n    assert extended_euclidean_algorithm(21, 14) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(3, 5) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(6, 8) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(2, 3) == (0, 1, -1)\\n    assert extended_euclidean_algorithm(3, 2) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(29, 17) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(15, 9) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(7, 2) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(10, 6) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(17, 12) == (0, 1, -1)\\n    assert extended_euclidean_algorithm(5, 4) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(16, 8) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(9, 6) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(25, 12) == (0, 1, -1)\\n    assert extended_euclidean_algorithm(4, 3) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(3, 1) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(5, 2) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(19, 6) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(13, 6) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(7, 1) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(19, 4) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(23, 9) == (0, 1, -1)\\n    assert extended_euclidean_algorithm(10, 5) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(12, 3) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(15, 2) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(8, 5) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(16, 4) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(7, 1) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(13, 3) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(11, 2) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(6, 1) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(5, 1) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(4, 1) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(3, 1) == (0, 1, -1)\\n    assert extended_euclidean_algorithm(2, 1) == (1, -1, 1)\\n    assert extended_euclidean_algorithm(1, 1) == (0, 1, -1)\\n    assert extended_euclidean_algorithm(1, 0) == (1, 0, 0)\\n    assert extended_euclidean_algorithm(0, 0) == (0, 0, 0)\\n    \\n\\nif __name__ == \"__main__\":\\n    test_extended_euclidean_algorithm()',\n",
       "  100.0),\n",
       " ('def areaBetween(requestContext, seriesList):\\n  \\n  assert len(seriesList) == 2, \"areaBetween series argument must reference *exactly* 2 series\"\\n  lower = seriesList[0]\\n  upper = seriesList[1]\\n\\n  lower.options[\\'stacked\\'] = True\\n  lower.options[\\'invisible\\'] = True\\n\\n  upper.options[\\'stacked\\'] = True\\n  lower.name = upper.name = \"areaBetween(%s)\" % upper.pathExpression\\n  return seriesList',\n",
       "  'import pytest\\nfrom source import areaBetween\\n\\nclass TestAreaBetween:\\n\\n    def test_areaBetween(self):\\n        seriesList = [1, 2, 3, 4, 5]\\n        with pytest.raises(AssertionError):\\n            areaBetween(None, seriesList)',\n",
       "  22.0),\n",
       " ('def reward_CACC_paper(state, opts):\\n    \\n    Hw, dHw = state[\"Hw\"], state[\"dHw\"]\\n    desired_Hw = opts.desired_headway\\n    assert 0 <= Hw <= 10.1, f\"ERROR: Headway value of {Hw} is invalid in reward function!\"\\n\\n    if Hw < desired_Hw * 0.25:\\n        reward = -100\\n    elif desired_Hw * 0.25 <= Hw < desired_Hw * 0.875:\\n        reward = -5 if dHw < 0 else 1\\n    elif desired_Hw * 0.875 <= Hw < desired_Hw * 0.95:\\n        reward = 5\\n    elif desired_Hw * 0.95 <= Hw < desired_Hw * 1.05:\\n        reward = 50\\n    elif desired_Hw * 1.05 <= Hw < desired_Hw * 1.125:\\n        reward = 5\\n    elif desired_Hw * 1.125 <= Hw < 10:\\n        reward = 1 if dHw < 0 else -1\\n    elif 10 <= Hw:\\n        reward = 0.5 if dHw < 0 else -100\\n\\n    # Cost for acceleration and jerk to favour comfortable driving.\\n    reward -= opts.reward_control_weight * state[\"a_dem\"]**2\\n    # Note: We do not divide by env_dt, consequently the time factor needs to be\\n    # considered in reward_control_jerk_weight\\n    reward -= opts.reward_control_jerk_weight * (state[\"a_dem\"] - state[\"last_a_dem\"])**2\\n\\n    return reward',\n",
       "  'import pytest\\nfrom source import reward_CACC_paper\\n\\nclass TestRewardFunction:\\n\\n    def test_reward_CACC_paper(self):\\n        state = {\"Hw\": 3, \"dHw\": 2, \"a_dem\": 1, \"last_a_dem\": 0}\\n        opts = type(\\'\\', {}, {\"desired_headway\": 5, \"reward_control_weight\": 1, \"reward_control_jerk_weight\": 1})()\\n        assert reward_CACC_paper(state, opts) == -1, \\'Test 1 Failed\\'\\n\\n        state = {\"Hw\": 7, \"dHw\": -1.5, \"a_dem\": 0.1, \"last_a_dem\": -0.2}\\n        opts = type(\\'\\', {}, {\"desired_headway\": 5, \"reward_control_weight\": 1, \"reward_control_jerk_weight\": 1})()\\n        assert reward_CACC_paper(state, opts) == 5, \\'Test 2 Failed\\'\\n\\n        state = {\"Hw\": 9.5, \"dHw\": -0.03, \"a_dem\": 0.05, \"last_a_dem\": 0.02}\\n        opts = type(\\'\\', {}, {\"desired_headway\": 5.5, \"reward_control_weight\": 1, \"reward_control_jerk_weight\": 1})()\\n        assert reward_CACC_paper(state, opts) == 50, \\'Test 3 Failed\\'\\n\\n        state = {\"Hw\": 10.01, \"dHw\": 0.01, \"a_dem\": -0.02, \"last_a_dem\": 0.01}\\n        opts = type(\\'\\', {}, {\"desired_headway\": 5.5, \"reward_control_weight\": 1, \"reward_control_jerk_weight\": 1})()\\n        assert reward_CACC_paper(state, opts) == 0.5, \\'Test 4 Failed\\'',\n",
       "  5.0),\n",
       " ('def accuracy(p2p, gt_p2p, D1_geod, return_all=False, sqrt_area=None):\\n    \\n\\n    dists = D1_geod[(p2p,gt_p2p)]\\n    if sqrt_area is not None:\\n        dists /= sqrt_area\\n\\n    if return_all:\\n        return dists.mean(), dists\\n\\n    return dists.mean()',\n",
       "  \"import pytest\\nfrom source import accuracy\\nimport numpy as np\\n\\nclass TestAccuracy:\\n    def test_accuracy(self):\\n        p2p = 'point1'\\n        gt_p2p = 'point2'\\n        D1_geod = {'point1': np.array([1,2,3]), 'point2': np.array([4,5,6])}\\n        sqrt_area = np.sqrt(10)\\n        assert np.isclose(accuracy(p2p, gt_p2p, D1_geod, sqrt_area=sqrt_area), np.linalg.norm(D1_geod[(p2p,gt_p2p)]/sqrt_area)/2)\",\n",
       "  29.0),\n",
       " ('def is_frozen(H):\\n    \\n    try:\\n        return H.frozen\\n    except AttributeError:\\n        return False',\n",
       "  'import pytest\\n\\ndef test_is_frozen():\\n    from source import H\\n    assert is_frozen(H)',\n",
       "  20.0),\n",
       " ('def A_approx(Q_feed, deltaT_diff, Kt_approx):\\n               \\n    return Q_feed / (deltaT_diff * Kt_approx)',\n",
       "  'import sys\\nsys.path.append(\".\") # this is to import source.py from the same directory\\nimport source \\n\\ndef test_A_approx():\\n    assert source.A_approx(1, 2, 3) == 0.5',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef g_least_squares(d_of_fake: torch.Tensor, invert_labels: bool = False):\\n    r\\n    assert isinstance(d_of_fake, torch.Tensor)\\n    assert isinstance(invert_labels, bool)\\n    if invert_labels:\\n        return d_of_fake.sigmoid().pow(2).mean()\\n    return (1 - d_of_fake.sigmoid()).pow(2).mean()',\n",
       "  'import torch\\nimport pytest\\n\\nfrom source import g_least_squares\\n\\ndef test_g_least_squares():\\n    # creating a random tensor\\n    tensor_input = torch.randn(100, 100)\\n\\n    # testing with invert_labels False\\n    output_invert_false = g_least_squares(tensor_input, invert_labels=False)\\n    assert isinstance(output_invert_false, torch.Tensor), \"output is not a tensor when invert_labels is False\"\\n\\n    # testing with invert_labels True\\n    output_invert_true = g_least_squares(tensor_input, invert_labels=True)\\n    assert isinstance(output_invert_true, torch.Tensor), \"output is not a tensor when invert_labels is True\"',\n",
       "  38.0),\n",
       " ('import numpy\\n\\ndef axes2points(x, y):\\n    \\n\\n    # Reverse y coordinates to have them start at bottom of array\\n    y = numpy.flipud(y)\\n\\n    # Repeat x coordinates for each y (fastest varying)\\n    X = numpy.kron(numpy.ones(len(y)), x)\\n\\n    # Repeat y coordinates for each x (slowest varying)\\n    Y = numpy.kron(y, numpy.ones(len(x)))\\n\\n    # Check\\n    N = len(X)\\n    assert len(Y) == N\\n\\n    # Create Nx2 array of x and y coordinates\\n    X = numpy.reshape(X, (N, 1))\\n    Y = numpy.reshape(Y, (N, 1))\\n    P = numpy.concatenate((X, Y), axis=1)\\n\\n    # Return\\n    return P',\n",
       "  'import numpy\\nimport source  # Assuming the source code is in a file named \\'source.py\\'\\n\\ndef test_axes2points():\\n    x = numpy.array([1, 2, 3])\\n    y = numpy.array([4, 5, 6])\\n\\n    # Test case 1\\n    P = source.axes2points(x, y)\\n    assert P.shape == (6, 2), \"Test case 1 failed: The output shape is not (6, 2)\"\\n\\n    # Test case 2\\n    x = numpy.array([1, 2])\\n    y = numpy.array([4, 5, 6])\\n    P = source.axes2points(x, y)\\n    assert P.shape == (3, 2), \"Test case 2 failed: The output shape is not (3, 2)\"\\n\\n    # Test case 3\\n    x = numpy.array([1, 2, 3])\\n    y = numpy.array([4])\\n    P = source.axes2points(x, y)\\n    assert P.shape == (1, 2), \"Test case 3 failed: The output shape is not (1, 2)\"\\n\\n    # Test case 4\\n    x = numpy.array([1])\\n    y = numpy.array([4, 5, 6])\\n    P = source.axes2points(x, y)\\n    assert P.shape == (3, 2), \"Test case 4 failed: The output shape is not (3, 2)\"',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef _format_faces_indices(faces_indices, max_index):\\n    \\n    faces_indices = torch.tensor(faces_indices, dtype=torch.int64)\\n\\n    # Change to 0 based indexing.\\n    faces_indices[(faces_indices > 0)] -= 1\\n\\n    # Negative indexing counts from the end.\\n    faces_indices[(faces_indices < 0)] += max_index\\n\\n    # Check indices are valid.\\n    if not (\\n        torch.all(faces_indices < max_index) and torch.all(faces_indices >= 0)\\n    ):\\n        raise ValueError(\"Faces have invalid indices.\")\\n\\n    return faces_indices',\n",
       "  '# test_source.py\\nimport pytest\\nimport torch\\nfrom source import _format_faces_indices\\n\\ndef test_format_faces_indices():\\n    faces_indices = [1, -2, 3, 0, -4]\\n    max_index = 4\\n    expected = torch.tensor([0, 1, 2, 0, 3], dtype=torch.int64)\\n    result = _format_faces_indices(faces_indices, max_index)\\n    assert torch.all(result == expected), \"The function did not return the expected result.\"\\n\\nif __name__ == \"__main__\":\\n    test_format_faces_indices()',\n",
       "  88.0),\n",
       " ('def round_filters(filters, width_coefficient, depth_divisor, min_depth):\\n    \\n    if not width_coefficient:\\n        return filters\\n    min_depth = min_depth\\n    filters *= width_coefficient\\n    min_depth = min_depth or depth_divisor  # pay attention to this line when using min_depth\\n    # follow the formula transferred from official TensorFlow implementation\\n    new_filters = max(min_depth, int(filters + depth_divisor / 2) // depth_divisor * depth_divisor)\\n    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\\n        new_filters += depth_divisor\\n    return int(new_filters)',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import round_filters\\n\\ndef test_round_filters():\\n    # Test with normal positive integers\\n    assert round_filters(56, 2, 8, 10) == 64\\n    # Test with zero width_coefficient\\n    assert round_filters(56, 0, 8, 10) == 56\\n    # Test with depth_divisor equal to min_depth\\n    assert round_filters(56, 2, 10, 10) == 64\\n    # Test with depth_divisor greater than min_depth\\n    assert round_filters(56, 2, 12, 10) == 72\\n    # Test with depth_divisor less than min_depth\\n    assert round_filters(56, 2, 8, 12) == 96\\n    # Test with depth_divisor and min_depth equals to zero\\n    assert round_filters(56, 2, 0, 0) == 56',\n",
       "  80.0),\n",
       " ('import torch\\n\\ndef axisangle_between_vec(vec1, vec2, eps=1e-6):\\n    \\n    # type: (Tensor, Tensor, float) -> Tensor\\n    where_small = torch.abs(torch.sum(vec2 - vec1, dim=-1)) < eps\\n    # Normalize vectors\\n    vec1 = vec1 / torch.norm(vec1, dim=-1, keepdim=True)\\n    vec2 = vec2 / torch.norm(vec2, dim=-1, keepdim=True)\\n    # Calculate angle and axis\\n    angle = torch.acos(torch.sum(vec1 * vec2, dim=-1, keepdim=True))\\n    axis = torch.cross(vec1, vec2)\\n    axis_norm = torch.norm(axis, dim=-1)\\n    where_small = where_small | (axis_norm < eps)\\n    axis[where_small] = 0.0\\n    axis[~where_small] = axis[~where_small] / axis_norm[~where_small].unsqueeze(-1)\\n    # Combine and return value\\n    return angle * axis',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('..')\\nfrom source import axisangle_between_vec\\n\\ndef test_axisangle_between_vec():\\n    vec1 = torch.Tensor([1, 0, 0])\\n    vec2 = torch.Tensor([0, 1, 0])\\n    result = axisangle_between_vec(vec1, vec2)\\n    assert torch.allclose(result[:, 0], torch.Tensor([1.5707963267948966]))\\n    assert torch.allclose(result[:, 1:], torch.zeros_like(result[:, 1:]))\\n    assert torch.allclose(result[0, :], torch.Tensor([0.0, 0.0, 0.0]))\",\n",
       "  17.0),\n",
       " ('import torch\\n\\ndef my_listperm2matperm(listperm):\\n    \\n    n_objects = listperm.size()[1]\\n    eye = torch.eye(n_objects, dtype=torch.int, device=listperm.device)[listperm]\\n    # eye= torch.tensor(eye, dtype=torch.int32)\\n    return eye',\n",
       "  '# test_source.py\\nimport pytest\\nimport torch\\nfrom source import my_listperm2matperm\\n\\ndef test_my_listperm2matperm():\\n    # Create a random listperm\\n    listperm = torch.randint(0, 10, (3, 4))\\n    \\n    # Get the resulting matrix from the function\\n    result = my_listperm2matperm(listperm)\\n    \\n    # Check that the resulting matrix has ones at the indices specified by listperm\\n    assert torch.allclose(result[listperm[:, 0], listperm[:, 1]], 1)\\n    assert torch.allclose(result[listperm[:, 2], listperm[:, 3]], 1)\\n    assert torch.allclose(result[listperm[:, 0], listperm[:, 3]], 0)\\n    assert torch.allclose(result[listperm[:, 2], listperm[:, 1]], 0)\\n    assert torch.allclose(result[listperm[:, 1], listperm[:, 2]], 0)\\n    assert torch.allclose(result[listperm[:, 3], listperm[:, 0]], 0)',\n",
       "  80.0),\n",
       " (\"def update_portfolio(returns, withdrawal, stock_allocation, balance):\\n    \\n    return (\\n        # stocks plus appreciation and dividends\\n        balance * stock_allocation * (1 + returns['stocks'])\\n        # bonds plus bond income\\n        + balance * (1 - stock_allocation) * (1 + returns['bonds'])\\n        # end of year withdrawal\\n        - withdrawal\\n    )\",\n",
       "  \"# test_source.py \\n\\nfrom source import update_portfolio  # import the function from source.py\\n\\ndef test_update_portfolio():\\n    returns = {'stocks': 0.05, 'bonds': 0.03}  # sample return values\\n    withdrawal = 1000  # sample withdrawal value\\n    stock_allocation = 0.5  # sample stock allocation\\n    balance = 10000  # sample balance\\n\\n    # execute the function and make a test assertion\\n    assert update_portfolio(returns, withdrawal, stock_allocation, balance) == 1083.99999999999999\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef run_mat_interp_back(kdat, coef_mat_real, coef_mat_imag, griddat):\\n    \\n    real_kdat = kdat[:, 0, :].t().reshape(-1, kdat.shape[0])\\n    imag_kdat = kdat[:, 1, :].t().reshape(-1, kdat.shape[0])\\n    coef_mat_real_t = coef_mat_real.t()\\n    coef_mat_imag_t = coef_mat_imag.t()\\n\\n    # apply multiplies with complex conjugate\\n    griddat[:, 0, :] = torch.mm(\\n        coef_mat_real_t,\\n        real_kdat\\n    ).t() + torch.mm(\\n        coef_mat_imag_t,\\n        imag_kdat\\n    ).t()\\n    griddat[:, 1, :] = torch.mm(\\n        coef_mat_real_t,\\n        imag_kdat\\n    ).t() - torch.mm(\\n        coef_mat_imag_t,\\n        real_kdat\\n    ).t()\\n\\n    return griddat',\n",
       "  'import torch\\nimport pytest\\nimport os\\n\\nFILE_PATH = \\'source.py\\'\\n\\ndef test_run_mat_interp_back():\\n    if not os.path.exists(FILE_PATH):\\n        pytest.skip(\"Source file does not exist\")\\n\\n    from source import run_mat_interp_back  # import the function from source.py\\n\\n    # Create dummy data\\n    kdat = torch.rand((10,2,10))\\n    coef_mat_real = torch.rand((10,10))\\n    coef_mat_imag = torch.rand((10,10))\\n    griddat = torch.rand((10,2,10))\\n\\n    # Call the function with the dummy data\\n    result = run_mat_interp_back(kdat, coef_mat_real, coef_mat_imag, griddat)\\n\\n    # Add your assertions here\\n    assert result.shape == griddat.shape, \"The shape of the result does not match the expected result\"',\n",
       "  100.0),\n",
       " ('def split_into_chunks(batch, chunk_sizes):\\n  \\n  assert sum(chunk_sizes) == len(batch)\\n  flush_chunk_size, support_chunk_size, _ = chunk_sizes\\n  query_start = flush_chunk_size + support_chunk_size\\n\\n  flush_chunk = batch[:flush_chunk_size]\\n  support_chunk = batch[flush_chunk_size:query_start]\\n  query_chunk = batch[query_start:]\\n\\n  return (flush_chunk, support_chunk, query_chunk)',\n",
       "  'import os\\nimport pytest\\nfrom source import split_into_chunks\\n\\ndef test_split_into_chunks():\\n  batch = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\n  chunk_sizes = [3, 2, 5]\\n  result = split_into_chunks(batch, chunk_sizes)\\n  assert result[0] == [1, 2, 3]\\n  assert result[1] == [4, 5]\\n  assert result[2] == [6, 7, 8, 9, 10]',\n",
       "  100.0),\n",
       " ('def prop_get_refradius(wf):\\n    \\n    return wf.z - wf.z_w0',\n",
       "  '# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append(\".\") # to import source.py from the same directory\\nfrom source import Workflow  # replace Workflow with the actual class name\\n\\nclass TestSource:\\n\\n    def setup_method(self):\\n        # setup any common settings here\\n        self.wf = Workflow()  # replace Workflow with the actual class name\\n\\n    def test_get_refradius(self):\\n        # setup any specific settings for this test\\n        # assuming z and z_w0 are attributes of the Workflow class\\n        self.wf.z = 10\\n        self.wf.z_w0 = 5\\n\\n        # make one assertion per test\\n        assert self.wf.prop_get_refradius() == 5',\n",
       "  50.0),\n",
       " ('import numpy\\n\\ndef vec2ang(vector):\\n    \\n    intensity = numpy.linalg.norm(vector)\\n    r2d = 180. / numpy.pi\\n    x, y, z = vector\\n    declination = r2d * numpy.arctan2(y, x)\\n    inclination = r2d * numpy.arcsin(z / intensity)\\n    return [intensity, inclination, declination]',\n",
       "  'import pytest\\nimport numpy\\n\\ndef test_vec2ang():\\n    assert vec2ang([1, 1, 1]) == [3.7416573867739413, 45.0, 45.0]\\n    assert vec2ang([0, 0, 1]) == [1.0, 0.0, 0.0]\\n    assert vec2ang([0, 1, 0]) == [1.0, 90.0, 0.0]\\n    assert vec2ang([-1, -1, -1]) == [3.7416573867739413, -45.0, -45.0]',\n",
       "  0.0),\n",
       " ('def scale(x, a, b, c, d):\\n    \\n    return (x - a) / (b - a) * (d - c) + c',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\"..\") # Adds the directory above to the path to import source.py\\n\\nfrom source import scale\\n\\ndef test_scale():\\n    assert scale(2, 1, 3, 2, 4) == 3',\n",
       "  100.0),\n",
       " (\"def format_time(time):\\n    \\n    mins, seconds = divmod(int(time), 60)\\n    hours, minutes = divmod(mins, 60)\\n\\n    if hours > 0:\\n        return '{:0>2d}:{:0>2d}:{:0>2d}'.format(hours, minutes, seconds)\\n\\n    elif minutes > 0:\\n        return '{:0>2d}:{:0>2d}'.format(minutes, seconds)\\n\\n    elif seconds > 0:\\n        return '{:.0f} sec'.format(seconds)\\n\\n    elif time >= 1e-3:\\n        return '{:.0f} ms'.format(time * 1e3)\\n\\n    elif time >= 1e-6:\\n        # microseconds\\n        return '{:.0f} μs'.format(time * 1e6)\\n\\n    # nanoseconds or smaller\\n    return '{:.0f} ns'.format(time * 1e9)\",\n",
       "  \"# test_source.py\\nimport pytest\\nimport source  # assuming the source code is in a file named source.py in the same directory\\n\\ndef test_format_time():\\n    assert source.format_time(3661) == '01:01:01'\\n    assert source.format_time(3600) == '01:00:00'\\n    assert source.format_time(61) == '00:01:01'\\n    assert source.format_time(1) == '00:00:01'\\n    assert source.format_time(1e-3) == '00:00:00.000'\\n    assert source.format_time(1e-6) == '00:00:00.000'\\n    assert source.format_time(1e-9) == '00:00:00.000'\",\n",
       "  50.0),\n",
       " ('def upperApproximation(df, concept):\\n    \\n    attribute_colnames = df.columns[:-1]\\n    decision_colname = df.columns[-1]\\n    original_decision = df[decision_colname].copy()\\n    # Find cases for which decisions to be replaced with special\\n    subset_lower_approximation_na = df.groupby(list(attribute_colnames)).filter(lambda x:  (x[decision_colname] == concept).any(), dropna = False)\\n    subset_lower_approximation_na = subset_lower_approximation_na.apply(lambda x: x.isna().any(), axis = 1)\\n    # Replace uncertain cases and cases not matching to concet with \"SPECIAL_DECISION\"\\n    df.loc[subset_lower_approximation_na, decision_colname] = \"SPECIAL_DECISION\"\\n    df.loc[~subset_lower_approximation_na, decision_colname] = concept\\n    return (df, original_decision)',\n",
       "  'import sys\\nsys.path.append(\".\")\\nimport source \\nimport pytest\\n\\ndef test_upperApproximation():\\n    # Arrange\\n    df = pd.DataFrame({\"A\": [1, 2, 3, 4, 5], \"B\": [6, 7, 8, 9, 10], \"C\": [11, 12, 13, 14, 15], \"D\": [16, \"SPECIAL_DECISION\", 17, 18, 19]})\\n    concept = \"CONCET\"\\n    \\n    # Act\\n    result, original_decision = source.upperApproximation(df, concept)\\n\\n    # Assert\\n    assert not result.isna().any().any(), \"Replacement result contains NaN values\"\\n    assert result[\"D\"].to_list() == [\"CONCET\", \"CONCET\", \"CONCET\", \"CONCET\", \"CONCET\"], \"Decision not correctly replaced\"\\n    assert original_decision.to_list() == [\"CONCET\", \"SPECIAL_DECISION\", \"CONCET\", \"CONCET\", \"CONCET\"], \"Original decision not correctly replaced\"',\n",
       "  11.0),\n",
       " ('import torch\\n\\ndef convert_corner_to_pyplot_repr(box_labels):\\n    \\n    x = (box_labels[:,0]).reshape(-1,1)\\n    y = (box_labels[:,1]).reshape(-1,1)\\n    height = (box_labels[:,2] - box_labels[:,0]).reshape(-1, 1)\\n    width = (box_labels[:,3] - box_labels[:,1]).reshape(-1, 1)\\n\\n    return torch.cat((x, y, height, width), axis = 1)',\n",
       "  \"from source import *\\nimport pytest\\nimport torch\\nimport numpy as np\\nimport os\\ncurrent_dir = os.path.dirname(os.path.abspath(__file__))\\nsys.path.insert(0, current_dir + '/..')\\nfrom source import convert_corner_to_pyplot_repr\\n\\ndef test_convert_corner_to_pyplot_repr():\\n    box_labels = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])\\nexpected_output = torch.tensor([[1, 2, 2, 1], [5, 6, 6, 5]])\\n    result = convert_corner_to_pyplot_repr(box_labels)\\nassert torch.allclose(result, expected_output)\",\n",
       "  0.0),\n",
       " ('def forward_prop_affine_transform(a_prev, weight, bias):\\n    \\n\\n    return a_prev.dot(weight) + bias',\n",
       "  '# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append(\"..\") # This is to append the parent directory in order to import source.py\\nfrom source import forward_prop_affine_transform\\n\\ndef test_forward_prop_affine_transform():\\n    a_prev = [[1, 2, 3], [4, 5, 6]]\\n    weight = [[7, 8, 9], [10, 11, 12]]\\n    bias = [13, 14, 15]\\n\\n    result = forward_prop_affine_transform(a_prev, weight, bias)\\n\\n    assert result == [[58, 64, 70], [139, 154, 169]]',\n",
       "  100.0),\n",
       " ('def is_force_boundary(x):\\n    \\n    # Particle does not live on a boundary\\n    bnd = [None, None, None]\\n    if x[0] > 1.55:\\n        # Force loaded in negative direction on the right hand side\\n        bnd[2] = -1\\n    return bnd',\n",
       "  \"import sys\\nsys.path.insert(0, '..') # This will add the parent directory to the path, allowing the import of the 'source' module\\n\\nimport pytest\\nfrom source import is_force_boundary\\n\\ndef test_is_force_boundary():\\n    assert is_force_boundary([1.56, 0, 0]) == [-1]\",\n",
       "  100.0),\n",
       " ('def orthogonal_vector(vector):\\n    \\n    return -1 * vector[1], vector[0]',\n",
       "  \"import sys\\nsys.path.append('.')\\nfrom source import orthogonal_vector\\n\\ndef test_orthogonal_vector():\\n    vector = (1, 2)\\n    expected_result = (-2, 1)\\n    assert orthogonal_vector(vector) == expected_result\",\n",
       "  100.0),\n",
       " ('import numpy\\n\\ndef vec2ang(vector):\\n    \\n    intensity = numpy.linalg.norm(vector)\\n    r2d = 180. / numpy.pi\\n    x, y, z = vector\\n    declination = r2d * numpy.arctan2(y, x)\\n    inclination = r2d * numpy.arcsin(z / intensity)\\n    return [intensity, inclination, declination]',\n",
       "  'import pytest\\nimport numpy\\nfrom source import vec2ang  # assuming the function is in source.py\\n\\nclass TestVec2Ang:\\n\\n    def test_output_type(self):\\n        vector = [1, 2, 3]\\n        result = vec2ang(vector)\\n        assert isinstance(result, list), \"Should return a list\"\\n\\n    def test_first_value(self):\\n        vector = [1, 0, 0]\\n        result = vec2ang(vector)\\n        assert numpy.isclose(result[0], numpy.linalg.norm(vector)), \"Incorrect intensity\"\\n\\n    def test_second_value(self):\\n        vector = [0, 1, 0]\\n        result = vec2ang(vector)\\n        assert numpy.isclose(result[1], numpy.arctan2(vector[1], vector[0])), \"Incorrect inclination\"\\n\\n    def test_third_value(self):\\n        vector = [0, 0, 1]\\n        result = vec2ang(vector)\\n        assert numpy.isclose(result[2], numpy.arcsin(vector[2] / numpy.linalg.norm(vector))), \"Incorrect declination\"\\n\\n    def test_input_type(self):\\n        with pytest.raises(TypeError):\\n            vec2ang(\"not a list\")\\n\\n    def test_input_length(self):\\n        with pytest.raises(ValueError):\\n            vec2ang([1,2])',\n",
       "  100.0),\n",
       " ('def A_approx(Q_feed, deltaT_diff, Kt_approx):\\n               \\n    return Q_feed / (deltaT_diff * Kt_approx)',\n",
       "  'import pytest\\nfrom source import A_approx  # replace with actual import statement\\n\\ndef test_A_approx():\\n    assert A_approx(100, 2, 3) == 50  # replace with your assert statement',\n",
       "  100.0),\n",
       " ('def ms(applied, allowed, knockdown=1):\\n    r\\n    return (applied / (knockdown * allowed)) - 1',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')\\nimport source\\n\\ndef test_ms():\\n    assert source.ms(5, 3) == -0.5\",\n",
       "  67.0),\n",
       " ('def cross_entropy(y_pred, y_true):\\n    \\n    return (-y_true * y_pred.log_softmax(dim=1)).sum(dim=1).mean()',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\"./\")  # This line is to append the directory of source.py to the sys path \\nfrom source import cross_entropy\\n\\ndef test_cross_entropy():\\n    y_pred = ...  # Insert your test values for y_pred (e.g., torch tensor)\\n    y_true = ...  # Insert your test values for y_true (e.g., torch tensor)\\n    expected_result = ...  # Insert the expected result\\n    assert torch.isclose(cross_entropy(y_pred, y_true), expected_result)  # Check if the result is close to the expected result',\n",
       "  50.0),\n",
       " (\"def get_default_opts(cyclic=False):\\n    \\n    return {\\n        'default_sweep_sequence': 'R',\\n        'bond_compress_method': 'svd',\\n        'bond_compress_cutoff_mode': 'rel' if cyclic else 'sum2',\\n        'bond_expand_rand_strength': 1e-6,\\n        'local_eig_tol': 1e-3,\\n        'local_eig_ncv': 4,\\n        'local_eig_backend': None,\\n        'local_eig_maxiter': None,\\n        'local_eig_EPSType': None,\\n        'local_eig_ham_dense': None,\\n        'local_eig_norm_dense': None,\\n        'periodic_segment_size': 1 / 2,\\n        'periodic_compress_method': 'isvd',\\n        'periodic_compress_norm_eps': 1e-6,\\n        'periodic_compress_ham_eps': 1e-6,\\n        'periodic_compress_max_bond': -1,\\n        'periodic_nullspace_fudge_factor': 1e-12,\\n        'periodic_canonize_inv_tol': 1e-10,\\n        'periodic_orthog_tol': 1e-6,\\n        'is_hermitian': True,\\n    }\",\n",
       "  \"import pytest\\nfrom source import get_default_opts\\n\\ndef test_get_default_opts():\\n    # Testing default values\\n    opts = get_default_opts()\\n    \\n    assert opts == {\\n        'default_sweep_sequence': 'R',\\n        'bond_compress_method': 'svd',\\n        'bond_compress_cutoff_mode': 'rel',\\n        'bond_expand_rand_strength': 1e-6,\\n        'local_eig_tol': 1e-3,\\n        'local_eig_ncv': 4,\\n        'local_eig_backend': None,\\n        'local_eig_maxiter': None,\\n        'local_eig_EPSType': None,\\n        'local_eig_ham_dense': None,\\n        'local_eig_norm_dense': None,\\n        'periodic_segment_size': 1 / 2,\\n        'periodic_compress_method': 'isvd',\\n        'periodic_compress_norm_eps': 1e-6,\\n        'periodic_compress_ham_eps': 1e-6,\\n        'periodic_compress_max_bond': -1,\\n        'periodic_nullspace_fudge_factor': 1e-12,\\n        'periodic_canonize_inv_tol': 1e-10,\\n        'periodic_orthog_tol': 1e-6,\\n        'is_hermitian': True,\\n    }\",\n",
       "  100.0),\n",
       " ('def dist_2(x0, y0, x1, y1):\\n    r\\n    d0 = x1 - x0\\n    d1 = y1 - y0\\n    return d0 * d0 + d1 * d1',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')\\nimport source\\n\\ndef test_dist_2():\\n    with pytest.raises(RuntimeError):\\n        with pytest.raises(UnboundLocalError):\\n            assert source.dist_2(0, 0, 1, 1) == 2.0\",\n",
       "  40.0),\n",
       " ('import torch\\n\\ndef normalize3d(tensor, mean, std, inplace=False):\\n    \\n    if not inplace:\\n        tensor = tensor.clone()\\n\\n    mean = torch.as_tensor(mean, dtype=torch.float32, device=tensor.device)\\n    std = torch.as_tensor(std, dtype=torch.float32, device=tensor.device)\\n    tensor.sub_(mean[:, None, None, None]).div_(std[:, None, None, None])\\n    return tensor',\n",
       "  'import torch\\nimport source  # assuming source.py is in the same directory\\n\\ndef test_normalize3d():\\n    tensor = torch.tensor([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\\n    mean = [2, 3, 4]\\n    std = [2, 2, 2]\\n    expected_output = torch.tensor([[[-1, -1, -1], [1, 1, 1]], [[-1, -1, -1], [1, 1, 1]]])\\n    \\n    assert torch.allclose(source.normalize3d(tensor, mean, std), expected_output)',\n",
       "  88.0),\n",
       " ('def calculate_rewards_common(covariates, coeffs_common):\\n    \\n    return covariates[:, 9:11].dot(coeffs_common).reshape(-1, 1)',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import calculate_rewards_common\\n\\ndef test_calculate_rewards_common_function():\\n    covariates = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]])\\n    coeffs_common = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])\\n    result = calculate_rewards_common(covariates, coeffs_common)\\n    expected_result = np.array([146, 163])\\n    assert np.array_equal(result, expected_result), \"Test failed!\"',\n",
       "  100.0),\n",
       " ('def in_sector_bounding_box(polar_point, radius, start_theta, end_theta):\\n    \\n    point_radius, point_theta = polar_point\\n\\n    if start_theta > end_theta:\\n        return (point_radius < radius and (point_theta >= start_theta or\\n                                           point_theta < end_theta))\\n    else:\\n        return (point_radius < radius and point_theta >= start_theta and\\n                point_theta < end_theta)',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\nfrom source import in_sector_bounding_box\\n\\ndef test_in_sector_bounding_box():\\n    # Test case 1: Point is within the sector.\\n    assert in_sector_bounding_box((1, 2), 3, 1, 4) == True\\n    # Test case 2: Point is outside the sector but within the radius.\\n    assert in_sector_bounding_box((2, 2), 3, 1, 4) == False\\n    # Test case 3: Point is outside the sector and outside the radius.\\n    assert in_sector_bounding_box((4, 2), 3, 1, 4) == False\\n    # Test case 4: Point is on the boundary of the sector but outside the radius.\\n    assert in_sector_bounding_box((3, 2), 3, 1, 4) == False\\n    # Test case 5: Point is on the boundary of the radius but outside the sector.\\n    assert in_sector_bounding_box((2, 1), 3, 1, 4) == False\\n    # Test case 6: Point is on the boundary of the sector and radius.\\n    assert in_sector_bounding_box((3, 3), 3, 1, 4) == True\",\n",
       "  80.0),\n",
       " (\"def rotate(list_a: list, places: int):\\n    \\n    if not isinstance(list_a, list):\\n        raise TypeError('The argument given is not of `list` type.')\\n\\n    places = places % len(list_a)\\n    rotated = list_a[places:]\\n    rotated.extend(list_a[:places])\\n    return rotated\",\n",
       "  'import pytest\\n\\nfrom source import rotate\\n\\ndef test_rotate_list():\\n    assert rotate([1, 2, 3, 4, 5], 1) == [5, 1, 2, 3, 4]\\n    assert rotate([1, 2, 3, 4, 5], 2) == [4, 5, 1, 2, 3]\\n    assert rotate([1, 2, 3, 4, 5], 3) == [3, 4, 5, 1, 2]\\n    assert rotate([1, 2, 3, 4, 5], 4) == [2, 3, 4, 5, 1]\\n    assert rotate([1, 2, 3, 4, 5], 5) == [1, 2, 3, 4, 5]\\n    assert rotate([1, 2, 3, 4, 5], 6) == [2, 3, 4, 5, 1]\\n    assert rotate([1, 2, 3, 4, 5], -1) == [5, 1, 2, 3, 4]\\n    assert rotate([1, 2, 3, 4, 5], -2) == [4, 5, 1, 2, 3]\\n    assert rotate([1, 2, 3, 4, 5], -3) == [3, 4, 5, 1, 2]\\n    assert rotate([1, 2, 3, 4, 5], -4) == [2, 3, 4, 5, 1]\\n    assert rotate([1, 2, 3, 4, 5], -5) == [1, 2, 3, 4, 5]\\n    assert rotate([1, 2, 3, 4, 5], -6) == [2, 3, 4, 5, 1]\\n    assert rotate([1], 1) == [1]\\n    assert rotate([1], -1) == [1]\\n    assert rotate([], 1) == []\\n    assert rotate([], -1) == []',\n",
       "  86.0),\n",
       " (\"def generate_arrow_points(position, arrow_vector, thickness=5.0, size_multiplier=1.0, arrow_head_width_mul=0.75, tip_to_base_ratio=2.0 / 3.0):\\n    \\n\\n    thickness *= size_multiplier\\n    px, py = position\\n\\n    arr_vec = (\\n        arrow_vector[0] * size_multiplier,\\n        arrow_vector[1] * size_multiplier\\n    )  # scale up the original arrow vector describing the arrow's direction\\n\\n    vec_length = (arr_vec[0] ** 2 + arr_vec[1] ** 2) ** 0.5\\n    if not vec_length:\\n        return ((0, 0), ) * 7\\n\\n    avp_norm = (\\n        -arr_vec[1] / vec_length,\\n        arr_vec[0] / vec_length\\n    )  # normalize the perpendicular arrow vector\\n\\n    arrow_head_width = thickness * arrow_head_width_mul\\n    # multiply the arrow body width by the arrow head thickness multiplier\\n\\n    avp_scaled = (\\n        avp_norm[0] * arrow_head_width,\\n        avp_norm[1] * arrow_head_width\\n    )  # scale up the normalized perpendicular arrow vector\\n\\n    point0 = (\\n        avp_norm[0] * thickness,\\n        avp_norm[1] * thickness\\n    )\\n\\n    point1 = (\\n        point0[0] + arr_vec[0] * tip_to_base_ratio,\\n        point0[1] + arr_vec[1] * tip_to_base_ratio\\n    )\\n\\n    point2 = (\\n        point1[0] + avp_scaled[0],\\n        point1[1] + avp_scaled[1]\\n    )\\n\\n    point3 = arr_vec  # tip of the arrow\\n    mulp4 = -(thickness * 2.0 + arrow_head_width * 2.0)\\n    # multiplier to mirror the normalized perpendicular arrow vector\\n\\n    point4 = (\\n        point2[0] + avp_norm[0] * mulp4,\\n        point2[1] + avp_norm[1] * mulp4\\n    )\\n\\n    point5 = (\\n        point4[0] + avp_scaled[0],\\n        point4[1] + avp_scaled[1]\\n    )\\n\\n    point6 = (\\n        point5[0] + ((-arr_vec[0]) * tip_to_base_ratio),\\n        point5[1] + ((-arr_vec[1]) * tip_to_base_ratio)\\n    )\\n\\n    return (\\n        (int(point6[0] + px), int(point6[1] + py)),\\n        (int(point5[0] + px), int(point5[1] + py)),\\n        (int(point4[0] + px), int(point4[1] + py)),\\n        (int(point3[0] + px), int(point3[1] + py)),\\n        (int(point2[0] + px), int(point2[1] + py)),\\n        (int(point1[0] + px), int(point1[1] + py)),\\n        (int(point0[0] + px), int(point0[1] + py))\\n    )\",\n",
       "  \"from source import *\\nimport source\\nimport pytest\\n\\ndef test_generate_arrow_points():\\n    position = (10, 20)\\n    arrow_vector = (1, 1)\\n    result = source.generate_arrow_points(position, arrow_vector)\\n    assert len(result) == 7, 'The function should return a tuple with 7 points'\",\n",
       "  95.0),\n",
       " ('def fibonacci(length, a, b):\\n    \\n    if length == 1:\\n        return (a,)\\n    if length == 2:\\n        return (a, b)\\n\\n    first = (a,)\\n    second = (a, b)\\n    while True:\\n        next = second + first\\n        if len(next) >= length:\\n            return next[0:length]\\n        first = second\\n        second = next',\n",
       "  '# test_source.py\\n\\nimport pytest\\nfrom source import fibonacci\\n\\ndef test_fibonacci_1():\\n    result = fibonacci(1, 0, 1)\\n    assert result == (0,)\\n\\ndef test_fibonacci_2():\\n    result = fibonacci(2, 0, 1)\\n    assert result == (0, 1)\\n\\ndef test_fibonacci_3():\\n    result = fibonacci(3, 0, 1)\\n    assert result == (0, 1, 1)\\n\\n# Add more tests as needed to ensure full code coverage.',\n",
       "  85.0),\n",
       " ('def r_to_depth(x, interval):\\n    \\n    return x * interval / 3600.',\n",
       "  \"# test_source.py\\nimport pytest\\nimport os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\\n\\nfrom source import r_to_depth\\n\\ndef test_r_to_depth():\\n    assert r_to_depth(1, 1) == 0.03333333333333333\",\n",
       "  100.0),\n",
       " ('def process_data(data, dict_):\\n    \\n    indicator = dict_[\"ESTIMATION\"][\"indicator\"]\\n    outcome = dict_[\"ESTIMATION\"][\"dependent\"]\\n    D = data[indicator].values\\n\\n    data1 = data[data[indicator] == 1]\\n    data2 = data[data[indicator] == 0]\\n\\n    X1 = data1[dict_[\"TREATED\"][\"order\"]].values\\n    X0 = data2[dict_[\"UNTREATED\"][\"order\"]].values\\n    Z1 = data1[dict_[\"CHOICE\"][\"order\"]].values\\n    Z0 = data2[dict_[\"CHOICE\"][\"order\"]].values\\n\\n    Y1 = data1[outcome].values\\n    Y0 = data2[outcome].values\\n\\n    return D, X1, X0, Z1, Z0, Y1, Y0',\n",
       "  'import os\\nimport sys\\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \\'..\\')))\\nimport source  # The directory should be relative to this test file\\nimport pytest\\nimport pandas as pd\\n\\ndef test_process_data():\\n    # Assume we have a pandas DataFrame named \\'data\\' \\n    data = pd.DataFrame()  # This should be replaced with real data\\n    dict_ = {\"ESTIMATION\": {\"indicator\": \"indicator_column\", \"dependent\": \"outcome_column\"}, \\n             \"TREATED\": {\"order\": \"treated_order_column\"},\\n             \"UNTREATED\": {\"order\": \"untreated_order_column\"},\\n             \"CHOICE\": {\"order\": \"choice_order_column\"}}\\n    \\n    D, X1, X0, Z1, Z0, Y1, Y0 = source.process_data(data, dict_)\\n    \\n    assert isinstance(D, pd.Series)\\n    assert isinstance(X1, pd.Series)\\n    assert isinstance(X0, pd.Series)\\n    assert isinstance(Z1, pd.Series)\\n    assert isinstance(Z0, pd.Series)\\n    assert isinstance(Y1, pd.Series)\\n    assert isinstance(Y0, pd.Series)',\n",
       "  31.0),\n",
       " ('def cropping_center(x, crop_shape, batch=False):\\n    \\n    orig_shape = x.shape\\n    if not batch:\\n        h0 = int((orig_shape[0] - crop_shape[0]) * 0.5)\\n        w0 = int((orig_shape[1] - crop_shape[1]) * 0.5)\\n        x = x[h0 : h0 + crop_shape[0], w0 : w0 + crop_shape[1]]\\n    else:\\n        h0 = int((orig_shape[1] - crop_shape[0]) * 0.5)\\n        w0 = int((orig_shape[2] - crop_shape[1]) * 0.5)\\n        x = x[:, h0 : h0 + crop_shape[0], w0 : w0 + crop_shape[1]]\\n    return x',\n",
       "  'import pytest\\nimport numpy as np\\nimport source\\n\\ndef test_cropping_center():\\n    x = np.random.rand(100, 100)\\n    crop_shape = (50, 50)\\n    result = source.cropping_center(x, crop_shape)\\n    assert np.array_equal(result.shape, crop_shape), \"The shape of the returned array doesn\\'t match the expected shape\"\\n    x = np.random.rand(5, 100, 100)\\n    crop_shape = (50, 50)\\n    result = source.cropping_center(x, crop_shape, batch=True)\\n    assert np.array_equal(result.shape, (5, *crop_shape)), \"The shape of the returned array doesn\\'t match the expected shape\"',\n",
       "  100.0),\n",
       " ('def timestamp_to_microseconds(ts):\\n    \\n    microsecond_amount = 1000000  # 1 second worth of microseconds\\n    return ts.timestamp() * microsecond_amount',\n",
       "  'import pytest\\nfrom source import timestamp_to_microseconds\\n\\ndef test_timestamp_to_microseconds():\\n    import time\\n    import datetime\\n    # create a dummy timestamp\\n    ts = time.time()\\n    # convert the timestamp to microseconds\\n    microseconds = timestamp_to_microseconds(ts)\\n    # create a baseline time\\n    baseline = datetime.datetime.now().timestamp()\\n    # calculate the difference in microseconds\\n    difference = abs(baseline*1000000 - microseconds)\\n    # Assert that the difference is less than 1 second (1,000,000 microseconds)\\n    assert difference < 1000000',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef cells_to_bboxes(predictions, anchors, S, is_preds=True):\\n    \\n    batch_size = predictions.shape[0]\\n    num_anchors = len(anchors)\\n    box_predictions = predictions[..., 1:5]\\n    if is_preds:\\n        anchors = anchors.reshape(1, num_anchors, 1, 1, 2)\\n        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\\n        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\\n        scores = torch.sigmoid(predictions[..., 0:1])\\n    else:\\n        scores = predictions[..., 0:1]\\n\\n    cell_indices = (\\n        torch.arange(S)\\n            .repeat(predictions.shape[0], num_anchors, S, 1)\\n            .unsqueeze(-1)\\n            .to(predictions.device)\\n    )\\n    x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\\n    y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\\n    w_h = 1 / S * box_predictions[..., 2:4]\\n    converted_bboxes = torch.cat((scores, x, y, w_h), dim=-1).reshape(batch_size, num_anchors * S * S, 5)\\n    return converted_bboxes.tolist()',\n",
       "  'import torch\\nimport pytest\\n\\ndef test_cells_to_bboxes():\\n    predictions = torch.rand((1, 3, 3, 5))\\n    anchors = [1, 1]\\n    S = 3\\n    is_preds = True\\n    result = cells_to_bboxes(predictions, anchors, S, is_preds)\\n    assert len(result) == 1',\n",
       "  0.0),\n",
       " ('def frame_error(y_true, y_pred):\\n    \\n\\n    if y_true.ndim > 1:\\n        raise ValueError(\\n            \"frame_error only defined for 1-dimensional inputs\"\\n            \" but y_true.ndim is {}\".format(y_true.ndim)\\n        )\\n\\n    if y_pred.ndim > 1:\\n        raise ValueError(\\n            \"frame_error only defined for 1-dimensional inputs\"\\n            \" but y_pred.ndim is {}\".format(y_pred.ndim)\\n        )\\n\\n    if y_true.shape[-1] != y_pred.shape[-1]:\\n        raise ValueError(\\n            \"y_true and y_pred should have the same length.\"\\n            \"y_true.shape is {} and y_pred.shape is {}\".format(\\n                y_true.shape, y_pred.shape\\n            )\\n        )\\n\\n    return 1 - sum(y_true == y_pred) / y_true.shape[-1]',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import frame_error\\n\\ndef test_frame_error():\\n    y_true = np.array([1, 2, 3, 4])\\n    y_pred = np.array([1, 2, 3, 5])\\n    assert np.isclose(frame_error(y_true, y_pred), 0.5)',\n",
       "  62.0),\n",
       " ('def delta(r, a, Q=0):\\n    \\n    delta = (r ** 2) - (2 * r) + (a ** 2) + Q ** 2\\n\\n    return delta',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import delta\\n\\ndef test_delta():\\n    assert delta(1, 2) == 5',\n",
       "  100.0),\n",
       " ('def no_intersections(nodes1, degree1, nodes2, degree2):\\n    r\\n    # NOTE: This is a circular import.\\n    # pylint: disable=import-outside-toplevel\\n    from bezier.hazmat import triangle_intersection\\n\\n    # pylint: enable=import-outside-toplevel\\n\\n    located = triangle_intersection.locate_point(\\n        nodes2, degree2, nodes1[0, 0], nodes1[1, 0]\\n    )\\n    if located is not None:\\n        return None, True\\n\\n    located = triangle_intersection.locate_point(\\n        nodes1, degree1, nodes2[0, 0], nodes2[1, 0]\\n    )\\n    if located is not None:\\n        return None, False\\n\\n    return [], None',\n",
       "  'import pytest\\nimport os\\nimport sys\\n\\nCURRENT_DIR = os.path.dirname(__file__)\\nsys.path.append(os.path.join(CURRENT_DIR, \"..\"))  # Adds higher directory to import \\'bezier\\' module\\n\\nfrom bezier.hazmat import triangle_intersection\\nfrom source import no_intersections\\n\\n\\ndef test_no_intersections():\\n    nodes1 = [[0, 0], [1, 0], [0, 1]]\\n    degree1 = 2\\n    nodes2 = [[0.5, 0], [1, 0.5]]\\n    degree2 = 2\\n\\n    located, result = no_intersections(nodes1, degree1, nodes2, degree2)\\n    assert located == [], \"Test failed: 1\"\\n    assert result is None, \"Test failed: 2\"',\n",
       "  0.0),\n",
       " ('def Ug(M1, M2, rho, d_pc):\\n    \\n    G = 6.67408*1E-11  # m3 kg−1 s−2\\n    Msol = 1.98847*1E30  # kg\\n    GM = 1.3271244*1E20  # m3 s-2\\n    s = rho * d_pc  # AU\\n    AU = 149597870700  # m/AU\\n    Ug = -G * (M1*M2 * Msol**2)/(s*AU)  # kg2 m2 s-2 = kg J (1 J = kg m2 s-2)\\n    return Ug',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\".\")  # necessary to import source.py from the same directory\\nfrom source import Ug\\n\\ndef test_ug():\\n    assert Ug(1, 1, 1, 1) == 1',\n",
       "  100.0),\n",
       " ('def rgba2rgb(rgba):\\n    # type: (np.ndarray) -> np.ndarray\\n    \\n    rgb = rgba[:, :, :3]\\n    return rgb',\n",
       "  '# test_source.py\\n\\nimport sys\\nsys.path.append(\".\")  # This is to import source.py from the same directory\\nimport source  # This is where your function rgba2rgb is defined\\nimport numpy as np\\n\\ndef test_rgba2rgb():\\n    # create a numpy array with random values\\n    rgba = np.random.rand(10, 10, 4)\\n    # call the function and get the result\\n    result = source.rgba2rgb(rgba)\\n    # perform assertion to check if the shape of the result is as expected\\n    assert result.shape == (10, 10, 3), \"The shape of the output is not as expected\"',\n",
       "  100.0),\n",
       " (\"def prandtl(cp=None, mu=None, k=None, nu=None, alpha=None):\\n    \\n    if cp and mu and k:\\n        pr = (cp * mu) / k\\n    elif nu and alpha:\\n        pr = nu / alpha\\n    else:\\n        raise ValueError('Must provide (cp, mu, k) or (nu, alpha)')\\n\\n    return pr\",\n",
       "  '# -*- coding: utf-8 -*-\\nimport pytest\\nimport source  # the file with the source code\\n\\nclass TestPrandtl:\\n\\n    def test_prandtl_with_cp_mu_k(self):\\n        assert source.prandtl(cp=1, mu=2, k=3) == 2/3\\n\\n    def test_prandtl_with_nu_alpha(self):\\n        assert source.prandtl(nu=4, alpha=5) == 4/5\\n\\n    def test_prandtl_exception(self):\\n        with pytest.raises(ValueError):\\n            source.prandtl()',\n",
       "  100.0),\n",
       " ('def linear_line(x,intercept,slope):\\n    \\n    return slope * x + intercept',\n",
       "  \"# test_source.py\\nimport sys\\nsys.path.append('..') # to include the parent directory in the import path\\nimport source\\n\\ndef test_linear_line():\\n    assert source.linear_line(3,2,1) == 5\",\n",
       "  100.0),\n",
       " ('def get_pixel_dist(pixel, red, green, blue):\\n    \\n    red_distance = red - pixel.red\\n    green_distance = green - pixel.green\\n    blue_distance = blue - pixel.blue\\n    color_distance = (red_distance ** 2 + green_distance ** 2 + blue_distance ** 2) ** 0.5\\n    return color_distance',\n",
       "  '# test_source.py\\n\\nimport sys\\nsys.path.append(\"..\") # to include the parent directory in the path\\nfrom source import Pixel\\nimport pytest\\n\\ndef test_get_pixel_dist():\\n    pixel = Pixel(10, 20, 30)\\n    assert get_pixel_dist(pixel, 10, 20, 30) == 0\\n    assert get_pixel_dist(pixel, 10, 21, 30) != 0\\n    assert get_pixel_dist(pixel, 11, 20, 30) != 0\\n    assert get_pixel_dist(pixel, 10, 20, 31) != 0\\n\\n\\nif __name__ == \"__main__\":\\n    pytest.main()',\n",
       "  17.0),\n",
       " ('def get_text_colour_analytics_sentiment(score):\\n    \\n    # Get our score into the range [0..4], which is our shade \\'strength\\' - higher => brighter shade\\n    truncated = min(abs(int(score)), 4)\\n    col_shade = (4 - truncated) * 51\\n\\n    if score >= 0:\\n        # Positive sentiment => Green shade\\n        background_colour = \"{0:0>2X}{1:0>2X}{2:0>2X}\".format(col_shade, 255, col_shade)\\n    else:\\n        # Negative sentiment => Red shade\\n        background_colour = \"{0:0>2X}{1:0>2X}{2:0>2X}\".format(255, col_shade, col_shade)\\n\\n    return background_colour',\n",
       "  \"import os\\nimport subprocess\\nimport pytest\\nfrom source import get_text_colour_analytics_sentiment\\n\\ndef test_positive_sentiment():\\n    assert get_text_colour_analytics_sentiment(0.5).lower() == '0f00ff00'\\n\\ndef test_negative_sentiment():\\n    assert get_text_colour_analytics_sentiment(-0.5).lower() == 'ff000000'\\n\\ndef test_zero_sentiment():\\n    assert get_text_colour_analytics_sentiment(0).lower() == '7f7f7f00'\\n\\ndef test_random_score():\\n    assert get_text_colour_analytics_sentiment(0.234).lower() != '00000000'\\n\\ndef test_high_score():\\n    assert get_text_colour_analytics_sentiment(5).lower() == '00ff0000'\\n\\ndef test_low_score():\\n    assert get_text_colour_analytics_sentiment(-5).lower() == 'ff0000ff'\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef normalized_linspace(length, dtype=None, device=None):\\n    \\n    if isinstance(length, torch.Tensor):\\n        length = length.to(device, dtype)\\n    first = -(length - 1.0) / length\\n    return torch.arange(length, dtype=dtype, device=device) * (2.0 / length) + first',\n",
       "  'import pytest\\nimport torch\\nfrom source import normalized_linspace\\n\\n@pytest.mark.parametrize(\"length\", [10, 20, 30])\\ndef test_normalized_linspace(length):\\n    result = normalized_linspace(length)\\n    assert torch.isclose(result[0], -0.5).item(), \"The first element should be -0.5\"\\n    assert torch.isclose(result[-1], 0.5).item(), \"The last element should be 0.5\"\\n    assert result.shape == torch.Size([length]), \"The output shape should be torch.Size([length])\"',\n",
       "  83.0),\n",
       " ('def sun_rot_elements_at_epoch(T, d):\\n    \\n    ra = 286.13\\n    dec = 63.87\\n    W = 84.176 + 14.1844000 * d\\n\\n    return ra, dec, W',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import sun_rot_elements_at_epoch\\n\\ndef test_sun_rot_elements_at_epoch():\\n    T, d = 0, 0\\n    ra, dec, W = sun_rot_elements_at_epoch(T, d)\\n    assert ra == 286.13, \"The RA value is not as expected\"',\n",
       "  100.0),\n",
       " ('def filter_samples(samples, amplitude_tol=1e-6):\\n    \\n    if len(samples) == 0:\\n        return samples\\n\\n    amplitude_filter = samples[:, 0] > amplitude_tol\\n    T2_filter = samples[:, 1] > 0\\n\\n    filtered_samples = samples[amplitude_filter & T2_filter]\\n\\n    return filtered_samples',\n",
       "  'import sys\\nsys.path.append(\".\")\\nimport source  # assuming the source code file is in the same directory\\n\\ndef test_filter_samples():\\n    samples = [[1e-7, 1], [1e-5, -1], [1e-9, 0], [1e-6, 0]]\\n    result = source.filter_samples(samples)\\n    assert len(result) == 2, \\'Test Case 1 Failed\\'\\n    assert all(result[:, 0] <= 1e-6), \\'Test Case 2 Failed\\'\\n    assert all(result[:, 1] > 0), \\'Test Case 3 Failed\\'',\n",
       "  43.0),\n",
       " ('import torch\\n\\ndef weighted_norm(input, Re):\\n    \\n    Re = Re.to(input.device)\\n    return torch.matmul(torch.matmul(input.transpose(1, 2), Re), input).squeeze()',\n",
       "  '# test_source.py\\nimport pytest\\nimport torch\\nfrom source import weighted_norm  # assuming the function is in source.py\\n\\ndef test_weighted_norm():\\n    # Create random tensor inputs\\n    input = torch.randn(10, 10)\\n    Re = torch.randn(10, 10)\\n\\n    # Compute the weighted norm\\n    result = weighted_norm(input, Re)\\n\\n    # Check if the output has the expected shape\\n    assert result.shape == (10,)\\n\\n    # Check if the output is close to zero for random input\\n    assert torch.allclose(result, torch.zeros_like(result), atol=1e-6)',\n",
       "  100.0),\n",
       " (\"def polygon_to_line(gdf, geometry_field='geometry'):\\n    \\n    gdf[geometry_field] = gdf[geometry_field].boundary\\n    return gdf\",\n",
       "  \"import sys\\nimport os\\nsys.path.append(os.path.abspath(os.path.dirname(__file__) + '/' + '..'))\\nimport source  # assumes source.py is in the same directory as this test file\\nimport pytest\\n\\nclass TestPolygonToLine:\\n    \\n    @pytest.fixture\\n    def polygon_to_line_fixture(self):\\n        # Create a test geometry dataframe here\\n        gdf = ...\\n        return gdf\\n    \\n    def test_polygon_to_line(self, polygon_to_line_fixture):\\n        # Perform a single assertion per test\\n        assert source.polygon_to_line(polygon_to_line_fixture) is not None\",\n",
       "  67.0),\n",
       " ('def length(segments):\\n    \\n    return segments[:, 1] - segments[:, 0]',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport numpy as np\\nfrom source import length\\n\\ndef test_length_func():\\n    # Arrange\\n    segments = np.array([[0, 10], [20, 30], [35, 40]])\\n\\n    # Act\\n    result = length(segments)\\n\\n    # Assert\\n    assert np.array_equal(result, np.array([10, 10, 5]))',\n",
       "  100.0),\n",
       " ('def binarize(inputs, threshold=.5):\\n    \\n    return inputs >= threshold',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import binarize\\n\\ndef test_binarize():\\n    assert binarize([1, 2, 3]) == [True, True, True]\\n    assert binarize([1, .5, 0]) == [True, False, False]\\n    assert binarize([0.2, 0.5, 0.8]) == [False, False, True]\\n    assert binarize([-1, -2, -3]) == [False, False, False]',\n",
       "  100.0),\n",
       " ('def get_lst(jd, lon):\\n    \\n\\n    dd = jd - 2451545.0\\n\\n    lmst = ((280.46061837 + 360.98564736629 * dd +\\n            # 0.000388 * (dd / 36525.)**2 +  # 0.1s / century, can be neglected here\\n             lon) % 360) / 15.\\n\\n    return lmst',\n",
       "  '# test_source.py\\nimport source\\nimport pytest\\n\\ndef test_get_lst():\\n    # Test based on approximate values\\n    assert source.get_lst(2458789.5, 0) == 259.75',\n",
       "  100.0),\n",
       " ('def _toolkit_get_topk_bottomk(values, k=5):\\n    \\n\\n    top_values = values.topk(\"value\", k=k)\\n    top_values = top_values[top_values[\"value\"] > 0]\\n\\n    bottom_values = values.topk(\"value\", k=k, reverse=True)\\n    bottom_values = bottom_values[bottom_values[\"value\"] < 0]\\n\\n    return (top_values, bottom_values)',\n",
       "  \"import pytest\\nfrom source import _toolkit_get_topk_bottomk  # assuming that the function is in source.py\\n\\n\\ndef test_toolkit_get_topk_bottomk():\\n    # Case 1: Normal usage, top 5 values\\n    values = [1, 5, -2, 10, -8, 0, 9, 7, 3, -6]\\n    result = _toolkit_get_topk_bottomk(values)\\n    assert len(result[0]) == 5 and len(result[1]) == 5\\n    assert all(val > 0 for val in result[0]['value'])\\n    assert all(val > 0 for val in result[1]['value'])\\n\\n    # Case 2: Check values when all values are negative\\n    values = [-1, -5, -2, -10, -8, -9, -7, -3, -6, 0]\\n    result = _toolkit_get_topk_bottomk(values)\\n    assert len(result[0]) == 5 and len(result[1]) == 5\\n    assert all(val < 0 for val in result[0]['value'])\\n    assert all(val < 0 for val in result[1]['value'])\\n\\n    # Case 3: Check values when there is a mix of positive and negative numbers\\n    values = [-1, -5, 2, -10, 8, -9, 7, -3, 6, 0]\\n    result = _toolkit_get_topk_bottomk(values)\\n    assert len(result[0]) == 5 and len(result[1]) == 5\\n    assert all(val > 0 for val in result[0]['value'])\\n    assert all(val < 0 for val in result[1]['value'])\\n\\n    # Case 4: Check values when there are duplicate positive and negative numbers\\n    values = [-1, -5, 2, -10, 8, -9, 7, -3, 6, 0, 6]\\n    result = _toolkit_get_topk_bottomk(values)\\n    assert len(result[0]) == 5 and len(result[1]) == 5\\n    assert all(val > 0 for val in result[0]['value'])\\n    assert all(val < 0 for val in result[1]['value'])\\n\\n    # Case 5: Check values when k is equal to length of array\\n    values = [-1, -5, 2, -10, 8, -9, 7, -3, 6, 0]\\n    result = _toolkit_get_topk_bottomk(values, k=9)\\n    assert len(result[0]) == 9 and len(result[1]) == 9\\n    assert all(val > 0 for val in result[0]['value'])\\n    assert all(val < 0 for val in result[1]['value'])\\n\\n    # Case 6: Check with empty list\\n    values = []\\n    result = _toolkit_get_topk_bottomk(values)\\n    assert len(result[0]) == 0 and len(result[1]) == 0\",\n",
       "  33.0),\n",
       " ('import torch\\n\\ndef make_pvmlnet():\\n    \\n    # The first convolution has no padding so that after training it\\n    # can be fused with the initial mean/var normalization.\\n    model = torch.nn.Sequential(\\n        torch.nn.Conv2d(3, 96, 7, 4, padding=0),\\n        torch.nn.BatchNorm2d(96),\\n        torch.nn.ReLU(),\\n        torch.nn.Conv2d(96, 192, 3, 2, padding=1),\\n        torch.nn.BatchNorm2d(192),\\n        torch.nn.ReLU(),\\n        torch.nn.Conv2d(192, 192, 3, 1, padding=1),\\n        torch.nn.BatchNorm2d(192),\\n        torch.nn.ReLU(),\\n        torch.nn.Conv2d(192, 384, 3, 2, padding=1),\\n        torch.nn.BatchNorm2d(384),\\n        torch.nn.ReLU(),\\n        torch.nn.Conv2d(384, 384, 3, 1, padding=1),\\n        torch.nn.BatchNorm2d(384),\\n        torch.nn.ReLU(),\\n        torch.nn.Conv2d(384, 512, 3, 2, padding=1),\\n        torch.nn.BatchNorm2d(512),\\n        torch.nn.ReLU(),\\n        torch.nn.Conv2d(512, 512, 3, 1, padding=1),\\n        torch.nn.BatchNorm2d(512),\\n        torch.nn.ReLU(),\\n        torch.nn.Conv2d(512, 512, 3, 2, padding=1),\\n        torch.nn.BatchNorm2d(512),\\n        torch.nn.ReLU(),\\n        torch.nn.Conv2d(512, 512, 3, 1, padding=1),\\n        torch.nn.BatchNorm2d(512),\\n        torch.nn.ReLU(),\\n        torch.nn.Conv2d(512, 1024, 4, 1, padding=0),\\n        torch.nn.BatchNorm2d(1024),\\n        torch.nn.ReLU(),\\n        torch.nn.Dropout2d(),\\n        torch.nn.Conv2d(1024, 1024, 1, 1, padding=0),\\n        torch.nn.BatchNorm2d(1024),\\n        torch.nn.ReLU(),\\n        torch.nn.Dropout2d(),\\n        torch.nn.Conv2d(1024, 1000, 1, 1, padding=0),\\n        torch.nn.AdaptiveAvgPool2d(1)\\n    )\\n    return model',\n",
       "  'import pytest\\nimport torch\\nfrom source import make_pvmlnet\\n\\ndef test_make_pvmlnet():\\n    model = make_pvmlnet()\\nassert isinstance(model, torch.nn.Module)',\n",
       "  50.0),\n",
       " ('def newton_iteration_step(phi, phiprime, g, s, p):\\n    r\\n    R = phi.parent()\\n    x,y = R.gens()\\n    g = R(g).univariate_polynomial()\\n    s = R(s).univariate_polynomial()\\n    p = R(p).univariate_polynomial()\\n\\n    pnext = p**2\\n    gnext = g - phi(x,g).univariate_polynomial()*s\\n    gnext = gnext % pnext\\n    snext = 2*s - phiprime(x,gnext).univariate_polynomial()*s**2\\n    snext = snext % pnext\\n\\n    gnext = R(gnext)\\n    snext = R(snext)\\n    pnext = R(pnext)\\n    return gnext,snext,pnext',\n",
       "  \"import sys\\nsys.path.append('.')\\nimport source\\nimport pytest\\n\\ndef test_newton_iteration_step():\\n    phi = lambda x, g: x ** 2 + g\\n    phiprime = lambda x, g: 2 * x\\n    g = 1\\n    s = 2\\n    p = 3\\n    assert source.newton_iteration_step(phi, phiprime, g, s, p) == (1, 1, 0)\",\n",
       "  12.0),\n",
       " ('def wc(tdb, v, **kwargs):\\n    \\n    default_kwargs = {\\n        \"round\": True,\\n    }\\n    kwargs = {**default_kwargs, **kwargs}\\n\\n    wci = (10.45 + 10 * v ** 0.5 - v) * (33 - tdb)\\n\\n    # the factor 1.163 is used to convert to W/m2\\n    wci = wci * 1.163\\n\\n    if kwargs[\"round\"]:\\n        wci = round(wci, 1)\\n\\n    return {\"wci\": wci}',\n",
       "  'import pytest\\nimport os\\nimport source  # assuming the source code is in a file named source.py in the same directory\\n\\ndef test_wc():\\n    # given\\n    tdb = 30\\n    v = 5\\n    expected_wci = (10.45 + 10 * v ** 0.5 - v) * (33 - tdb) * 1.163\\n    if kwargs[\"round\"]:\\n        expected_wci = round(expected_wci, 1)\\n    # when\\n    result = source.wc(tdb, v, round=True)\\n    # then\\n    assert type(result) == dict\\n    assert \"wci\" in result\\n    assert result[\"wci\"] == expected_wci',\n",
       "  12.0),\n",
       " ('def get_ref_multilen_at_nuc_pos(row, ref_nuc_seq_dict):\\n    \\n    nuc_pos_1based = row[\"pos\"]\\n    nuc_pos_0based = nuc_pos_1based - 1\\n    nuc = str(ref_nuc_seq_dict[\"MN908947.3\"].seq[nuc_pos_0based: nuc_pos_0based + row[\"length\"]])\\n    return nuc',\n",
       "  'from source import get_ref_multilen_at_nuc_pos\\n\\ndef test_get_ref_multilen_at_nuc_pos():\\n    ref_nuc_seq_dict = {\"MN908947.3\": \"ATCGTACGATCGATCGATCGATCG\"}  # example dict, replace with actual data\\n    row = {\"pos\": 2, \"length\": 3}  # example data, replace with actual data\\n    assert get_ref_multilen_at_nuc_pos(row, ref_nuc_seq_dict) == \"CGT\"',\n",
       "  80.0),\n",
       " ('def subtract(x, y):\\n    \\n    return (x - y)',\n",
       "  \"import sys\\nsys.path.append('.')\\nimport source as s\\n\\ndef test_subtract():\\n    assert s.subtract(5, 2) == 3, 'The function did not return the expected result'\",\n",
       "  100.0),\n",
       " (\"def variance_threshold_selector(data, threshold=0.5):\\n    \\n    from sklearn.feature_selection import VarianceThreshold\\n\\n     # Transpose to work with sklearn's implementation, which assumes features are columns:\\n    data = data.T\\n    # Fit selector:\\n    selector = VarianceThreshold(threshold)\\n    selector.fit(data)\\n    # Apply transformation to df:\\n    data_red = data[data.columns[selector.get_support(indices=True)]]\\n    # Transpose back so genes are rows again:\\n    return data_red.T\",\n",
       "  '# test_source.py\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.feature_selection import VarianceThreshold\\nfrom source import variance_threshold_selector\\n\\ndef test_variance_threshold_selector():\\n    # Mocking data\\n    data = pd.DataFrame(np.random.rand(100, 10))\\n\\n    # Apply function\\n    result = variance_threshold_selector(data)\\n\\n    # Mocking VarianceThreshold selector\\n    selector = VarianceThreshold(0.5)\\n    selector.fit(data.T)\\n    expected_result = data.T[data.columns[selector.get_support(indices=True)]]\\n\\n    # Assertion\\n    assert np.array_equal(result, expected_result)',\n",
       "  71.0),\n",
       " ('import torch\\n\\ndef normalize3d(tensor, mean, std, inplace=False):\\n    \\n    if not inplace:\\n        tensor = tensor.clone()\\n\\n    mean = torch.as_tensor(mean, dtype=torch.float32, device=tensor.device)\\n    std = torch.as_tensor(std, dtype=torch.float32, device=tensor.device)\\n    tensor.sub_(mean[:, None, None, None]).div_(std[:, None, None, None])\\n    return tensor',\n",
       "  '',\n",
       "  0.0),\n",
       " (\"def read_detector_pars(filename):\\n    \\n    return ascii.read(\\n        filename, guess=False, format='no_header', delimiter=r'\\\\s',\\n        names=('OBSMODE', 'SCALE', 'NX', 'NY'),\\n        converters={'OBSMODE': [ascii.convert_numpy(str)],\\n                    'SCALE': [ascii.convert_numpy(float)],\\n                    'NX': [ascii.convert_numpy(int)],\\n                    'NY': [ascii.convert_numpy(int)]})\",\n",
       "  \"# test_source.py\\nimport pytest\\nfrom source import read_detector_pars\\n\\ndef test_read_detector_pars():\\n    # Arrange\\n    filename = 'test_data.txt'\\n    # Act\\n    result = read_detector_pars(filename)\\n    # Assert\\n    assert type(result) is tuple\\n    assert len(result) == 4\",\n",
       "  100.0),\n",
       " ('def parabolic(f, x):\\n    \\n    xv = 1 / 2. * (f[x - 1] - f[x + 1]) / (f[x - 1] - 2 * f[x] + f[x + 1]) + x\\n    yv = f[x] - 1 / 4. * (f[x - 1] - f[x + 1]) * (xv - x)\\n\\n\\n    return (xv, yv)',\n",
       "  'import pytest\\nimport source\\n\\ndef test_parabolic():\\n    f = [1, 2, 3, 4, 5]\\n    x = 3\\n    assert source.parabolic(f, x) == (3, 4)',\n",
       "  50.0),\n",
       " (\"def filternum(filt='foo'):\\n    \\n    if filt == 'foo':\\n        return list(range(5))\\n    else:\\n        filters = {'u': 0, 'g': 1, 'r': 2, 'i': 3, 'z': 4}\\n        return filters[filt]\",\n",
       "  \"import pytest\\nfrom source import filternum\\n\\ndef test_filternum_foo():\\n    assert filternum('foo') == list(range(5))\\n\\ndef test_filternum_invalid():\\n    with pytest.raises(KeyError):\\n        filternum('bar')\",\n",
       "  100.0),\n",
       " ('def runge_kutta_method(f, t0: float, x0: float, timestep: float, end: float, exact_solution=None):\\n    \\n    if end < t0:\\n        raise ValueError(\"Initial time is larger than the end time!\")\\n\\n    # Store the time steps\\n    time_steps = [t0]\\n    # Store the value at each time step\\n    values = [x0]\\n    # Store the exact values of the solutions at each time step, if the exact\\n    # solution is provided\\n    if exact_solution:\\n        exact_values = [exact_solution(t0)]\\n\\n    # Now start solving the differential equation numerically\\n    t = t0\\n    x = x0\\n    while t < end:\\n        t = t + timestep\\n        time_steps.append(t)\\n        \\n        k1 = f(x) * timestep\\n        k2 = f(x + 0.5 * k1) * timestep\\n        k3 = f(x + 0.5 * k2) * timestep\\n        k4 = f(x + k3) * timestep\\n\\n        x = x + 1.0 / 6.0 * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\\n        values.append(x)\\n        \\n        if exact_solution:\\n            exact_values.append(exact_solution(t))\\n\\n    return time_steps, values, None if not exact_solution else exact_values',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\"..\") # This will add the parent directory to the path to import the source file\\nfrom runge_kutta import runge_kutta_method\\n\\ndef test_runge_kutta_method():\\n    def f(x):\\n        # This is a simple example. The function you actually want to test may be more complex.\\n        return 1\\n\\n    with pytest.raises(ValueError):\\n        runge_kutta_method(f, 1, 0, 0.1, 0)\\n\\n    runge_kutta_method(f, 0, 1, 0.1, 1, lambda t: t+2)',\n",
       "  0.0),\n",
       " ('def similarity_threshold(h, bands):\\n    \\n    return (1.0 / bands) ** (1 / (h / bands))',\n",
       "  \"import pytest\\nimport sys\\nsys.path.insert(0, './')\\nfrom source import similarity_threshold\\n\\ndef test_similarity_threshold():\\n    assert similarity_threshold(100, 10) == 1.0\\n    assert similarity_threshold(200, 10) == 1.111111111111111\\n    assert similarity_threshold(300, 10) == 1.222222222222222\\n    assert similarity_threshold(400, 10) == 1.333333333333333\\n    assert similarity_threshold(500, 10) == 1.4444444444444444\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef maximal_eigenvector_dim2(J):\\n    r\\n    N = J.shape[0]\\n    a = J[:, 0, 0]\\n    b = J[:, 0, 1]\\n    d = J[:, 1, 1]\\n    sq = (b ** 2) + ((a - d) / 2.) ** 2\\n    ev = (a + d) / 2. + sq.sqrt()\\n    u = torch.cat((-b.reshape((N, 1)),\\n                   (a - ev).reshape((N, 1))\\n                   ), 1)\\n    normu = torch.norm(u, dim=1)\\n    notcool = (normu == 0.)\\n    nb_notcool = notcool.sum()\\n    if nb_notcool>0:\\n        u[notcool, :] = torch.randn((nb_notcool, 2),\\n                                    dtype=J.dtype,\\n                                    device=J.device)\\n    result = u / torch.norm(u, dim=1).reshape((N, 1))\\n    return result',\n",
       "  'import pytest\\nimport torch\\nfrom source import maximal_eigenvector_dim2\\n\\ndef test_maximal_eigenvector_dim2():\\n    J = torch.tensor([[2.0, 1.0, 2.0], [3.0, 2.0, 2.0]])\\n    expected_output = torch.tensor([[3.0, 1.0], [2.0, 2.0]])\\n    result = maximal_eigenvector_dim2(J)\\n    assert torch.allclose(result, expected_output, atol=1e-5)',\n",
       "  18.0),\n",
       " (\"def water(target, temperature='pore.temperature', salinity='pore.salinity'):\\n    r\\n    T = target[temperature]\\n    if salinity in target.keys():\\n        S = target[salinity]\\n    else:\\n        S = 0\\n    a1 = 9.9992293295E+02\\n    a2 = 2.0341179217E-02\\n    a3 = -6.1624591598E-03\\n    a4 = 2.2614664708E-05\\n    a5 = -4.6570659168E-08\\n    b1 = 8.0200240891E-01\\n    b2 = -2.0005183488E-03\\n    b3 = 1.6771024982E-05\\n    b4 = -3.0600536746E-08\\n    b5 = -1.6132224742E-11\\n    TC = T-273.15\\n    rho_w = a1 + a2*TC + a3*TC**2 + a4*TC**3 + a5*TC**4\\n    d_rho = b1*S + b2*S*TC + b3*S*(TC**2) + b4*S*(TC**3) + b5*(S**2)*(TC**2)\\n    rho_sw = rho_w + d_rho\\n    value = rho_sw\\n    return value\",\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\nfrom source import water\\n\\ndef test_water():\\n    target = {'pore.temperature': 300, 'pore.salinity':100}\\n    assert water(target) == 1631.39158720846205\",\n",
       "  9.0),\n",
       " ('def dist_2(x0, y0, x1, y1):\\n    r\\n    d0 = x1 - x0\\n    d1 = y1 - y0\\n    return d0 * d0 + d1 * d1',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")  # This helps Pytest to import source.py from the same directory\\nimport source  # This imports the source code\\n\\ndef test_dist_2():\\n    assert source.dist_2(0, 0, 1, 1) == 2  # tests if the function returns correct value for given input',\n",
       "  40.0),\n",
       " ('def calc_scale_factor(Align_x_y_sizes, mosaic_x_y_sizes):\\n    \\n    x_scale_fact = Align_x_y_sizes[0]/mosaic_x_y_sizes[0]\\n    y_scale_fact = Align_x_y_sizes[1]/mosaic_x_y_sizes[1]\\n    return (x_scale_fact+ y_scale_fact)/2',\n",
       "  'import pytest\\nfrom source import calc_scale_factor\\n\\ndef test_calc_scale_factor():\\n    Align_x_y_sizes = [100,200]\\n    mosaic_x_y_sizes = [200,400]\\n    assert calc_scale_factor(Align_x_y_sizes, mosaic_x_y_sizes) == 0.5',\n",
       "  100.0),\n",
       " (\"import numpy\\n\\ndef ravd(result, reference):\\n    \\n    result = numpy.atleast_1d(result.astype(numpy.bool))\\n    reference = numpy.atleast_1d(reference.astype(numpy.bool))\\n        \\n    vol1 = numpy.count_nonzero(result)\\n    vol2 = numpy.count_nonzero(reference)\\n    \\n    if 0 == vol2:\\n        raise RuntimeError('The second supplied array does not contain any binary object.')\\n    \\n    return (vol1 - vol2) / float(vol2)\",\n",
       "  \"import numpy\\nimport pytest\\nimport sys\\nsys.path.append('.')\\nfrom source import ravd\\n\\ndef test_ravd():\\n    result = numpy.array([[1, 0, 1], [0, 1, 0], [1, 1, 1]])\\n    reference = numpy.array([[0, 1, 0], [1, 0, 1], [0, 0, 0]])\\nexpected_result = 0.5\\nassert numpy.isclose(ravd(result, reference), expected_result), 'Test failed'\",\n",
       "  22.0),\n",
       " ('def extract_lsb(origin: int, count: int):\\n    \\n    if origin == 0:\\n        return \"00\"\\n    if origin == 1:\\n        return \"01\"\\n    binary_origin = bin(origin)\\n    binary_origin = binary_origin[-count:]\\n\\n    return binary_origin',\n",
       "  'import pytest\\nfrom source import extract_lsb\\n\\ndef test_extract_lsb():\\n    assert extract_lsb(0, 2) == \"00\"\\n    assert extract_lsb(1, 2) == \"01\"\\n    assert extract_lsb(10, 2) == \"10\"\\n    assert extract_lsb(1024, 3) == \"000\"\\n    assert extract_lsb(1024, 4) == \"0000\"',\n",
       "  100.0),\n",
       " ('def disjoint_bounds(bounds1, bounds2):\\n    \\n    return (bounds1[0] > bounds2[2] or bounds1[2] < bounds2[0] or\\n            bounds1[1] > bounds2[3] or bounds1[3] < bounds2[1])',\n",
       "  'import source  # assuming source.py is in the same directory\\n\\ndef test_disjoint_bounds():\\n    bounds1 = (1, 2, 3, 4)\\n    bounds2 = (5, 6, 7, 8)\\n    assert not source.disjoint_bounds(bounds1, bounds2)',\n",
       "  100.0),\n",
       " ('def multiply_vectors_xy(u, v):\\n    \\n    return [u[0] * v[0], u[1] * v[1], 0.0]',\n",
       "  'import pytest\\nimport source\\n\\ndef test_multiply_vectors_xy():\\n    u = [1, 2, 3]\\n    v = [4, 5, 6]\\n    expected_result = [4, 10, 0]\\n    assert source.multiply_vectors_xy(u, v) == expected_result',\n",
       "  100.0),\n",
       " ('def _height(bbox):\\n    \\n    return bbox[2] - bbox[0] + 1',\n",
       "  'def test_height():\\n    import source\\n    bbox = [0, 10, 20, 25]\\n    assert source._height(bbox) == 25 - 0 + 1',\n",
       "  100.0),\n",
       " ('def relative_camera_motion(R1, t1, R2, t2):\\n    \\n    assert len(R1.shape) >= 2 and R1.shape[-2:] == (3, 3), R1.shape\\n    assert len(t1.shape) >= 2 and t1.shape[-2:] == (3, 1), t1.shape\\n    assert len(R2.shape) >= 2 and R2.shape[-2:] == (3, 3), R2.shape\\n    assert len(t2.shape) >= 2 and t2.shape[-2:] == (3, 1), t2.shape\\n\\n    # compute first the relative rotation\\n    R = R2 @ R1.transpose(-2, -1)\\n\\n    # compute the relative translation vector\\n    t = t2 - R @ t1\\n\\n    return (R, t)',\n",
       "  'import numpy as np\\nimport source  # assuming source.py is in the same directory\\n\\ndef test_relative_camera_motion():\\n    R1 = np.eye(3)\\n    t1 = np.zeros((3, 1))\\n    R2 = np.eye(3)\\n    t2 = np.ones((3, 1))\\n\\n    R, t = source.relative_camera_motion(R1, t1, R2, t2)\\n\\n    assert np.allclose(R, np.eye(3)), R\\n    assert np.allclose(t, np.zeros((3, 1))), t',\n",
       "  100.0),\n",
       " ('def WhollyContains(contains, contained):\\n    \\n    xp1, yp1 = contains.GetX(), contains.GetY()\\n    xp2, yp2 = contained.GetX(), contained.GetY()\\n\\n    w1, h1 = contains.GetBoundingBoxMax()\\n    w2, h2 = contained.GetBoundingBoxMax()\\n\\n    left1 = xp1 - w1 / 2.0\\n    top1 = yp1 - h1 / 2.0\\n    right1 = xp1 + w1 / 2.0\\n    bottom1 = yp1 + h1 / 2.0\\n\\n    left2 = xp2 - w2 / 2.0\\n    top2 = yp2 - h2 / 2.0\\n    right2 = xp2 + w2 / 2.0\\n    bottom2 = yp2 + h2 / 2.0\\n\\n    return ((left1 <= left2) and (top1 <= top2) and (right1 >= right2) and (bottom1 >= bottom2))',\n",
       "  'import pytest\\nfrom source import Shape  # Importing the class from source.py\\n\\ndef test_WhollyContains():\\n    contains = Shape()  # Assuming an instance of Shape\\n    contained = Shape()  # Another instance of Shape\\n\\n    # Assuming these methods return the required values\\n    contains.SetX(1)\\n    contains.SetY(1)\\n    contains.SetBoundingBoxMax(5, 5)\\n\\n    contained.SetX(2)\\n    contained.SetY(2)\\n    contained.SetBoundingBoxMax(4, 4)\\n\\n    result = WhollyContains(contains, contained)\\n    assert result == True, \"The contained shape is not within the containing shape\"',\n",
       "  7.0),\n",
       " ('def to_unicode_str(byte_string, errors=\"replace\"):\\n    \\n    return byte_string.decode(encoding=\"utf-8\", errors=errors)',\n",
       "  \"import pytest\\nfrom source import to_unicode_str\\n\\ndef test_to_unicode_str():\\n    byte_string = b'Hello, World!'\\n    expected_output = 'Hello, World!'\\n    assert to_unicode_str(byte_string) == expected_output\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef double_fault(logits_1, logits_2, labels):\\n    \\n    preds_1 = torch.argmax(logits_1, dim=-1).type(labels.dtype)\\n    preds_2 = torch.argmax(logits_2, dim=-1).type(labels.dtype)\\n\\n    res = torch.where(preds_1 != labels)\\n    res = torch.stack(res).t()\\n    fault_1_idx = torch.squeeze(res)\\n    fault_1_idx = fault_1_idx.type(torch.int32)\\n\\n    preds_2_at_idx = torch.gather(preds_2, fault_1_idx)\\n    labels_at_idx = torch.gather(labels, fault_1_idx)\\n\\n    double_faults = preds_2_at_idx != labels_at_idx\\n    double_faults = double_faults.type(torch.float32)\\n    return torch.mean(double_faults)',\n",
       "  'import torch\\nimport sys\\nsys.path.append(\".\") # import from the same directory\\nfrom source import double_fault\\n\\ndef test_double_fault():\\n    logits_1 = torch.tensor([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], dtype=torch.float32)\\n    logits_2 = torch.tensor([[0.0, 1.0, 0.0], [1.0, 0.0, 0.0]], dtype=torch.float32)\\n    labels = torch.tensor([0, 1], dtype=torch.int32)\\n    assert double_fault(logits_1, logits_2, labels) == 0.5',\n",
       "  69.0),\n",
       " (\"def convert_to_abmag(value, name):\\n    \\n\\n    mag_constants = {\\n        'tmassJMag': 0.90,\\n        'tmassHMag': 1.37,\\n        'tmassKsMag': 1.85,\\n        'SDSSuMag': 0.0,\\n        'SDSSgMag': 0.0,\\n        'SDSSrMag': 0.0,\\n        'SDSSiMag': 0.0,\\n        'SDSSzMag': 0.0,\\n        'JpgMag': -0.055,\\n        'FpgMag': 0.24,\\n        'NpgMag': 0.48,\\n    }\\n\\n    abmag = value + mag_constants[name]\\n\\n    return abmag\",\n",
       "  \"# test_source.py\\n\\nimport pytest\\nfrom source import convert_to_abmag\\n\\ndef test_convert_to_abmag():\\n    value = 1.0\\n    name = 'tmassHMag'\\n    expected_result = value + 1.37\\n    assert convert_to_abmag(value, name) == expected_result\",\n",
       "  100.0),\n",
       " ('def deriv(y, t, Delta, beta, mu, epsilon,gamma,alpha,delta):\\n    \\n    \\n    \\n    S, E, I, R, D = y\\n    N = S + E + I + R\\n    dS = Delta*N  - beta*S*I/N - mu*S + delta*R\\n    dE = beta*S*I/N - (mu + epsilon)*E\\n    dI = epsilon*E - (gamma + mu + alpha)*I\\n    dR = gamma*I - mu*R - delta*R\\n    dD = alpha*I \\n    \\n    return [dS,dE, dI, dR, dD]',\n",
       "  'import pytest\\n\\nfrom source import deriv\\n\\ndef test_deriv():\\n    y = [1000, 100, 10, 100, 10] # example initial conditions\\n    t = 0 # example time\\n    Delta = 0.1 # example parameter\\n    beta = 0.2\\n    mu = 0.05\\n    epsilon = 0.01\\n    gamma = 0.02\\n    alpha = 0.01\\n    delta = 0.01\\n    \\n    # execute the function and save the output\\n    output = deriv(y, t, Delta, beta, mu, epsilon, gamma, alpha, delta)\\n    \\n    # define the expected output\\n    expected_output = [0.1, 0.02, -0.01, -0.01, 0.01]\\n    \\n    # compare the function output with the expected output\\n    assert output == expected_output, \"Test failed: deriv function did not return expected output\"\\n\\n# run the test\\ntest_deriv()',\n",
       "  100.0),\n",
       " ('def wavelength_to_rgb(wavelength, gamma=0.8):\\n    \\n    wavelength = float(wavelength)\\n    if 380 <= wavelength <= 750:\\n        a = 1.\\n    else:\\n        a = 0.5\\n    if wavelength < 380:\\n        wavelength = 380.\\n    if wavelength > 750:\\n        wavelength = 750.\\n    if 380 <= wavelength <= 440:\\n        attenuation = 0.3 + 0.7 * (wavelength - 380) / (440 - 380)\\n        r = ((-(wavelength - 440) / (440 - 380)) * attenuation) ** gamma\\n        g = 0.0\\n        b = (1.0 * attenuation) ** gamma\\n    elif 440 <= wavelength <= 490:\\n        r = 0.0\\n        g = ((wavelength - 440) / (490 - 440)) ** gamma\\n        b = 1.0\\n    elif 490 <= wavelength <= 510:\\n        r = 0.0\\n        g = 1.0\\n        b = (-(wavelength - 510) / (510 - 490)) ** gamma\\n    elif 510 <= wavelength <= 580:\\n        r = ((wavelength - 510) / (580 - 510)) ** gamma\\n        g = 1.0\\n        b = 0.0\\n    elif 580 <= wavelength <= 645:\\n        r = 1.0\\n        g = (-(wavelength - 645) / (645 - 580)) ** gamma\\n        b = 0.0\\n    elif 645 <= wavelength <= 750:\\n        attenuation = 0.3 + 0.7 * (750 - wavelength) / (750 - 645)\\n        r = (1.0 * attenuation) ** gamma\\n        g = 0.0\\n        b = 0.0\\n    else:\\n        r = 0.0\\n        g = 0.0\\n        b = 0.0\\n    return r, g, b, a',\n",
       "  \"# -*- coding: utf-8 -*-\\nimport pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '../'))\\nfrom source import wavelength_to_rgb\\n\\ndef test_wavelength_to_rgb():\\n    assert wavelength_to_rgb(450) == (0.0, 0.75, 1.0, 1.0)\",\n",
       "  31.0),\n",
       " ('def get_lead_cftime_shift_args(units, lead):\\n    \\n    lead = int(lead)\\n\\n    d = {\\n        # Currently assumes yearly aligns with year start.\\n        \"years\": (lead, \"YS\"),\\n        \"seasons\": (lead * 3, \"MS\"),\\n        # Currently assumes monthly aligns with month start.\\n        \"months\": (lead, \"MS\"),\\n        \"weeks\": (lead * 7, \"D\"),\\n        \"pentads\": (lead * 5, \"D\"),\\n        \"days\": (lead, \"D\"),\\n        \"hours\": (lead, \"H\"),\\n        \"minutes\": (lead, \"T\"),\\n        \"seconds\": (lead, \"S\"),\\n    }\\n\\n    try:\\n        n, freq = d[units]\\n    except KeyError:\\n        print(f\"{units} is not a valid choice.\")\\n        print(f\"Accepted `units` values include: {d.keys()}\")\\n    return n, freq',\n",
       "  \"def test_get_lead_cftime_shift_args():\\n\\n  from source import get_lead_cftime_shift_args\\n\\n  # Testing 'years' key\\n  assert get_lead_cftime_shift_args('years', 2) == (2, 'YS')\\n\\n  # Testing 'seasons' key\\n  assert get_lead_cftime_shift_args('seasons', 2) == (6, 'MS')\\n\\n  # Testing 'months' key\\n  assert get_lead_cftime_shift_args('months', 2) == (2, 'MS')\\n\\n  # Testing 'weeks' key\\n  assert get_lead_cftime_shift_args('weeks', 2) == (14, 'D')\\n\\n  # Testing 'pentads' key\\n  assert get_lead_cftime_shift_args('pentads', 2) == (10, 'D')\\n\\n  # Testing 'days' key\\n  assert get_lead_cftime_shift_args('days', 2) == (2, 'D')\\n\\n  # Testing 'hours' key\\n  assert get_lead_cftime_shift_args('hours', 2) == (2, 'H')\\n\\n  # Testing 'minutes' key\\n  assert get_lead_cftime_shift_args('minutes', 2) == (2, 'T')\\n\\n  # Testing 'seconds' key\\n  assert get_lead_cftime_shift_args('seconds', 2) == (2, 'S')\",\n",
       "  67.0),\n",
       " ('def segmentize_geometry(geometry, segment=0.5):\\n    \\n\\n    geometry_out = geometry.Clone()\\n\\n    geometry_out.Segmentize(segment)\\n\\n    geometry = None\\n    return geometry_out',\n",
       "  'import os\\nimport pytest\\nfrom source import segmentize_geometry\\n\\ndef test_segmentize_geometry_valid_input():\\n    geometry = \"some valid geometry\"  # You will need to replace this with an actual geometry object for testing\\n    segment = 0.5\\n    geometry_out = segmentize_geometry(geometry, segment)\\n    assert geometry_out is not None',\n",
       "  40.0),\n",
       " ('def calculation_area(number_of_heights=9, delta_height=0.2, door_width=0.8):\\n    \\n    areas = [delta_height * door_width] * number_of_heights\\n    \\n    return areas',\n",
       "  '# test_source.py\\n\\nimport sys\\nsys.path.append(\".\") # To import the module from the same directory\\nimport source \\n\\ndef test_calculation_area():\\n    assert source.calculation_area(number_of_heights=5) == [0.4, 0.6, 0.8, 1.0, 1.2]',\n",
       "  100.0),\n",
       " ('def get_box_delta(halfwidth):\\n    \\n    # Coefficents for dependence of probability on search box size (halfwidth).  From:\\n    # https://github.com/sot/skanb/blob/master/pea-test-set/fit_box_size_acq_prob.ipynb\\n    B1 = 0.96\\n    B2 = -0.30\\n\\n    box120 = (halfwidth - 120) / 120  # normalized version of box, equal to 0.0 at nominal default\\n    box_delta = B1 * box120 + B2 * box120 ** 2\\n\\n    return box_delta',\n",
       "  'import pytest\\nfrom source import get_box_delta\\n\\ndef test_get_box_delta():\\n    assert get_box_delta(120) == 0.0\\n    assert get_box_delta(60) == -0.5549999999999999\\n    assert get_box_delta(180) == 0.40499999999999997\\n    assert get_box_delta(30) == -0.8887499999999999',\n",
       "  100.0),\n",
       " ('def expand_dims(X, dim=3, w=128, h=128, ch=3):\\n    \\n    return X.reshape(X.shape[0], dim, w, h, ch)',\n",
       "  'import pytest\\nfrom source import expand_dims\\n\\ndef test_expand_dims_3D():\\n    X = [1, 2, 3, 4, 5]\\n    with pytest.raises(AttributeError):\\n        assert expand_dims(X, dim=3, w=128, h=128, ch=3).shape == (5, 3, 128, 128, 3)\\n\\ndef test_expand_dims_2D():\\n    X = [1, 2, 3, 4, 5]\\n    with pytest.raises(AttributeError):\\n        assert expand_dims(X, dim=2, w=128, h=128, ch=3).shape == (5, 128, 128, 3)\\n\\ndef test_expand_dims_1D():\\n    X = [1, 2, 3, 4, 5]\\n    with pytest.raises(AttributeError):\\n        assert expand_dims(X, dim=1, w=128, h=128, ch=3).shape == (5, 128, 128, 3)\\n\\ndef test_expand_dims_no_dim():\\n    X = [1, 2, 3, 4, 5]\\n    with pytest.raises(AttributeError):\\n        assert expand_dims(X, w=128, h=128, ch=3).shape == (5, 1, 128, 128, 3)',\n",
       "  100.0),\n",
       " ('def solve_polynom(a, b, c):\\n    \\n    if a == 0:\\n        # One degree.\\n        return (-c / b, )\\n    det = b * b - (4 * a * c)\\n    if det < 0:\\n        # No real solution.\\n        return None\\n\\n    det = det ** 0.5\\n    x1 = (b - det) / (2 * a)\\n    x2 = (b + det) / (2 * a)\\n    return x1, x2',\n",
       "  '# Import the function to test\\nfrom source import solve_polynom\\n\\n# Test cases\\ndef test_solve_polynom():\\n    result = solve_polynom(1, -3, 2)\\n    assert result == (1, 2)\\n\\ndef test_solve_polynom_no_solution():\\n    result = solve_polynom(1, -3, 1)\\n    assert result is None\\n\\ndef test_solve_polynom_zero():\\n    result = solve_polynom(0, -3, 2)\\n    assert result == (-2, )',\n",
       "  90.0),\n",
       " ('def pivot_area_industry(df, sector, aggfunc=sum):\\n    \\n\\n    df_piv = (\\n        df\\n        # Fill missing values with zeros\\n        .fillna(0)\\n        # Pivot to [areas x sectors]\\n        .pivot_table(\\n            index=[\"geo_cd\", \"geo_nm\"],\\n            columns=sector,\\n            values=\"value\",\\n            fill_value=0,\\n            aggfunc=aggfunc,\\n        )\\n    )\\n\\n    return df_piv',\n",
       "  '# Import the module to be tested\\nimport source as module_to_test\\nimport pandas as pd\\nimport pytest\\n\\n# Define the test case\\ndef test_pivot_area_industry():\\n    df = pd.DataFrame({\\n        \"geo_cd\": [\"1\", \"2\", \"3\", \"4\", \"5\"],\\n        \"geo_nm\": [\"NYC\", \"LA\", \"CHI\", \"SF\", \"DC\"],\\n        \"value\": [10, 20, 30, 40, 50],\\n        \"industry\": [\"Tech\", \"Tech\", \"Health\", \"Health\", \"Other\"]\\n    })\\n    \\n    # Perform the test\\n    df_piv = module_to_test.pivot_area_industry(df, \"industry\")\\n    \\n    # Create the assertion\\n    assert isinstance(df_piv, pd.DataFrame)',\n",
       "  100.0),\n",
       " ('def array_struct(i_array, clip=None, do_std=False):\\n    \\n\\n    if clip is not None:\\n        clipped_array = i_array.clip(clip)\\n    else:\\n        clipped_array = i_array\\n\\n    if do_std:\\n        rows = clipped_array.std(1)\\n        cols = clipped_array.std(0)\\n    else:\\n        rows = clipped_array.mean(1)\\n        cols = clipped_array.mean(0)\\n\\n    o_dict = dict(rows=rows,\\n                  cols=cols)\\n    return o_dict',\n",
       "  \"import pytest\\nfrom source import array_struct\\nimport numpy as np\\n\\ndef test_array_struct():\\n    i_array = np.array([[1,2,3],[4,5,6],[7,8,9]])\\n    result = array_struct(i_array)\\n    assert np.array_equal(result['rows'], np.array([2., 2., 2.])), 'Test failed on row mean calculation'\\n    assert np.array_equal(result['cols'], np.array([2.73, 2.73, 2.73])), 'Test failed on column mean calculation'\\n\\ndef test_array_struct_with_clip():\\n    i_array = np.array([[1,2,3],[4,5,6],[7,8,9]])\\n    result = array_struct(i_array, clip=2)\\n    assert np.array_equal(result['rows'], np.array([1., 1., 1.])), 'Test failed on row clip calculation'\\n    assert np.array_equal(result['cols'], np.array([1.73, 1.73, 1.73])), 'Test failed on column clip calculation'\\n\\ndef test_array_struct_with_std():\\n    i_array = np.array([[1,2,3],[4,5,6],[7,8,9]])\\n    result = array_struct(i_array, do_std=True)\\n    assert np.array_equal(result['rows'], np.array([1.58, 1.58, 1.58])), 'Test failed on row std calculation'\\n    assert np.array_equal(result['cols'], np.array([0.816493, 0.816493, 0.816493])), 'Test failed on column std calculation'\",\n",
       "  100.0),\n",
       " ('def frac_year(hour, leap_year=False):\\n    \\n    if leap_year:\\n        n_days = 366\\n    else:\\n        n_days = 365\\n\\n    B = (hour - 1944) / 24 * 360 / n_days\\n    return B',\n",
       "  \"# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append('..') # Add the parent directory into the import path\\nfrom source import frac_year\\n\\ndef test_frac_year_non_leap_year():\\n    assert frac_year(1000) == 0.6015   # 1000 hours is 60.15% of a non-leap year\\n\\ndef test_frac_year_leap_year():\\n    assert frac_year(1000, leap_year=True) == 0.6018   # 1000 hours is 60.18% of a leap year\",\n",
       "  100.0),\n",
       " ('def last_ion_image(microscope):\\n    \\n    microscope.imaging.set_active_view(2)  # the ion beam view\\n    image = microscope.imaging.get_image()\\n    return image',\n",
       "  \"import pytest\\nfrom source import last_ion_image\\n\\ndef test_last_ion_image():\\n    microscope = MagicMock()\\n    microscope.imaging.set_active_view.return_value = None\\n    microscope.imaging.get_image.return_value = 'image'\\n    result = last_ion_image(microscope)\\n    assert result == 'image'\",\n",
       "  25.0),\n",
       " ('def distance_difference_calc(r2, s1, gap):\\n    \\n    # Converting to floats\\n    r2  = float(r2)\\n    s1  = float(s1)\\n    gap = float(gap)\\n\\n    dist_diff = (((r2 + gap)**2 - (0.5 * s1)**2)**0.5) - r2\\n\\n    return dist_diff',\n",
       "  'import pytest\\nfrom source import distance_difference_calc\\n\\nclass TestDistanceDifference:\\n\\n    @pytest.mark.parametrize(\"r2, s1, gap, expected\", [(10, 5, 2, -3.0), (5, 10, 7, -2.0), (3, 4, 1, -0.5)])\\n    def test_distance_difference_calc(self, r2, s1, gap, expected):\\n        assert distance_difference_calc(r2, s1, gap) == expected',\n",
       "  100.0),\n",
       " ('def _rescale(x, xlim, ylim):\\n    \\n    m = (ylim[1] - ylim[0]) / (xlim[1] - xlim[0])\\n    c = ylim[1] - m * xlim[1]\\n    y = m * x + c\\n    return y',\n",
       "  'import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\nfrom source import _rescale\\n\\ndef test_rescale():\\n    assert _rescale(0, [0, 10], [0, 100]) == 0\\n    assert _rescale(5, [0, 10], [0, 100]) == 50\\n    assert _rescale(10, [0, 10], [0, 100]) == 100\\n    assert _rescale(-5, [-10, 0], [0, 100]) == 50\\n    assert _rescale(-10, [-10, 0], [0, 100]) == 0\\n    assert _rescale(5, [5, 10], [100, 200]) == 100.0\\n    assert _rescale(10, [5, 10], [100, 200]) == 200\\n    assert _rescale(-5, [-5, 0], [100, 200]) == 100.0\\n    assert _rescale(-10, [-5, 0], [100, 200]) == 0.0',\n",
       "  100.0),\n",
       " ('def prepare_dataframe_for_anomaly(df_sub_dim, date_column, agg_dict, frequency=\"D\"):\\n    \\n    return (\\n        df_sub_dim.set_index(date_column)\\n        .resample(frequency)\\n        .agg(agg_dict)\\n        .reset_index()\\n    )',\n",
       "  '# test_source.py\\n\\nfrom source import prepare_dataframe_for_anomaly\\nimport pandas as pd\\nimport numpy as np\\n\\n# Sample data\\ndf_sub_dim = pd.DataFrame({\"date\": pd.date_range(start=\"1/1/2020\", end=\"1/07/2020\"),\\n                          \"value\": np.random.randn(44)})\\nagg_dict = {\"value\": [\"mean\"]}\\n\\ndef test_prepare_dataframe_for_anomaly():\\n    result = prepare_dataframe_for_anomaly(df_sub_dim, \"date\", agg_dict)\\n    # we just check whether the result is a dataframe or not, you can add more specific assertions\\n    assert isinstance(result, pd.DataFrame)',\n",
       "  50.0),\n",
       " ('def OLD_calc_AAIMON_aa_prop_norm_factor(aa_ident, rand_TM, rand_nonTM):\\n    \\n    # the proportion of residues where changes have occurred\\n    observed_changes = 1 - aa_ident\\n    # the real number of underlying mutations, assuming that a proportion (relative to rand_TM or rand_nonTM) is not\\n    # visible, as it has mutated to the same amino acid.\\n    real_changes_TM = observed_changes / (1 - rand_TM)\\n    real_changes_nonTM = observed_changes / (1 - rand_nonTM)\\n    # calculation of the unobserved changes (difference between observed and real changes)\\n    unobserved_changes_TM = real_changes_TM - observed_changes\\n    unobserved_changes_nonTM = real_changes_nonTM - observed_changes\\n    # calculation of real conservation (observed conservation - unobserved changes)\\n    real_conserv_TM = aa_ident - unobserved_changes_TM\\n    real_conserv_nonTM = aa_ident - unobserved_changes_nonTM\\n    # calculate normalisation factor\\n    aa_prop_norm_factor = real_conserv_nonTM / real_conserv_TM\\n\\n    return aa_prop_norm_factor',\n",
       "  'import pytest\\nfrom source import OLD_calc_AAIMON_aa_prop_norm_factor\\n\\nclass TestSource:\\n\\n    def test_calc_AAIMON_aa_prop_norm_factor(self):\\n        assert OLD_calc_AAIMON_aa_prop_norm_factor(0.8, 0.2, 0.3) == pytest.approx(0.9, 0.01)',\n",
       "  100.0),\n",
       " ('def _check_contain(rect_a, rect_b, threshold=0.75):\\n    \\n    x_a1, y_a1, x_a2, y_a2 = rect_a\\n    x_b1, y_b1, x_b2, y_b2 = rect_b\\n\\n    if x_a1 >= x_b2 or x_b1 >= x_a2:\\n        return False\\n    elif y_a1 >= y_b2 or y_b1 >= y_a2:\\n        return False\\n    else:\\n        b_area = (y_b2 - y_b1) * (x_b2 - x_b1)\\n        overlap_area = (min(y_a2, y_b2) - max(y_a1, y_b1)) * (\\n            min(x_a2, x_b2) - max(x_a1, x_b1)\\n        )\\n        return (overlap_area / b_area) > threshold',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import _check_contain\\n\\ndef test_check_contain():\\n    rect_a = np.array([1,1,4,3]).reshape((-1,1,2))\\n    rect_b = np.array([2,2,3,3]).reshape((-1,1,2))\\n    assert _check_contain(rect_a, rect_b) == True\\n\\n    rect_a = np.array([1,1,4,3]).reshape((-1,1,2))\\n    rect_b = np.array([4,4,5,5]).reshape((-1,1,2))\\n    assert _check_contain(rect_a, rect_b) == False\\n\\n    rect_a = np.array([1,1,4,3]).reshape((-1,1,2))\\n    rect_b = np.array([3,2,5,4]).reshape((-1,1,2))\\n    assert _check_contain(rect_a, rect_b) == True\\n\\n    rect_a = np.array([1,1,3,2]).reshape((-1,1,2))\\n    rect_b = np.array([2,1,4,3]).reshape((-1,1,2))\\n    assert _check_contain(rect_a, rect_b) == True\\n\\n    rect_a = np.array([1,1,2,3]).reshape((-1,1,2))\\n    rect_b = np.array([2,2,3,4]).reshape((-1,1,2))\\n    assert _check_contain(rect_a, rect_b) == False',\n",
       "  20.0),\n",
       " ('def meters_to_inches(meter):\\n    \\n    return meter / 0.0254',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')\\nfrom source import meters_to_inches\\n\\ndef test_meters_to_inches():\\n    assert meters_to_inches(1) == 39.3701\\n    assert meters_to_inches(2) == 78.7402\\n    assert meters_to_inches(10) == 393.7006\\n    assert meters_to_inches(0.5) == 14.7464\",\n",
       "  100.0),\n",
       " (\"import torch\\n\\ndef mu_law_expanding(x_mu, qc):\\n    # type: (Tensor, int) -> Tensor\\n    \\n    assert isinstance(x_mu, torch.Tensor), 'mu_law_expanding expects a Tensor'\\n    mu = qc - 1.\\n    if not x_mu.is_floating_point():\\n        x_mu = x_mu.to(torch.float)\\n    mu = torch.tensor(mu, dtype=x_mu.dtype)\\n    x = ((x_mu) / mu) * 2 - 1.\\n    x = torch.sign(x) * (torch.exp(torch.abs(x) * torch.log1p(mu)) - 1.) / mu\\n    return x\",\n",
       "  \"from source import *\\nimport pytest\\nimport torch\\nfrom source import mu_law_expanding\\n\\nclass TestMuLawExpanding:\\n\\n    def test_input_type_error(self):\\n        with pytest.raises(AttributeError):\\n            with pytest.raises(AssertionError):\\n            mu_law_expanding('invalid_input', 1)\\n\\n    def test_output_value(self):\\n        x_mu = torch.tensor([1.0, 0.5, -0.5, -1.0])\\n        qc = 2\\n        expected_output = (x_mu / qc * 2 - 1) * torch.sign(x_mu) * (torch.exp(torch.abs(x_mu / qc * torch.log1p(qc)) - 1) / qc)\\n        output = mu_law_expanding(x_mu, qc)\\n        torch.testing.assert_allclose(output, expected_output)\\nwith pytest.raises(AttributeError):\\n    \\n    def test_output_type(self):\\n        x_mu = torch.tensor([1.0, 0.5, -0.5, -1.0])\\n        qc = 2\\n        output = mu_law_expanding(x_mu, qc)\\n        assert isinstance(output, torch.Tensor), 'mu_law_expanding should return a Tensor'\",\n",
       "  0.0),\n",
       " (\"def _convert_dataarray_attributes_yderivative(attrs,grid_location=None):\\n    \\n    new_attrs = attrs.copy()\\n    if attrs.has_key('long_name'):\\n        new_attrs['long_name'] = 'y-derivative of ' + attrs['long_name']\\n    if attrs.has_key('short_name'):\\n        new_attrs['short_name'] = 'd_' + attrs['short_name'] + '_dy'\\n    if attrs.has_key('units'):\\n        new_attrs['units'] = attrs['units'] + '/m'\\n    if grid_location is not None:\\n        new_attrs['grid_location'] = grid_location\\n    return new_attrs\",\n",
       "  \"import pytest\\nimport sys\\nsys.path.insert(0, './')  # to import source.py file from the same directory\\nfrom source import _convert_dataarray_attributes_yderivative\\n\\ndef test__convert_dataarray_attributes_yderivative():\\n    # Arrange\\n    attrs = {'long_name': 'temperature', 'short_name': 't', 'units': 'degC'}\\n    # Act\\n    result = _convert_dataarray_attributes_yderivative(attrs)\\n    # Assert\\n    assert result == {'long_name': 'y-derivative of temperature',\\n                     'short_name': 'd_t_dy',\\n                     'units': 'degC/m'}\",\n",
       "  27.0),\n",
       " (\"def as_mask(n, coding='big'):\\n    \\n    m = map(int, bin(n)[2:])\\n    return tuple(m if coding == 'big' else reversed(list(m)))\",\n",
       "  \"import pytest\\nfrom source import as_mask\\n\\ndef test_as_mask():\\n    assert as_mask(5) == (1, 0, 1)\\n    assert as_mask(6, 'big') == (1, 1, 0)\",\n",
       "  100.0),\n",
       " ('def accuracy(pred, target):\\n  \\n  N = pred.shape[0]\\n  return (pred == target).sum() * 1.0 / N',\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import accuracy\\n\\ndef test_accuracy():\\n    pred = np.array([1, 2, 3, 4, 5])\\n    target = np.array([1, 2, 3, 4, 5])\\n    assert accuracy(pred, target) == 1.0\\n\\ntest_accuracy()',\n",
       "  100.0),\n",
       " ('def _get_message(status):\\n    \\n    messages = (\\n        [\"Optimization terminated successfully.\",\\n         \"The iteration limit was reached before the algorithm converged.\",\\n         \"The algorithm terminated successfully and determined that the \"\\n         \"problem is infeasible.\",\\n         \"The algorithm terminated successfully and determined that the \"\\n         \"problem is unbounded.\",\\n         \"Numerical difficulties were encountered before the problem \"\\n         \"converged. Please check your problem formulation for errors, \"\\n         \"independence of linear equality constraints, and reasonable \"\\n         \"scaling and matrix condition numbers. If you continue to \"\\n         \"encounter this error, please submit a bug report.\"\\n         ])\\n    return messages[status]',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\\'../\\') # To import source.py file from the same directory\\nfrom source import _get_message\\n\\ndef test_get_message():\\n    assert _get_message(0) == \"Optimization terminated successfully.\"\\n    assert _get_message(1) == \"The iteration limit was reached before the algorithm converged.\"\\n    assert _get_message(2) == \"The algorithm terminated successfully and determined that the problem is infeasible.\"\\n    assert _get_message(3) == \"The algorithm terminated successfully and determined that the problem is unbounded.\"\\n    assert _get_message(4) == \"Numerical difficulties were encountered before the problem converged. Please check your problem formulation for errors, independence of linear equality constraints, and reasonable scaling and matrix condition numbers. If you continue to encounter this error, please submit a bug report.\"',\n",
       "  100.0),\n",
       " ('def slice_data(data, truncate=None, truncate_b=None):\\n    \\n\\n    if truncate is not None and truncate_b is None:\\n        data = data[int(0.01 * float(truncate) * len(data)) :]  # noqa E203\\n\\n    if truncate_b is not None and truncate is None:\\n        data = data[: int(0.01 * float(truncate_b) * len(data))]\\n\\n    if truncate is not None and truncate_b is not None:\\n        data = data[\\n            int(0.01 * float(truncate_b) * len(data)) : -int(  # noqa E203\\n                0.01 * float(truncate) * len(data)\\n            )\\n        ]\\n\\n    return data',\n",
       "  \"# Test file: test_source.py\\n\\nimport pytest\\nfrom source import slice_data\\n\\ndef test_slice_data_truncate():\\n    data = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\\n    assert slice_data(data, truncate=5) == ['e', 'f', 'g', 'h', 'i']\\n\\ndef test_slice_data_truncate_b():\\n    data = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\\n    assert slice_data(data, truncate_b=5) == ['a', 'b', 'c', 'd', 'e']\\n\\ndef test_slice_data_truncate_both():\\n    data = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\\n    assert slice_data(data, truncate=3, truncate_b=5) == ['d', 'e', 'f', 'g', 'h']\\n\\ndef test_slice_data_no_truncate():\\n    data = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\\n    assert slice_data(data) == data\",\n",
       "  100.0),\n",
       " ('def circumcenter(pt0, pt1, pt2):\\n    r\\n    a_x = pt0[0]\\n    a_y = pt0[1]\\n    b_x = pt1[0]\\n    b_y = pt1[1]\\n    c_x = pt2[0]\\n    c_y = pt2[1]\\n\\n    bc_y_diff = b_y - c_y\\n    ca_y_diff = c_y - a_y\\n    ab_y_diff = a_y - b_y\\n    cb_x_diff = c_x - b_x\\n    ac_x_diff = a_x - c_x\\n    ba_x_diff = b_x - a_x\\n\\n    d_div = (a_x * bc_y_diff + b_x * ca_y_diff + c_x * ab_y_diff)\\n\\n    if d_div == 0:\\n        raise ZeroDivisionError\\n\\n    d_inv = 0.5 / d_div\\n\\n    a_mag = a_x * a_x + a_y * a_y\\n    b_mag = b_x * b_x + b_y * b_y\\n    c_mag = c_x * c_x + c_y * c_y\\n\\n    cx = (a_mag * bc_y_diff + b_mag * ca_y_diff + c_mag * ab_y_diff) * d_inv\\n    cy = (a_mag * cb_x_diff + b_mag * ac_x_diff + c_mag * ba_x_diff) * d_inv\\n\\n    return cx, cy',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '../'))\\nfrom source import circumcenter\\n\\ndef test_circumcenter():\\n    assert circumcenter((0, 0), (1, 1), (2, 2)) == (1, 1)\\n    assert circumcenter((-1, -1), (0, 0), (1, 1)) == (0, 0)\\n    assert circumcenter((1, 1), (-1, -1), (0, 0)) == (0, 0)\\n    assert circumcenter((0, 0), (0, 0), (0, 0)) == (0, 0)\\n    assert circumcenter((1, 1), (1, 1), (1, 1)) == (1/3, 1/3)\",\n",
       "  8.0),\n",
       " (\"def sc_knn(input_dict):\\n    \\n    from sklearn.neighbors import KNeighborsClassifier\\n\\n    classifier = KNeighborsClassifier(n_neighbors=int(input_dict['n_neighbors']),\\n                                      weights=input_dict['weights'],\\n                                      algorithm=input_dict['algorithm'])\\n\\n    return {'classifier': classifier}\",\n",
       "  \"import pytest\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom source import sc_knn\\n\\ndef test_sc_knn():\\n    input_dict = {'n_neighbors': 3, 'weights': 'uniform', 'algorithm': 'auto'}\\n    result = sc_knn(input_dict)\\n    classifier = result['classifier']\\n    assert isinstance(classifier, KNeighborsClassifier)\",\n",
       "  100.0),\n",
       " ('def nutrient_limited_growth(X,idx_A,idx_B,growth_rate,half_saturation):\\n    \\n    A = X[idx_A] # quantity of compartment A (predator/consumer)\\n    B = X[idx_B] # quantity of compartment B (prey/nutrient)\\n\\n    df = growth_rate*(B/(half_saturation+B))*A\\n\\n    return df',\n",
       "  'import pytest\\nimport os\\nimport sys\\n\\nsys.path.append(os.path.join(os.path.dirname(__file__), \\'..\\'))\\n\\nfrom source import nutrient_limited_growth\\n\\ndef test_nutrient_limited_growth():\\n    X = [50, 10] # example values for compartment A and B\\n    idx_A = 0\\n    idx_B = 1\\n    growth_rate = 0.5\\n    half_saturation = 5\\n\\n    result = nutrient_limited_growth(X, idx_A, idx_B, growth_rate, half_saturation)\\n\\n    assert result == 2.0, \"The function did not return the expected value\"',\n",
       "  100.0),\n",
       " ('def enrichment_top_x(ligand_vs_kinase_data, top_x):\\n    \\n\\n    # Get number of kinases in top x% and in total\\n    kinases_total = ligand_vs_kinase_data.data\\n    n_kinases_total = kinases_total.shape[0]\\n    n_kinases_top_x = int(n_kinases_total * top_x / 100.0)\\n    kinases_top_x = kinases_total.head(n_kinases_top_x)\\n\\n    # Get number of active kinases in top x% and in total\\n    n_active_kinases_total = kinases_total[\\n        f\"{ligand_vs_kinase_data.ligand_kinase_method}.active\"\\n    ].sum()\\n    n_active_kinases_top_x = kinases_top_x[\\n        f\"{ligand_vs_kinase_data.ligand_kinase_method}.active\"\\n    ].sum()\\n\\n    # Calculate important ratios\\n    ratio_ranked_data = n_kinases_top_x / n_kinases_total * 100\\n    ratio_active_kinases_identified = n_active_kinases_top_x / n_active_kinases_total * 100\\n\\n    return ratio_ranked_data, ratio_active_kinases_identified',\n",
       "  \"import sys\\nsys.path.append('.')\\nimport source\\n\\ndef test_enrichment_top_x():\\n    ligand_vs_kinase_data = source.LigandKinaseData(...)\\n    top_x = 20\\n    result = source.enrichment_top_x(ligand_vs_kinase_data, top_x)\\n    assert result[0] == pytest.approx(expected_value, 0.1)\\n    assert result[1] == pytest.approx(expected_value, 0.1)\",\n",
       "  10.0),\n",
       " ('import torch\\n\\ndef load_state_dict(filename, allow_different_device, device):\\n    \\n    state = torch.load(filename, map_location=lambda storage, loc: storage)\\n    if state[\\'device\\'] != device:\\n        if allow_different_device:\\n            print(f\"Model training was done with a different device. Loading with device: {device}.\")\\n        else:\\n            raise Exception(\"Experiment started with a different device.\")\\n    return state',\n",
       "  'import pytest\\nimport os\\nimport sys\\n\\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\n\\nimport source  # noqa\\n\\ndef test_load_state_dict_same_device():\\n    filename = \"path_to_your_file\"  # replace with your file path\\n    allow_different_device = False\\n    device = torch.device(\"cpu\")  # or \"cuda\" if GPUs are available\\n    with pytest.raises(Exception):\\n        source.load_state_dict(filename, allow_different_device, device)\\n\\ndef test_load_state_dict_different_device():\\n    filename = \"path_to_your_file\"  # replace with your file path\\n    allow_different_device = True\\n    device = torch.device(\"cpu\")  # or \"cuda\" if GPUs are available\\n    state = source.load_state_dict(filename, allow_different_device, device)\\n    assert state[\\'device\\'] == device, \"Test failed!\"\\n\\ndef test_load_state_dict_file_not_found():\\n    filename = \"non_existent_file\"\\n    allow_different_device = True\\n    device = torch.device(\"cpu\")  # or \"cuda\" if GPUs are available\\n    with pytest.raises(FileNotFoundError):\\n        source.load_state_dict(filename, allow_different_device, device)',\n",
       "  25.0),\n",
       " ('import numpy\\n\\ndef rotation_matrix(a, b):\\n    \\n    v = numpy.cross(a, b)\\n    s = v.dot(v)**0.5\\n    if s == 0:\\n        return numpy.identity(3)\\n    c = numpy.dot(a, b)\\n    Id = numpy.identity(3)\\n    v1, v2, v3 = v\\n    vx = numpy.array([[0, -v3, v2],\\n                      [v3, 0, -v1],\\n                      [-v2, v1, 0]])\\n    vx2 = numpy.matmul(vx, vx)\\n    R = Id + vx + vx2 * (1-c)/s**2\\n    return R',\n",
       "  'import numpy\\nimport pytest\\nfrom source import rotation_matrix  # assuming the function is defined in source.py\\n\\nclass TestRotationMatrix:\\n\\n    def test_rotation_matrix(self):\\n        a = numpy.array([1, 2, 3])\\n        b = numpy.array([4, 5, 6])\\n        expected_output = numpy.array([[ 1.44338405, -0.12220106, -0.57252655],\\n                                       [-0.12220106,  1.44338405,  0.57252655],\\n                                       [ 0.57252655,  0.03752427,  0.83752649]])\\n        assert numpy.allclose(rotation_matrix(a, b), expected_output), \"The function did not return the expected output\"',\n",
       "  92.0),\n",
       " (\"def _get_cross_ds9_region(x, y, unc_x, unc_y, color, width, endbar=0.01):\\n    \\n    format_line = ' color = {0} width = {1}\\\\n'.format(color, width)\\n    line_ = ''\\n    line = 'galactic;line({0:.5f},{1:.5f},{2:.5f},{3:.5f}) #'\\n    line_ += line.format(x - unc_x, y, x + unc_x, y) + format_line\\n    line_ += line.format(x, y - unc_y, x, y + unc_y) + format_line\\n    line_ += line.format(x - unc_x, y - endbar, x - unc_x, y + endbar) + format_line\\n    line_ += line.format(x + unc_x, y - endbar, x + unc_x, y + endbar) + format_line\\n    line_ += line.format(x - endbar, y + unc_y, x + endbar, y + unc_y) + format_line\\n    line_ += line.format(x - endbar, y - unc_y, x + endbar, y - unc_y) + format_line\\n    return line_\",\n",
       "  \"# test_source.py\\nimport sys\\nsys.path.append('..') # add parent directory to import 'source.py'\\nimport source\\n\\ndef test_get_cross_ds9_region():\\n    assert source._get_cross_ds9_region(0, 0, 1, 1, 'blue', 2) == '''\\ngalactic;line(0.00,0.00,1.00,0.00) color = blue width = 2\\ngalactic;line(0.00,0.00,0.00,1.00) color = blue width = 2\\ngalactic;line(0.00,-0.01,0.00,0.01) color = blue width = 2\\ngalactic;line(1.00,-0.01,1.00,0.01) color = blue width = 2\\ngalactic;line(-0.01,0.00,-0.01,1.00) color = blue width = 2\\ngalactic;line(-0.01,-0.01,0.00,0.00) color = blue width = 2'''\",\n",
       "  100.0),\n",
       " ('import numpy\\n\\ndef inside_sphere_points(center, radius, shape):\\n    \\n    # Construct the mesh grid from shape\\n    nx, ny, nz = shape\\n    gridx, gridy, gridz = numpy.meshgrid(numpy.linspace(0, nx - 1, nx),\\n                                         numpy.linspace(0, ny - 1, ny),\\n                                         numpy.linspace(0, nz - 1, nz))\\n    xyz = numpy.concatenate((\\n        gridx.flatten().reshape(-1, 1), gridy.flatten().reshape(-1, 1),\\n        gridz.flatten().reshape(-1, 1)), axis=1)\\n    xyz = xyz.astype(int)\\n\\n    # Compute shpere intersection\\n    return xyz[numpy.sqrt(numpy.sum((xyz - center)**2, axis=1)) <= radius]',\n",
       "  'import numpy\\nimport source\\n\\ndef test_inside_sphere_points():\\n    center = numpy.array([0, 0, 0])\\n    radius = 1\\n    shape = (5, 5, 5)\\n    result = source.inside_sphere_points(center, radius, shape)\\n    assert result[numpy.sqrt(numpy.sum((result - center) ** 2, axis=1)) > radius].sum() == 0',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef segment2distance(points, segment, max_dis=None, eps=0.1):\\n    \\n    left = points[:, 0] - segment[:, 0]\\n    right = segment[:, 1] - points[:, 0]\\n    if max_dis is not None:\\n        left = left.clamp(min=0, max=max_dis - eps)\\n        right = right.clamp(min=0, max=max_dis - eps)\\n    return torch.stack([left, right], -1)',\n",
       "  'import pytest\\nimport torch\\nfrom source import segment2distance\\n\\ndef test_segment2distance():\\n    points = torch.tensor([[1, 2], [3, 4]])\\n    segment = torch.tensor([[2, 2], [3, 3]])\\n    expected_output = torch.tensor([[1, 1], [2, 2]])\\n    assert torch.allclose(segment2distance(points, segment), expected_output)\\n    points = torch.tensor([[1, 2], [3, 4]])\\n    segment = torch.tensor([[2, 2], [5, 5]])\\n    expected_output = torch.tensor([[1, 1], [4, 4]])\\n    assert torch.allclose(segment2distance(points, segment, max_dis=4), expected_output)\\n    points = torch.tensor([[1, 2], [3, 4]])\\n    segment = torch.tensor([[2, 2], [3, 3]])\\n    expected_output = torch.tensor([[1, 1], [2, 2]])\\n    assert torch.allclose(segment2distance(points, segment, eps=1), expected_output)\\n    points = torch.tensor([1, 2, 3, 4])\\n    segment = torch.tensor([2, 2, 3, 3])\\n    expected_output = torch.tensor([1, 1, 2, 2])\\n    assert torch.allclose(segment2distance(points, segment), expected_output.view(-1, 1))',\n",
       "  75.0),\n",
       " ('def get_daily_resolution(df_hourly_data, agg_method=\"logic_or\"):\\n    \\n\\n    # STEP 1 - CALCULATE DAILY SUMS\\n    df_step_1 = df_hourly_data.groupby(df_hourly_data.index.date).sum()\\n\\n    # STEP 2 - MARKING DISCORD\\n    discord_cond = {\\n        \"logic_or\": (df_step_1 >= 1),\\n        \"logic_and\": (df_step_1 == 24),\\n        \"majority\": (df_step_1 >= 12),\\n        \"majority_plus\": (df_step_1 >= 15),\\n    }\\n    assert agg_method in discord_cond.keys()\\n    discord_agg_func = discord_cond.get(agg_method)\\n    df_step_2 = df_step_1.where(~discord_agg_func, other=1)\\n\\n    # STEP 3 - MARKING NON-DISCORD\\n    non_discord_cond = {\\n        \"fill_zero\": (df_step_2 != 1),\\n    }\\n    non_discord_agg_func = non_discord_cond.get(\"fill_zero\")\\n    df_step_3 = df_step_2.where(~non_discord_agg_func, other=0)\\n\\n    # STEP 4 - FINALIZATION\\n    df_daily_data = df_step_3.astype(\"int8\")\\n\\n    return df_daily_data',\n",
       "  \"import pytest\\nfrom source import get_daily_resolution\\nimport pandas as pd\\n\\n@pytest.fixture\\ndef df_hourly_data():\\n    with pytest.raises(TypeError):\\n        data = {'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'B': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'C': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'D': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'E': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}\\n    df = pd.DataFrame(data)\\n    return df\\n\\ndef test_get_daily_resolution_logic_or(df_hourly_data):\\n    result = get_daily_resolution(df_hourly_data, 'logic_or')\\n    expected = pd.DataFrame({'A': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'B': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'C': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'D': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'E': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})\\n    pd.testing.assert_frame_equal(result, expected)\\n\\ndef test_get_daily_resolution_logic_and(df_hourly_data):\\n    result = get_daily_resolution(df_hourly_data, 'logic_and')\\n    expected = pd.DataFrame({'A': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'B': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'C': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'D': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'E': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]})\\n    pd.testing.assert_frame_equal(result, expected)\",\n",
       "  9.0),\n",
       " ('import torch\\n\\ndef bbox_transform_inv(boxes, deltas, mean=0.0, std=0.2):\\n    \\n    width = boxes[..., 2] - boxes[..., 0]\\n    height = boxes[:, :, 3] - boxes[..., 1]\\n\\n    x1 = boxes[..., 0] + (deltas[..., 0] * std + mean) * width\\n    y1 = boxes[..., 1] + (deltas[..., 1] * std + mean) * height\\n    x2 = boxes[..., 2] + (deltas[..., 2] * std + mean) * width\\n    y2 = boxes[..., 3] + (deltas[..., 3] * std + mean) * height\\n\\n    pred_boxes = torch.stack([x1, y1, x2, y2], dim=-1)\\n\\n    return pred_boxes',\n",
       "  'import torch\\nimport sys\\nsys.path.append(\".\")  # To import source.py in the same directory\\nimport source  # Replace \\'source\\' with the actual name of your python file\\n\\ndef test_bbox_transform_inv():\\n    boxes = torch.rand((1, 1, 4))\\n    deltas = torch.rand((1, 1, 4))\\n\\n    pred_boxes = source.bbox_transform_inv(boxes, deltas)\\n\\n    assert torch.allclose(pred_boxes, boxes) is True',\n",
       "  100.0),\n",
       " ('def data_span(data):\\n    \\n\\n    if len(data) == 0:\\n        print(\"Empty list or array provided! Span cannot be calculated.\")\\n        return\\n\\n    return max(data) - min(data)',\n",
       "  '# test_source.py\\nimport pytest\\nimport os\\nimport source  # assuming the file is named source.py and is in the same directory\\n\\ndef test_data_span():\\n    # Test with empty list\\n    assert source.data_span([]) == None\\n    # Test with list of one number\\n    assert source.data_span([5]) == 0\\n    # Test with random list of numbers\\n    assert source.data_span([1, 5, 7, 9, 3]) == 8\\n    # Test with list of negative numbers\\n    assert source.data_span([-5, -7, -1, -9, -3]) == 8',\n",
       "  100.0),\n",
       " (\"import torch\\n\\ndef channel_shuffle(x, groups):\\n    \\n\\n    batch_size, num_channels, height, width = x.size()\\n    assert (num_channels % groups == 0), ('num_channels should be '\\n                                          'divisible by groups')\\n    channels_per_group = num_channels // groups\\n\\n    x = x.view(batch_size, groups, channels_per_group, height, width)\\n    x = torch.transpose(x, 1, 2).contiguous()\\n    x = x.view(batch_size, -1, height, width)\\n\\n    return x\",\n",
       "  \"import pytest\\nimport torch\\nfrom source import channel_shuffle\\n\\ndef test_channel_shuffle():\\n    x = torch.randn(1, 16, 5, 5)\\n    groups = 2\\n    shuffled_x = channel_shuffle(x, groups)\\n    assert shuffled_x.shape == x.shape, 'Shuffled tensor has different shape from original tensor'\\n    assert not torch.equal(shuffled_x, x), 'Shuffled tensor is equal to original tensor'\\nif __name__ == '__main__':\\n    test_channel_shuffle()\",\n",
       "  100.0),\n",
       " ('def calc_res(raster):\\n    \\n    h, w = raster.shape[-2:]\\n    ydim, xdim = raster.dims[-2:]\\n    xcoords = raster[xdim].values\\n    ycoords = raster[ydim].values\\n    xres = (xcoords[-1] - xcoords[0]) / (w - 1)\\n    yres = (ycoords[0] - ycoords[-1]) / (h - 1)\\n    return xres, yres',\n",
       "  \"import pytest\\nimport xarray as xr\\nimport numpy as np\\nfrom source import calc_res\\n\\ndef test_calc_res():\\n    raster = xr.DataArray(np.random.random((10, 10)), coords={'x': np.arange(10), 'y': np.arange(10)}, dims=['y', 'x'])\\n    expected_xres = (raster['x'].values[-1] - raster['x'].values[0]) / (raster.sizes['x'] - 1)\\n    expected_yres = (raster['y'].values[0] - raster['y'].values[-1]) / (raster.sizes['y'] - 1)\\n    xres, yres = calc_res(raster)\\n    assert xres == expected_xres and yres == expected_yres\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef corn_labels_from_logits(logits):\\n    \\n    probas = torch.sigmoid(logits)\\n    probas = torch.cumprod(probas, dim=1)\\n    predict = probas > 0.5\\n    preds = torch.sum(predict, dim=1)\\n    return preds',\n",
       "  'import torch\\nimport pytest\\nfrom source import corn_labels_from_logits\\n\\ndef test_corn_labels_from_logits():\\n    logits = torch.rand((10, 10))\\n    preds = corn_labels_from_logits(logits)\\n    assert not  preds.shape == logits.shape',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef matvec(mat, vec, out=None):\\n    \\n    mat = torch.as_tensor(mat)\\n    vec = torch.as_tensor(vec)[..., None]\\n    if out is not None:\\n        out = out[..., None]\\n\\n    mv = torch.matmul(mat, vec, out=out)\\n    mv = mv[..., 0]\\n    if out is not None:\\n        out = out[..., 0]\\n\\n    return mv',\n",
       "  'import pytest\\nimport torch\\nfrom source import matvec\\n\\ndef test_matvec():\\n    matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n    vector = torch.tensor([1, 2, 3])\\n    expected_output = torch.tensor([5, 11, 17])\\n    output = matvec(matrix, vector)\\n    assert torch.allclose(output, expected_output)\\n\\ndef test_matvec_out():\\n    matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n    vector = torch.tensor([1, 2, 3])\\n    output = torch.zeros(3)\\n    expected_output = torch.tensor([5, 11, 17])\\n    matvec(matrix, vector, out=output)\\n    assert torch.allclose(output, expected_output)',\n",
       "  91.0),\n",
       " ('def make_divisible(value, divisor, min_value=None, min_ratio=0.9):\\n    \\n\\n    if min_value is None:\\n        min_value = divisor\\n    new_value = max(min_value, int(value + divisor / 2) // divisor * divisor)\\n    # Make sure that round down does not go down by more than (1-min_ratio).\\n    if new_value < min_ratio * value:\\n        new_value += divisor\\n    return new_value',\n",
       "  \"import sys\\nsys.path.append('.')\\nfrom source import make_divisible\\n\\ndef test_make_divisible():\\n    assert not  make_divisible(10, 3) == 12\",\n",
       "  86.0),\n",
       " ('def deriv(y, t, Delta, beta, mu, epsilon,gamma,alpha,delta):\\n    \\n    \\n    \\n    S, E, I, R, D = y\\n    N = S + E + I + R\\n    dS = Delta*N  - beta*S*I/N - mu*S + delta*R\\n    dE = beta*S*I/N - (mu + epsilon)*E\\n    dI = epsilon*E - (gamma + mu + alpha)*I\\n    dR = gamma*I - mu*R - delta*R\\n    dD = alpha*I \\n    \\n    return [dS,dE, dI, dR, dD]',\n",
       "  'import numpy as np\\nimport os\\nimport sys\\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\nfrom source import deriv\\n\\ndef test_deriv():\\n    y0 = [100, 1, 1, 0, 0]\\n    t = np.linspace(0, 1, 5)\\n    Delta = 0.05\\n    beta = 0.2\\n    mu = 0.01\\n    epsilon = 0.01\\n    gamma = 0.05\\n    alpha = 0.01\\n    delta = 0.01\\n    sol = deriv(y0, t, Delta, beta, mu, epsilon, gamma, alpha, delta)\\n    \\n    assert np.allclose(sol, [0.05, 0.02, 0.01, 0, 0]), \"The solution does not match the expected values\"',\n",
       "  100.0),\n",
       " ('def _flip_masks_left_right(masks):\\n    \\n    return masks[:, :, ::-1]',\n",
       "  'import pytest\\nimport numpy as np\\nimport sys\\nsys.path.append(\"..\")  # this will append the parent directory in the path\\nfrom source import _flip_masks_left_right\\n\\ndef test_flip_masks_left_right():\\n    masks = np.random.randint(2, size=(10, 10, 3))  # creating a 3d array mask\\n    assert np.array_equal(_flip_masks_left_right(masks), masks[:, :, ::-1])',\n",
       "  100.0),\n",
       " ('def vector(b,e):\\n    \\n    x,y,z = b\\n    X,Y,Z = e\\n    return (X-x, Y-y, Z-z)',\n",
       "  'from source import vector\\n\\ndef test_vector():\\n    b = (1, 2, 3)\\n    e = (4, 5, 6)\\n    result = vector(b, e)\\n    assert result == (3, 3, 3), \"The vectors are not equal\"',\n",
       "  100.0),\n",
       " ('def gather_points(points, index):\\n    \\n    batch_size = points.size(0)\\n    channels = points.size(1)\\n    num_centroids = index.size(1)\\n    index_expand = index.unsqueeze(1).expand(batch_size, channels, num_centroids)\\n    return points.gather(2, index_expand)',\n",
       "  '# Import the necessary package\\nimport sys\\nsys.path.append(\"./\")  # Add the directory containing source.py to the PATH\\nfrom source import gather_points  # Import the function from source.py\\nimport torch\\n\\ndef test_gather_points():\\n    # Create sample input data\\n    points = torch.randn(2, 3, 5)  # Random tensor of size 2x3x5\\n    index = torch.tensor([[1, 0, 1], [0, 2, 0]])  # Index tensor of size 2x3\\n\\n    # Call the function with the sample input data\\n    result = gather_points(points, index)\\n\\n    # Add your assertion here. For example:\\n    assert result.shape == (2, 3, 3), \"Shapes do not match\"\\n\\n# If the above function passed, the below line will print: OK\\nprint(\"All tests passed.\")',\n",
       "  100.0),\n",
       " (\"def evaluate_clf(pred_classes, true_classes):\\n    \\n    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\\n    accuracy = accuracy_score(true_classes, pred_classes)\\n    precision, recall, f_score, true_sum = precision_recall_fscore_support(true_classes,\\n                                                                           pred_classes,\\n                                                                           average='binary')\\n    return {'accuracy': accuracy,\\n            'precision': precision,\\n            'recall': recall,\\n            'f-score': f_score,\\n            'support': true_sum}\",\n",
       "  \"import pytest\\nfrom source import evaluate_clf  # replace with the actual import of your function\\n\\ndef test_evaluate_clf():\\n    pred_classes = [0, 1, 0, 1]\\n    true_classes = [0, 1, 0, 1]\\n\\n    result = evaluate_clf(pred_classes, true_classes)\\n\\n    assert result['accuracy'] == 1.0\\n    assert result['precision'] == 1.0\\n    assert result['recall'] == 1.0\\n    assert result['f-score'] == 1.0\\n    assert result['support'] == 4\",\n",
       "  100.0),\n",
       " ('def reduced_kernel_size_3d(input_tensor, kernel_size):\\n  \\n  assert len(kernel_size) == 3\\n  shape = input_tensor.get_shape().as_list()\\n  assert len(shape) == 5\\n  if None in shape[1:4]:\\n    kernel_size_out = kernel_size\\n  else:\\n    kernel_size_out = [min(shape[1], kernel_size[0]),\\n                       min(shape[2], kernel_size[1]),\\n                       min(shape[3], kernel_size[2])]\\n  return kernel_size_out',\n",
       "  'import pytest\\nfrom source import reduced_kernel_size_3d\\n\\ndef test_reduced_kernel_size_3d():\\n    input_tensor = \"This is a placeholder for a tensor object\"\\n    kernel_size = [5,7,9]\\n    expected_output = [5,7,9]\\n    assert reduced_kernel_size_3d(input_tensor, kernel_size) == expected_output\\n\\ndef test_reduced_kernel_size_3d_2():\\n    input_tensor = \"This is a placeholder for a tensor object\"\\n    kernel_size = [2,3,4]\\n    expected_output = [2,3,4]\\n    assert reduced_kernel_size_3d(input_tensor, kernel_size) == expected_output\\n\\ndef test_reduced_kernel_size_3d_3():\\n    input_tensor = \"This is a placeholder for a tensor object\"\\n    kernel_size = [10,11,None]\\n    expected_output = [10,11,None]\\n    assert reduced_kernel_size_3d(input_tensor, kernel_size) == expected_output',\n",
       "  38.0),\n",
       " (\"def generate_sample_to_signal_pivot_table(input_df, feature_key='POSITION'):\\n    \\n    assert not (\\n            set(['BARCODE', feature_key, 'signal_relative_to_C321']) -\\n            set(input_df))\\n\\n    pivot_table_df = (\\n            input_df.pivot_table(\\n                    index='BARCODE',\\n                    columns=[feature_key],\\n                    values='signal_relative_to_C321'))\\n\\n    # Get rid of columns that are entirely 0.\\n    pivot_table_df = pivot_table_df[\\n            pivot_table_df.columns[pivot_table_df.apply(sum) != 0]]\\n    return pivot_table_df\",\n",
       "  \"import pandas as pd\\nimport pytest\\nfrom source import generate_sample_to_signal_pivot_table\\n\\ndef test_generate_sample_to_signal_pivot_table():\\n    # Create a sample dataframe\\n    df = pd.DataFrame({\\n        'BARCODE': ['barcode1', 'barcode2', 'barcode3'],\\n        'POSITION': ['position1', 'position2', 'position3'],\\n        'signal_relative_to_C321': [1, 2, 3]\\n    })\\n\\n    # Call the function\\n    result = generate_sample_to_signal_pivot_table(df, 'POSITION')\\n\\n    # Create a sample expected output\\n    expected_output = pd.DataFrame({\\n        'barcode1': { 'position1': 1, 'position2': 0, 'position3': 0},\\n        'barcode2': { 'position1': 0, 'position2': 2, 'position3': 0},\\n        'barcode3': { 'position1': 0, 'position2': 0, 'position3': 3}\\n    })\\n\\n    # Assertion\\n    assert result.equals(expected_output)\",\n",
       "  100.0),\n",
       " ('def temporal_snapshots_ids(G):\\n    \\n    return G.temporal_snapshots_ids()',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport sys\\nsys.path.append(\".\") # To find source.py in the same directory\\nfrom source import temporal_snapshots_ids\\n\\ndef test_temporal_snapshots_ids():\\n    G = YourClass() # You need to provide the instance of the class \\n    assert temporal_snapshots_ids(G) == YourExpectedResult',\n",
       "  50.0),\n",
       " ('def solar_constant():\\n    r\\n    return 1367.',\n",
       "  'import pytest\\nimport source  # assuming the original code is in a file named source.py\\n\\ndef test_solar_constant():\\n    assert source.solar_constant() == 1367',\n",
       "  67.0),\n",
       " (\"def timestamp_to_ms(timestamp):\\n    \\n    num_integer_digits = len(str(timestamp).split('.')[0])\\n    return int(timestamp*10**(13 - num_integer_digits))\",\n",
       "  'import pytest\\nimport source\\n\\nclass TestTimestampToMS:\\n\\n    def test_timestamp_to_ms(self):\\n        assert source.timestamp_to_ms(123) == 1230000000000\\n        assert source.timestamp_to_ms(123.456) == 1234560000000\\n        with pytest.raises(AttributeError):\\n            assert source.timestamp_to_ms(123.456789) == 1234567890000',\n",
       "  100.0),\n",
       " ('def accuracy_score(y_true, y_pred):\\n    \\n    score = y_true == y_pred\\n    return score.sum() / len(y_true)',\n",
       "  'import pytest\\nimport source\\n\\ndef test_accuracy_score():\\n    y_true = [1, 2, 3, 4, 5]\\n    y_pred = [1, 2, 3, 4, 5]\\n    with pytest.raises(AttributeError):\\n        assert source.accuracy_score(y_true, y_pred) == 1.0',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef iou(a,b):\\n    \\n    \\n    area_a = a[:,2] * a[:,2] * a[:,3]\\n    area_b = b[:,2] * b[:,2] * b[:,3]\\n    \\n    minx = torch.max(a[:,0]-a[:,2]/2, b[:,0]-b[:,2]/2)\\n    maxx = torch.min(a[:,0]+a[:,2]/2, b[:,0]+b[:,2]/2)\\n    miny = torch.max(a[:,1]-a[:,2]*a[:,3]/2, b[:,1]-b[:,2]*b[:,3]/2)\\n    maxy = torch.min(a[:,1]+a[:,2]*a[:,3]/2, b[:,1]+b[:,2]*b[:,3]/2)\\n    zeros = torch.zeros(minx.shape,dtype = float)\\n    \\n    intersection = torch.max(zeros, maxx-minx) * torch.max(zeros,maxy-miny)\\n    union = area_a + area_b - intersection\\n    iou = torch.div(intersection,union)\\n    mean_iou = torch.mean(iou)\\n    \\n    return mean_iou',\n",
       "  '# test_source.py\\nimport source\\nimport torch\\n\\ndef test_iou():\\n    a = torch.tensor([[0,0,1,1],[1,1,2,2],[2,2,3,3]], dtype=torch.float)\\n    b = torch.tensor([[0,0,1,1],[1,1,2,2],[2,2,3,3]], dtype=torch.float)\\n\\n    result = source.iou(a, b)\\n    expected = torch.tensor(1.0, dtype=torch.float)\\n\\n    assert torch.isclose(result, expected)',\n",
       "  100.0),\n",
       " ('import numpy\\n\\ndef remove_outliers(x, factor=1.5):\\n    \\n    pctl25 = numpy.percentile(x, 25)\\n    pctl75 = numpy.percentile(x, 75)\\n    IQR = pctl75 - pctl25\\n    return x[(x >= pctl25 - (factor * IQR)) & (x <= pctl75 + (factor * IQR))]',\n",
       "  'import numpy\\nimport pytest\\nimport source  # assuming the original code is in a file named \\'source.py\\'\\n\\ndef test_remove_outliers_with_default_factor():\\n    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\n    result = source.remove_outliers(x)\\n    assert result == [2, 3, 4, 5, 6, 7, 8, 9, 10], \"Original list with outliers was: \" + str(x)\\n\\ndef test_remove_outliers_with_different_factor():\\n    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\n    result = source.remove_outliers(x, factor=2)\\n    assert result == [1, 2, 3, 4, 5, 6, 7, 8, 9], \"List with outliers was: \" + str(x)\\n\\ndef test_remove_outliers_with_outliers():\\n    x = [1, 2, 3, 4, 5, 6, 7, 800, 9, 10]\\n    result = source.remove_outliers(x)\\n    assert result == [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"List with outliers was: \" + str(x)\\n\\ndef test_remove_outliers_with_empty_list():\\n    x = []\\n    result = source.remove_outliers(x)\\n    assert result == [], \"Empty list was: \" + str(x)\\n\\ndef test_remove_outliers_with_one_element():\\n    x = [5]\\n    result = source.remove_outliers(x)\\n    assert result == [5], \"Single element list was: \" + str(x)\\n\\ndef test_remove_outliers_with_no_outliers():\\n    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\n    result = source.remove_outliers(x)\\n    assert result == [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"List with outliers was: \" + str(x)',\n",
       "  100.0),\n",
       " ('import numpy\\n\\ndef rotation_matrix(angle, axis):\\n    \\n    c = numpy.cos(angle)\\n    s = numpy.sin(angle)\\n    matrix = numpy.zeros((3, 3))\\n    matrix[0, 0] = c + axis[0] ** 2 * (1 - c)\\n    matrix[0, 1] = axis[0] * axis[1] * (1 - c) - axis[2] * s\\n    matrix[0, 2] = axis[0] * axis[2] * (1 - c) + axis[1] * s\\n    matrix[1, 0] = axis[1] * axis[0] * (1 - c) + axis[2] * s\\n    matrix[1, 1] = c + axis[1] ** 2 * (1 - c)\\n    matrix[1, 2] = axis[1] * axis[2] * (1 - c) - axis[0] * s\\n    matrix[2, 0] = axis[2] * axis[0] * (1 - c) - axis[1] * s\\n    matrix[2, 1] = axis[2] * axis[1] * (1 - c) + axis[0] * s\\n    matrix[2, 2] = c + axis[2] ** 2 * (1 - c)\\n    return matrix',\n",
       "  'import numpy as np\\nimport pytest\\nfrom source import rotation_matrix\\n\\ndef test_rotation_matrix():\\n    # Test with a random rotation axis and angle\\n    axis = np.array([1, 2, 3])\\n    angle = np.pi / 2\\n\\n    expected_output = np.array([[6.123233995736766e-17, 1.0, 0.0], \\n                               [0.0, 6.123233995736766e-17, -1.0], \\n                               [-1.0, 6.123233995736766e-17, 6.123233995736766e-17]])\\n\\n    assert np.allclose(rotation_matrix(angle, axis), expected_output, atol=1e-9)',\n",
       "  100.0),\n",
       " ('def list_sample_data():\\n    \\n    names = {\\n        \"bathymetry\": \"Table of ship bathymetric observations off Baja California\",\\n        \"fractures\": \"Table of hypothetical fracture lengths and azimuths\",\\n        \"hotspots\": \"Table of locations, names, and symbol sizes of hotpots from \"\\n        \" Mueller et al., 1993\",\\n        \"japan_quakes\": \"Table of earthquakes around Japan from NOAA NGDC database\",\\n        \"mars_shape\": \"Table of topographic signature of the hemispheric dichotomy of \"\\n        \" Mars from Smith and Zuber (1996)\",\\n        \"ocean_ridge_points\": \"Table of ocean ridge points for the entire world\",\\n        \"usgs_quakes\": \"Table of global earthquakes from the USGS\",\\n    }\\n    return names',\n",
       "  '',\n",
       "  0.0),\n",
       " ('def accuracy_ratio(train_accuracy, test_accuracy):\\n    \\n    return train_accuracy / test_accuracy',\n",
       "  'import pytest\\nfrom source import accuracy_ratio\\n\\ndef test_accuracy_ratio():\\n    assert accuracy_ratio(0.75, 0.60) == 0.75/0.60',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef get_conditional_pixels(batch, mask, num_colors):\\n    \\n    batch_size, channels, width, height = batch.size()\\n    # Add extra channel to keep mask\\n    cond_pixels = torch.zeros((batch_size, channels + 1, height, width))\\n    # Mask batch to only show visible pixels\\n    cond_pixels[:, :channels, :, :] = mask * batch.float()\\n    # Add mask scaled by number of colors in last channel dimension\\n    cond_pixels[:, -1:, :, :] = mask * (num_colors - 1)\\n    # Normalize conditional pixels to be in 0 - 1 range\\n    return cond_pixels / (num_colors - 1)',\n",
       "  '# contents of test_source.py\\nimport pytest\\nimport torch\\nfrom source import get_conditional_pixels\\n\\ndef test_get_conditional_pixels():\\n    # Create random batch, mask, and num_colors\\n    batch = torch.rand((10, 3, 64, 64))\\n    mask = torch.rand((10, 1, 64, 64))\\n    num_colors = 10\\n\\n    # Get conditional pixels\\n    cond_pixels = get_conditional_pixels(batch, mask, num_colors)\\n\\n    # Check if the shape of cond_pixels is correct\\n    assert cond_pixels.shape == (10, 4, 64, 64)\\n\\n    # Check if the first channel is equal to the original batch multiplied by mask\\n    assert torch.allclose(cond_pixels[:, 0, :, :], batch * mask)\\n\\n    # Check if the last channel is equal to the mask scaled by num_colors\\n    assert torch.allclose(cond_pixels[:, -1, :, :], mask * (num_colors - 1))\\n\\n    # Check if all channels are in 0 - 1 range\\n    assert torch.allclose(cond_pixels, cond_pixels * (num_colors - 1), atol=1e-6)',\n",
       "  100.0),\n",
       " (\"def polygon_list_to_dict(polygon):\\n    \\n    return {'points': polygon}\",\n",
       "  'import sys\\nsys.path.append(\".\")\\nfrom source import polygon_list_to_dict\\n\\ndef test_polygon_list_to_dict():\\n    polygon = [\\'point1\\', \\'point2\\', \\'point3\\']\\n    expected_result = {\\'points\\': polygon}\\n    assert polygon_list_to_dict(polygon) == expected_result',\n",
       "  100.0),\n",
       " ('def orthogonal_change_of_basis(A, B, a):\\n    \\n    return a*B*A',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\"./\")  # this line is added to import the source.py file in the same directory\\nfrom source import orthogonal_change_of_basis\\n\\ndef test_orthogonal_change_of_basis():\\n    A = [[1, 2], [3, 4]]\\n    B = [[1, 2], [3, 4]]\\n    a = 5\\n    assert orthogonal_change_of_basis(A, B, a) == [5, 10, 15, 20]',\n",
       "  100.0),\n",
       " ('def crop_image_around_point(image, y_ind, x_ind, min_radius):\\n    \\n\\n    x_min = x_ind - min_radius\\n    x_max = x_ind + min_radius\\n    y_min = y_ind - min_radius\\n    y_max = y_ind + min_radius\\n\\n    x_min = x_min if x_min > 0 else 0\\n    y_min = y_min if y_min > 0 else 0\\n    x_max = x_max if x_max <= image.shape[1] else image.shape[1]\\n    y_max = y_max if y_max <= image.shape[0] else image.shape[0]\\n\\n    return image[x_min:y_max, x_min:x_max], y_min, x_min',\n",
       "  'import pytest\\nimport source  # Assumed to be in the same directory\\n\\n\\ndef test_crop_image_around_point():\\n    # Creating a dummy test\\n    image = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20]]\\n    y_ind, x_ind, min_radius = 2, 3, 1\\n    expected_output = [[7, 8, 9], [11, 12, 13], [15, 16, 17]]\\n    y_min, x_min = 1, 2\\n\\n    assert source.crop_image_around_point(image, y_ind, x_ind, min_radius) == (expected_output, y_min, x_min)',\n",
       "  80.0),\n",
       " ('def time_slice(G, t_from, t_to=None):\\n    \\n    return G.time_slice(t_from, t_to)',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")  # To import source.py from the same directory\\nimport source  # Replace \\'source\\' with the actual name of your source file\\n\\ndef test_time_slice_t_to_none():\\n    G = source.Graph()  # Replace \\'Graph\\' with the actual class name in your source.py\\n    t_from = 1\\n    assert len(G.time_slice(t_from, None)) == len(G.time_slice(t_from))  # We use len() to check the length of the output which can be any property that can be used to check the correctness of the output.\\n\\ndef test_time_slice_t_to_some_value():\\n    G = source.Graph()\\n    t_from = 1\\n    t_to = 10\\n    assert len(G.time_slice(t_from, t_to)) == len(G.time_slice(t_from))  # We use len() to check the length of the output which can be any property that can be used to check the correctness of the output.',\n",
       "  50.0),\n",
       " ('def R_hill(mp, m_st, a_r,rp):\\n    \\n    return ( mp/(3*m_st) )**(1/3.) * a_r/rp',\n",
       "  '# source.py\\ndef R_hill(mp, m_st, a_r,rp):\\n    return ( mp/(3*m_st) )**(1/3.) * a_r/rp\\n\\n\\n# test_source.py\\nimport pytest\\nimport sys\\nimport os\\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\\n\\nfrom source import R_hill\\n\\ndef test_R_hill():\\n    # test with known values\\n    assert R_hill(2, 3, 4, 5) == (2/(3*3))**(1/3.) * 4/5\\n\\n    # additional tests can be added for different scenarios',\n",
       "  100.0),\n",
       " ('def bond_to_json(bond):\\n    \\n\\n    return (\\n        bond.get_atom1().get_id(),\\n        bond.get_atom2().get_id(),\\n        bond.get_order(),\\n        bond.get_periodicity(),\\n    )',\n",
       "  \"# test_source.py\\n\\nimport pytest\\nfrom source import Bond # assuming the source code is in a file named source.py in the same directory\\n\\ndef test_bond_to_json():\\n    # creating dummy Bond objects\\n    dummy_bond1 = Bond('1', '2', 2, 1)\\n    dummy_bond2 = Bond('3', '4', 3, 2)\\n\\n    # let's assume the Bond class has methods get_atom1, get_atom2, get_order, get_periodicity\\n    \\n    # testing using dummy bonds\\n    assert bond_to_json(dummy_bond1) == ('1', '2', 2, 1)\\n    assert bond_to_json(dummy_bond2) == ('3', '4', 3, 2)\",\n",
       "  50.0),\n",
       " ('def rk4_step(state, force, dt, k1=None):\\n    \\n    point, vector = state\\n    if k1 is None:\\n        k1 = force(point, vector)\\n    k2 = force(point + dt / 2 * vector, vector + dt / 2 * k1)\\n    k3 = force(point + dt / 2 * vector + dt ** 2 / 2 * k1,\\n               vector + dt / 2 * k2)\\n    k4 = force(point + dt * vector + dt ** 2 / 2 * k2, vector + dt * k3)\\n    point_new = point + dt * vector + dt ** 2 / 6 * (k1 + k2 + k3)\\n    vector_new = vector + dt / 6 * (k1 + 2 * k2 + 2 * k3 + k4)\\n    return point_new, vector_new',\n",
       "  '# test_source.py\\n\\nfrom source import rk4_step\\n\\ndef test_rk4_step():\\n    state = (0, 0)  # initialize state\\n    force = lambda point, vector: (0, 0)  # initialize force\\n    dt = 0.1  # time step\\n\\n    # Run the RK4 method\\n    state_new = rk4_step(state, force, dt)\\n\\n    # Assert that the state is as expected after the simulation\\n    assert state_new == (0.1, 0.1)  # update these values with the expected result',\n",
       "  50.0),\n",
       " ('def calc_rate(rate, rate_long, maturity):\\n    \\n    return rate + (rate_long - rate)*(maturity - 1) / 9',\n",
       "  \"from source import calc_rate\\n\\ndef test_calc_rate_normal_case():\\n    assert calc_rate(0.05, 0.055, 10) == 0.055, 'Test 1 Failed'\\n\\ndef test_calc_rate_edge_case():\\n    assert calc_rate(0.05, 0.055, 1) == 0.05, 'Test 2 Failed'\\n\\ndef test_calc_rate_high_maturity():\\n    assert calc_rate(0.05, 0.055, 20) == 0.06055555555555555, 'Test 3 Failed'\\n\\ndef test_calc_rate_zero_maturity():\\n    assert calc_rate(0.05, 0.055, 0) == 0.04944444444444445, 'Test 4 Failed'\",\n",
       "  100.0),\n",
       " ('def get_smallest_expected_face_size(image_shape, min_face_size, min_face_to_image_ratio):\\n    \\n\\n    image_ratio_based_size = round(min(image_shape[:2]) * min_face_to_image_ratio)\\n    return max(min_face_size, image_ratio_based_size)',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.') # to import source.py\\nfrom source import get_smallest_expected_face_size\\n\\ndef test_get_smallest_expected_face_size():\\n    image_shape = (100, 100)\\n    min_face_size = 50\\n    min_face_to_image_ratio = 0.5\\n    assert get_smallest_expected_face_size(image_shape, min_face_size, min_face_to_image_ratio) == 50\",\n",
       "  100.0),\n",
       " ('def triangular_vegetation_index(Green, Red, Rededge):\\n    \\n    # Rededge B06 750, Red B04 670, Green B03 550\\n    TVI = .5* (120*(Rededge - Green) - 200*(Red - Green))\\n    return TVI',\n",
       "  'import pytest\\nfrom source import triangular_vegetation_index\\n\\ndef test_triangular_vegetation_index():\\n    \"\"\"\\n    Test the triangular_vegetation_index function.\\n    \"\"\"\\n    # Test case 1\\n    assert triangular_vegetation_index(200, 150, 100) == 0, \"Test case 1 failed\"\\n    # Test case 2\\n    assert triangular_vegetation_index(300, 250, 200) == 50, \"Test case 2 failed\"\\n    # Test case 3\\n    assert triangular_vegetation_index(500, 400, 350) == 100, \"Test case 3 failed\"\\n    # Test case 4\\n    assert triangular_vegetation_index(700, 600, 650) == 150, \"Test case 4 failed\"\\n    # Test case 5\\n    assert triangular_vegetation_index(800, 700, 850) == 200, \"Test case 5 failed\"',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef pairwise_cosine_similarity(x, y):\\n    \\n    x = torch.div(x, torch.sqrt(torch.max(torch.sum(x ** 2), 1e-12)))\\n    y = torch.div(y, torch.sqrt(torch.max(torch.sum(y ** 2), 1e-12)))\\n    return torch.mm(x, torch.transpose(y, 1, 0))',\n",
       "  '# test_source.py\\nimport torch\\nimport pytest\\nfrom source import pairwise_cosine_similarity\\n\\ndef test_pairwise_cosine_similarity():\\n    x = torch.Tensor([[1,2,3],[4,5,6],[7,8,9]])\\n    y = torch.Tensor([[10,11,12],[13,14,15],[16,17,18]])\\n    result = pairwise_cosine_similarity(x, y)\\n    assert torch.allclose(result, torch.Tensor([[ 9.21333511,  8.37101207,  7.54407836],\\n                                                 [ 6.43025542,  5.51208765,  4.58473638],\\n                                                 [ 3.54407836,  2.51208765,  1.43025542]]), atol=1e-6)',\n",
       "  60.0),\n",
       " ('def reock_compactness(geom):\\n    \\n    from shapely.geometry import Point\\n    from pointpats.centrography import minimum_bounding_circle\\n\\n    points = list(zip(*geom.minimum_rotated_rectangle.exterior.coords.xy))\\n    center, radius = minimum_bounding_circle(points)\\n    mbc_poly = Point(*center).buffer(radius)\\n\\n    return geom.area/mbc_poly.area',\n",
       "  \"import pytest\\nfrom shapely.geometry import Polygon, Point\\nfrom your_module import reock_compactness  # replace 'your_module' with the actual module name\\n\\ndef test_reock_compactness():\\n    # create a test polygon\\n    geom = Polygon([(0, 0), (1, 0), (1, 1), (0, 1)])\\n\\n    # calculate the Reock-Compactness\\n    reock_compactness_value = reock_compactness(geom)\\n\\n    # the actual value depends on the shape of the polygon, below is just an example\\n    assert reock_compactness_value == 0.6931471805599444\",\n",
       "  0.0),\n",
       " (\"def log_mean_exp(ops, x, axis=None, keepdims=False):\\n    \\n    x = ops.convert_to_tensor(x)\\n    with ops.name_scope('log_mean_exp', values=[x]):\\n        x = ops.convert_to_tensor(x)\\n        x_max_keepdims = ops.reduce_max(x, axis=axis, keepdims=True)\\n        if not keepdims:\\n            x_max = ops.squeeze(x_max_keepdims, axis=axis)\\n        else:\\n            x_max = x_max_keepdims\\n        mean_exp = ops.reduce_mean(ops.exp(x - x_max_keepdims), axis=axis,\\n                                   keepdims=keepdims)\\n        return x_max + ops.log(mean_exp)\",\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import log_mean_exp\\nimport tensorflow as tf\\n\\ndef test_log_mean_exp():\\n    # Initialize TensorFlow to use the CPU\\n    tf.disable_v2_behavior()\\n\\n    # Test with random values\\n    with tf.Session() as sess:\\n        rand_values = tf.random.normal([10, 10])\\n        output = sess.run(log_mean_exp(rand_values))\\n        assert output.shape == rand_values.shape, \"Shape mismatch\"\\n\\n    # Test with axis specified\\n    with tf.Session() as sess:\\n        rand_values_axis = tf.random.normal([10, 10, 10])\\n        output_axis = sess.run(log_mean_exp(rand_values_axis, axis=1))\\n        assert output_axis.shape == tf.shape(rand_values_axis)[:-1], \"Shape mismatch with axis\"\\n\\n    # Test with keepdims True\\n    with tf.Session() as sess:\\n        rand_values_keepdims = tf.random.normal([10, 10, 10])\\n        output_keepdims = sess.run(log_mean_exp(rand_values_keepdims, keepdims=True))\\n        assert output_keepdims.shape == rand_values_keepdims.shape, \"Shape mismatch with keepdims True\"\\n\\n    # Test with keepdims False\\n    with tf.Session() as sess:\\n        rand_values_keepdims = tf.random.normal([10, 10, 10])\\n        output_keepdims = sess.run(log_mean_exp(rand_values_keepdims, keepdims=False))\\n        assert output_keepdims.shape == rand_values_keepdims.shape[:-1], \"Shape mismatch with keepdims False\"',\n",
       "  10.0),\n",
       " ('def standardized(array):\\n    \\n    return (array - array.mean()) / max(1e-4, array.std())',\n",
       "  '# source.py\\ndef standardized(array):\\n    \\n    return (array - array.mean()) / max(1e-4, array.std())\\n\\n# test_source.py\\nimport pytest\\nfrom source import standardized\\nimport numpy as np\\n\\ndef test_standardized():\\n    array = np.array([1,2,3,4,5])\\n    result = standardized(array)\\n    expected_result = (array - array.mean()) / max(1e-4, array.std())\\n    assert np.allclose(result, expected_result), \"The arrays do not match\"',\n",
       "  100.0),\n",
       " ('def length(P, Q, D):\\n    r\\n    from sympy.ntheory.continued_fraction import continued_fraction_periodic\\n    v = continued_fraction_periodic(P, Q, D)\\n    if type(v[-1]) is list:\\n        rpt = len(v[-1])\\n        nonrpt = len(v) - 1\\n    else:\\n        rpt = 0\\n        nonrpt = len(v)\\n    return rpt + nonrpt',\n",
       "  \"import pytest\\nfrom source import length\\nfrom unittest.mock import Mock, patch\\n\\ndef test_length():\\n    # mock the output of continued_fraction_periodic function\\n    continued_fraction_periodic_mock = Mock(return_value=[1, 2, 3])\\n    \\n    with patch('source.continued_fraction_periodic', continued_fraction_periodic_mock):\\n        assert length(1, 1, 1) == 3\",\n",
       "  10.0),\n",
       " ('def index(x, axis, index_spec):\\n    \\n    idx = [slice(None)] * x.ndim\\n    idx[axis] = index_spec\\n\\n    indexer = tuple(idx)\\n    return indexer',\n",
       "  \"import pytest\\nimport sys\\nimport os\\n\\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\\n\\nimport source  # Importing the source file\\n\\nclass TestSource:\\n\\n    def test_index(self):\\n        assert source.index([5, 5, 5], 0, slice(1, 3, 2)) == (slice(1, 3, 2), slice(None), slice(None))\",\n",
       "  40.0),\n",
       " ('def binary_search(a, x):\\n    \\n    left, right = 0, len(a) - 1\\n    while left <= right:\\n        mid = left + (right - left) // 2\\n\\n        a_mid = a[mid]\\n        if x == a_mid:\\n            return mid\\n\\n        # left--mid--x--right\\n        if a_mid < x:\\n            left = mid + 1\\n\\n        # left--x--mid--right\\n        elif x < a_mid:\\n            right = mid - 1\\n\\n    return -1',\n",
       "  'import pytest\\nimport source\\n\\ndef test_binary_search():\\n    assert source.binary_search([1, 2, 3, 4, 5, 6, 7], 6) == 5\\n    assert source.binary_search([1, 2, 3, 4, 5, 6, 7], 8) == -1\\n    assert source.binary_search([], 1) == -1\\n    assert source.binary_search([1], 1) == 0\\n    assert source.binary_search([2], 1) == -1',\n",
       "  100.0),\n",
       " ('def trans_shape_to_bounds(transform, shape):\\n    \\n    left, top = transform * (0, 0)\\n    right, bottom = transform * shape\\n    return (left, bottom, right, top)',\n",
       "  'import pytest\\nfrom source import trans_shape_to_bounds\\n\\nclass TestTransShapeToBounds:\\n\\n    def test_transform_identity(self):\\n        transform = ((1, 0), (0, 1))\\n        shape = (1, 1)\\n        assert trans_shape_to_bounds(transform, shape) == (0, 0, 1, 1)\\n\\n    def test_transform_translate(self):\\n        transform = ((2, 3), (4, 5))\\n        shape = (1, 1)\\n        assert trans_shape_to_bounds(transform, shape) == (2, 3, 2, 4)\\n\\n    def test_transform_scale(self):\\n        transform = ((2, 0), (0, 3))\\n        shape = (2, 2)\\n        assert trans_shape_to_bounds(transform, shape) == (0, 0, 4, 3)',\n",
       "  50.0),\n",
       " ('def virtual_start(x_start, slope_start, y_base, y_start):\\n    \\n    return (y_base - y_start) / slope_start + x_start',\n",
       "  '# source.py\\ndef virtual_start(x_start, slope_start, y_base, y_start):\\n    return (y_base - y_start) / slope_start + x_start\\n\\n\\n# test_source.py\\nimport pytest\\nfrom source import virtual_start\\n\\ndef test_virtual_start():\\n    assert virtual_start(0, 1, 1, 1) == 0',\n",
       "  100.0),\n",
       " ('def divide(slc, maxLen):\\n    \\n    sub_slices = []\\n    sub_start = slc.start\\n    step = 1 if (slc.step is None) else slc.step\\n    while sub_start < slc.stop:\\n        # Note: len(range(start,stop,step)) == stop-start+(step-1) // step\\n        sub_slices.append(slice(sub_start, min(sub_start + maxLen * step, slc.stop),\\n                                slc.step))\\n        sub_start += maxLen * step\\n    return sub_slices',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\"..\") # To find source.py\\nfrom source import divide\\n\\ndef test_divide():\\n    slic = slice(1,10,2)\\n    max_len = 3\\n    expected_output = [slice(1, 4, 2), slice(4, 7, 2), slice(7, 10, 2)]\\n    assert divide(slic, max_len) == expected_output',\n",
       "  100.0),\n",
       " ('def parse_residuelabels(label_file):\\n    \\n    if label_file is None:\\n        return None, None\\n\\n    ret = ({}, {})\\n    flarelabels = set()  # Only used to check for duplicates\\n    for line in label_file:\\n        line = line.strip()\\n        if not line:\\n            continue  # Ignore empty lines\\n\\n        columns = line.split(\"\\\\t\")\\n        residentifier = columns[0]\\n        flaretreepath = columns[1] if len(columns) > 1 else columns[0]\\n        flarelabel = flaretreepath.split(\".\")[-1]\\n        flarecolor = columns[2] if len(columns) > 2 else \"white\"\\n        if residentifier in ret:\\n            raise AssertionError(\"Residue identifier \\'\"+residentifier+\"\\' appears twice in \"+label_file.name)\\n        if flarelabel in flarelabels:\\n            raise AssertionError(\"Flare label \\'\"+flarelabel+\"\\' used twice in \"+label_file.name)\\n\\n        # ret[residentifier] = {\"label\": flarelabel, \"treepath\": flaretreepath, \"color\": flarecolor}\\n        ret[0][residentifier] = flaretreepath\\n        ret[1][residentifier] = flarecolor\\n        flarelabels.add(flarelabel)\\n\\n    return ret',\n",
       "  'import pytest\\nfrom source import parse_residuelabels\\n\\ndef test_parse_residuelabels():\\n    label_file = open(\"your_file.txt\", \"r\")  # replace with your file\\n    res, colors = parse_residuelabels(label_file)\\n\\n    assert isinstance(res, dict)\\n    assert isinstance(colors, dict)\\n\\n    for value in res.values():\\n        assert isinstance(value, str)\\n\\n    for value in colors.values():\\n        assert isinstance(value, str)\\n\\n    label_file.close()',\n",
       "  5.0),\n",
       " ('def average_color(color1, color2):\\n    \\n    return tuple(map(lambda x: int(x / 2), map(sum, zip(color1, color2))))',\n",
       "  'import sys\\nsys.path.append(\".\")\\nfrom source import average_color\\n\\ndef test_average_color():\\n    color1 = (255, 0, 0)\\n    color2 = (0, 255, 0)\\n    assert average_color(color1, color2) == (127, 127, 0)\\n\\ntest_average_color()',\n",
       "  100.0),\n",
       " ('def normalize(ys, amp=1.0):\\n    \\n    high, low = abs(max(ys)), abs(min(ys))\\n\\n    return amp * ys / max(high, low)',\n",
       "  \"Python\\nimport sys\\nsys.path.append('.')\\nfrom source import normalize\\nimport pytest\\n\\ndef test_normalize():\\n    ys = [1, -2, 3, -4, 5]\\n    assert abs(normalize(ys) - -0.21212121212121213) < 1e-09\\n    ys = [0, 0, 0, 0, 0]\\n    assert abs(normalize(ys) - 0) < 1e-09\\n    ys = [100, 200, 300, 400, 500]\\n    assert abs(normalize(ys) - 1.0) < 1e-09\\n    ys = [-100, -200, -300, -400, -500]\\n    assert abs(normalize(ys) - -1.0) < 1e-09\\n    ys = [100, -100, 200, -200, 300, -300, 400, -400, 500, -500]\\n    assert abs(normalize(ys) - 0.8782608705192424) < 1e-09\",\n",
       "  0.0),\n",
       " ('def null_distance_results(string1, string2, max_distance):\\n    \\n    if string1 is None:\\n        if string2 is None:\\n            return 0\\n        else:\\n            return len(string2) if len(string2) <= max_distance else -1\\n    return len(string1) if len(string1) <= max_distance else -1',\n",
       "  \"import pytest\\nimport sys\\nsys.path.insert(0, '..')\\nfrom source import null_distance_results\\n\\ndef test_null_distance_results():\\n    assert null_distance_results(None, None, 1) == 0\\n    assert null_distance_results(None, 'string', 1) == 5\\n    assert null_distance_results('string', None, 1) == 5\\n    assert null_distance_results('string', 'string', 1) == -1\\n    assert null_distance_results('string', 'string', 5) == -1\\n    assert null_distance_results('string1', 'string2', 10) == -1\\n    assert null_distance_results('string1', 'string2', 3) == 2\\n    assert null_distance_results('test', 'testing', 10) == 1\",\n",
       "  83.0),\n",
       " ('def HFlip_rotated_box(transform, rotated_boxes):\\n    \\n    # Transform x_center\\n    rotated_boxes[:, 0] = transform.width - rotated_boxes[:, 0]\\n    # Transform angle\\n    rotated_boxes[:, 4] = -rotated_boxes[:, 4]\\n    return rotated_boxes',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\".\") # To import source.py from the same directory\\nfrom source import HFlip_rotated_box # Import function from source.py\\nimport numpy as np\\n\\ndef test_HFlip_rotated_box():\\n    # Define a transform\\n    transform = {\"width\": 100}\\n    # Define a rotated_boxes\\n    rotated_boxes = np.array([[10, 20, 30, 40, 50]])\\n    # Call the function with the transform and rotated_boxes\\n    result = HFlip_rotated_box(transform, rotated_boxes)\\n    # Assert that the result is as expected\\n    assert np.array_equal(result, np.array([[90, 20, 30, 40, -50]]))',\n",
       "  50.0),\n",
       " ('def get_tolerance_min_max(value, expected_tolerance):\\n    \\n    # Convert it to int\\n    value = int(value)\\n\\n    # Expected tolerance %\\n    tolerance = (value * expected_tolerance) / 100\\n\\n    # Minimum tolerance value\\n    min_value = abs(value - tolerance)\\n\\n    # Maximum tolerance value\\n    max_value = abs(value + tolerance)\\n\\n    return (min_value, max_value)',\n",
       "  'import sys\\nsys.path.append(\".\")  # To import source.py file from the same directory\\nfrom source import get_tolerance_min_max\\n\\ndef test_get_tolerance_min_max():\\n    value = 100\\n    expected_tolerance = 10\\n    assert get_tolerance_min_max(value, expected_tolerance) == (90, 110)',\n",
       "  100.0),\n",
       " ('def _calculate_mid_points(bounds):\\n    \\n\\n    # Calculate mid points for x and y bound coords\\n    x_mids = (bounds[:, 0] + bounds[:, 2]) / 2.0\\n    y_mids = (bounds[:, 1] + bounds[:, 3]) / 2.0\\n\\n    return x_mids, y_mids',\n",
       "  '# test_source.py\\n\\nimport sys\\nsys.path.append(\"..\") # This will append the parent directory to the sys path\\nfrom source import _calculate_mid_points\\n\\ndef test_mid_points():\\n    bounds = [[0, 0, 10, 10], [20, 20, 30, 30]] # Array of 2 bounds\\n    x_mids, y_mids = _calculate_mid_points(bounds)\\n    \\n    # Assertion\\n    assert x_mids.shape == (2,), \"The x_mids array has incorrect shape\"\\n    assert y_mids.shape == (2,), \"The y_mids array has incorrect shape\"\\n    assert x_mids[0] == 5, \"The first x_mid value is incorrect\"\\n    assert y_mids[1] == 25, \"The second y_mid value is incorrect\"\\n    assert x_mids[1] == 15, \"The second x_mid value is incorrect\"\\n    assert y_mids[0] == 15, \"The first y_mid value is incorrect\"',\n",
       "  50.0),\n",
       " ('def gather_sequence_info(detections):\\n    \\n    min_frame_idx = int(detections[:, 0].min())\\n    max_frame_idx = int(detections[:, 0].max())\\n    seq_info = {\\n        \"detections\": detections,\\n        \"min_frame_idx\": min_frame_idx,\\n        \"max_frame_idx\": max_frame_idx,\\n    }\\n    return seq_info',\n",
       "  \"def test_gather_sequence_info():\\n    detections = [[10, 20], [25, 30], [35, 40]]\\n    result = gather_sequence_info(detections)\\n    assert result['min_frame_idx'] == 10, 'Min frame index is not correct'\",\n",
       "  0.0),\n",
       " ('def GassmannModel(Phi, Kdry, Gdry, Kmat, Kfl):\\n    \\n\\n    # Bulk modulus of saturated rock\\n    Ksat = Kdry + ((1 - Kdry / Kmat) ** 2) / (Phi / Kfl + (1 - Phi) / Kmat - Kdry / (Kmat ** 2))\\n    # Shear modulus of saturated rock\\n    Gsat = Gdry\\n    \\n    return Ksat, Gsat',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import GassmannModel  # This is assuming the function is in a file called source.py in the same directory\\n\\ndef test_GassmannModel():\\n    # Test 1: check if function returns expected values for given input\\n    Ksat, Gsat = GassmannModel(0.6, 0.1, 0.2, 0.3, 0.4)\\n    assert Ksat == 0.10000000000000006, \"Test 1 failed\"\\n    assert Gsat == 0.2, \"Test 1 failed\"\\n    \\n    # Test 2: check if function works with minimum input\\n    Ksat, Gsat = GassmannModel(0.5, 0.1, 0.2, 0.3, 0.4)\\n    assert Ksat == 0.10000000000000006, \"Test 2 failed\"\\n    assert Gsat == 0.2, \"Test 2 failed\"\\n    \\n    # Test 3: check if function works with maximum input\\n    Ksat, Gsat = GassmannModel(1, 1, 1, 1, 1)\\n    assert Ksat == 1.0000000000000004, \"Test 3 failed\"\\n    assert Gsat == 1.0000000000000004, \"Test 3 failed\"',\n",
       "  100.0),\n",
       " ('def compute_loss(target, output, criterion, weights, args, model, data):\\n    \\n    loss = criterion(output, target)\\n    return loss',\n",
       "  \"import pytest\\nfrom source import compute_loss\\nimport torch\\n\\ndef test_compute_loss():\\n    target = torch.randn(10)\\n    output = torch.randn(10)\\n    criterion = torch.nn.MSELoss()\\n    weights = None\\n    args = None\\n    model = None\\n    data = None\\n    result = compute_loss(target, output, criterion, weights, args, model, data)\\n    assert isinstance(result, torch.Tensor), 'The function did not return a torch.Tensor'\",\n",
       "  100.0),\n",
       " ('def gradient(color_one, color_two, blend_weight):\\n    \\n    if blend_weight < 0.0:\\n        blend_weight = 0.0\\n    elif blend_weight > 1.0:\\n        blend_weight = 1.0\\n    initial_weight = 1.0 - blend_weight\\n    return (int(color_one[0] * initial_weight + color_two[0] * blend_weight),\\n            int(color_one[1] * initial_weight + color_two[1] * blend_weight),\\n            int(color_one[2] * initial_weight + color_two[2] * blend_weight))',\n",
       "  'import pytest\\nfrom color_source import gradient\\n\\ndef test_gradient_function():\\n    color_one = (255, 0, 0)  # Red\\n    color_two = (0, 255, 0)  # Green\\n    blend_weight = 0.5\\n    result = gradient(color_one, color_two, blend_weight)\\n    assert result == (12750771, 65535, 65535), \"Test Failed\"',\n",
       "  0.0),\n",
       " ('def dense(x, w, b, phi):\\n    \\n    return phi(x @ w + b)',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport numpy as np\\nimport source  # assuming the original code is in a file named source.py\\n\\nclass TestSource:\\n    \\n    def test_dense(self):\\n        x = np.array([1, 2, 3])\\n        w = np.array([4, 5, 6])\\n        b = 7\\n        phi = np.tanh  # using tanh as an example function\\n\\n        # asserting the output of the dense function equals the expected output\\n        assert np.array_equal(source.dense(x, w, b, phi), phi(x @ w + b))',\n",
       "  100.0),\n",
       " (\"def to_kilowatts(data, derate, power_fac):\\n    \\n\\n    # btu/hr to kw\\n    data['q'] = derate*(0.29307107*data['heat_flow_rate'])/1000\\n    data['q'].fillna(0.0, inplace=True)\\n    # w to kw\\n    data['kw_used'] = power_fac * data['heatpump_power']/1000\\n    return data\",\n",
       "  '# test_source.py\\nimport os\\nimport pytest\\nimport pandas as pd\\nfrom source import to_kilowatts\\n\\ndef test_to_kilowatts():\\n    # Given\\n    data = pd.DataFrame({\\'heat_flow_rate\\': [1000, 2000, 3000], \\'heatpump_power\\': [5000, 6000, 7000]})\\n    derate = 0.8\\n    power_fac = 1.2\\n\\n    # When\\n    result = to_kilowatts(data, derate, power_fac)\\n\\n    # Then\\n    assert \\'q\\' in result.columns, \"Missing column \\'q\\'\"\\n    assert \\'kw_used\\' in result.columns, \"Missing column \\'kw_used\\'\"\\n    assert not result[\\'q\\'].isna().any(), \"Derated value should not be NaN\"\\n    assert not result[\\'kw_used\\'].isna().any(), \"Power factor applied value should not be NaN\"',\n",
       "  100.0),\n",
       " ('def apply_bitshift_scale(x, bitshift, do_round=True):\\n    \\n\\n    assert bitshift >= 0\\n\\n    if do_round and bitshift > 0:\\n        #x += np.sign(x) * (1 << (bitshift - 1))\\n        x += (1 << (bitshift - 1))\\n\\n    # check if an overflow is happending\\n    if x.min() < -(1 << 31) or x.max() > ((1 << 31) - 1):\\n        raise OverflowError()\\n\\n    x = x >> bitshift\\n\\n    return x',\n",
       "  'import pytest\\nimport numpy as np\\nimport source  # import the source code\\n\\ndef test_apply_bitshift_scale():\\n    x = np.array([1, 2, 3, 4], dtype=np.int32)\\n    bitshift = 2\\n    do_round = True\\n    expected_result = np.array([4, 8, 12, 16], dtype=np.int32)\\n\\n    # Test the function with positive inputs\\n    np.testing.assert_array_equal(source.apply_bitshift_scale(x, bitshift, do_round), expected_result)\\n\\n    x = np.array([-1, -2, -3, -4], dtype=np.int32)\\n    expected_result = np.array([-4, -8, -12, -16], dtype=np.int32)\\n\\n    # Test the function with negative inputs\\n    np.testing.assert_array_equal(source.apply_bitshift_scale(x, bitshift, do_round), expected_result)\\n\\n    x = np.array([0], dtype=np.int32)\\n    bitshift = 30\\n    do_round = False\\n\\n    # Test the function with large bitshift and do_round=False\\n    assert source.apply_bitshift_scale(x, bitshift, do_round) == 0\\n\\n    x = np.array([1 << 30], dtype=np.int32)\\n    bitshift = 1\\n    do_round = True\\n\\n    # Test the function with a number equal to the maximum integer value\\n    assert source.apply_bitshift_scale(x, bitshift, do_round) == (1 << (bitshift - 1))\\n\\n    x = np.array([(1 << 30) + 1], dtype=np.int32)\\n    bitshift = 1\\n    do_round = False\\n\\n    # Test the function with a number just greater than the maximum integer value\\n    with pytest.raises(OverflowError):\\n        source.apply_bitshift_scale(x, bitshift, do_round)',\n",
       "  88.0),\n",
       " ('import torch\\n\\ndef relative_position_embedding(seq_length, out_dim):\\n    \\n    inverse_freq = 1 / (10000**(torch.arange(0, out_dim, 2.0) / out_dim))\\n    pos_offsets = torch.arange(seq_length - 1, -1, -1)\\n    inputs = pos_offsets[:, None] * inverse_freq[None, :]\\n    return torch.cat((torch.sin(inputs), torch.cos(inputs)), dim=-1)',\n",
       "  'import torch\\nimport pytest\\nfrom source import relative_position_embedding\\n\\ndef test_relative_position_embedding():\\n    seq_length = 5\\n    out_dim = 3\\n    result = relative_position_embedding(seq_length, out_dim)\\n    assert result.shape == (seq_length, 2*out_dim)\\n    assert torch.allclose(result[:, ::2], torch.sin(result[:, ::2]))\\n    assert torch.allclose(result[:, 1::2], torch.cos(result[:, 1::2]))',\n",
       "  100.0),\n",
       " ('def _get_effective_padding_node_input(stride, padding, effective_padding_output):\\n  \\n  return stride * effective_padding_output + padding',\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\nfrom source import _get_effective_padding_node_input\\n\\ndef test_get_effective_padding_node_input():\\n    stride = 2\\n    padding = 1\\n    effective_padding_output = 3\\n    assert _get_effective_padding_node_input(stride, padding, effective_padding_output) == 7\",\n",
       "  100.0),\n",
       " ('def R_hill(mp, m_st, a_r,rp):\\n    \\n    return ( mp/(3*m_st) )**(1/3.) * a_r/rp',\n",
       "  '# test_source.py\\n\\nimport pytest\\nfrom source import R_hill  # Import the function from source.py\\n\\ndef test_R_hill():\\n    assert R_hill(1, 2, 3, 4) == 4.0, \"Test case 1 failed: Expected 4.0, but got \" + str(R_hill(1, 2, 3, 4))\\n    assert R_hill(5, 6, 7, 8) == 16.0, \"Test case 2 failed: Expected 16.0, but got \" + str(R_hill(5, 6, 7, 8))\\n    assert R_hill(9, 10, 11, 12) == 27.0, \"Test case 3 failed: Expected 27.0, but got \" + str(R_hill(9, 10, 11, 12))\\n    assert R_hill(13, 14, 15, 16) == 32.0, \"Test case 4 failed: Expected 32.0, but got \" + str(R_hill(13, 14, 15, 16))',\n",
       "  100.0),\n",
       " ('def find_heuristic_path(graph, distances, start_node):\\n    \\n    n = distances.shape[0]\\n    path = [start_node]\\n    dist = 0\\n\\n    while len(path) < n:\\n        node = path[-1]\\n        new_neighbor = min(graph[node].items(), key=lambda x: x[1][\"weight\"])\\n        graph.remove_node(node)\\n        path.append(new_neighbor[0])\\n        dist += new_neighbor[1][\\'weight\\']\\n\\n    dist += distances[path[-1], start_node]\\n\\n    return dist, path',\n",
       "  '# source.py\\nimport networkx as nx\\nimport numpy as np\\n\\ndef find_heuristic_path(graph, distances, start_node):\\n    \\n    n = distances.shape[0]\\n    path = [start_node]\\n    dist = 0\\n\\n    while len(path) < n:\\n        node = path[-1]\\n        new_neighbor = min(graph[node].items(), key=lambda x: x[1][\"weight\"])\\n        graph.remove_node(node)\\n        path.append(new_neighbor[0])\\n        dist += new_neighbor[1][\\'weight\\']\\n\\n    dist += distances[path[-1], start_node]\\n\\n    return dist, path\\n\\n# test_source.py\\nimport pytest\\nfrom source import find_heuristic_path\\n\\ndef test_find_heuristic_path():\\n    graph = nx.Graph()\\n    graph.add_weighted_edges_from([(1, 2, 5), (1, 3, 2), (2, 3, 3), (2, 4, 1), (3, 4, 2)])\\n    distances = np.array([[0, 5, 2, 3, 4], [10, 0, 0, 0, 0], [0, 0, 0, 1, 2], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]])\\n\\n    result_dist, result_path = find_heuristic_path(graph, distances, 1)\\n\\n    assert result_dist == 16\\n    assert result_path == [1, 2, 3]',\n",
       "  83.0),\n",
       " (\"import torch\\n\\ndef mu_law_expanding(x_mu, qc):\\n    # type: (Tensor, int) -> Tensor\\n    \\n    assert isinstance(x_mu, torch.Tensor), 'mu_law_expanding expects a Tensor'\\n    mu = qc - 1.\\n    if not x_mu.is_floating_point():\\n        x_mu = x_mu.to(torch.float)\\n    mu = torch.tensor(mu, dtype=x_mu.dtype)\\n    x = ((x_mu) / mu) * 2 - 1.\\n    x = torch.sign(x) * (torch.exp(torch.abs(x) * torch.log1p(mu)) - 1.) / mu\\n    return x\",\n",
       "  \"import torch\\nimport pytest\\n\\ndef test_mu_law_expanding():\\n    # Given\\n    x_mu = torch.tensor([100])\\n    qc = 200\\n\\n    # When\\n    result = mu_law_expanding(x_mu, qc)\\n\\n    # Then\\n    assert isinstance(result, torch.Tensor), 'mu_law_expanding function should return a Tensor'\\n    assert result.shape == x_mu.shape, 'The shape of the output tensor is different from the input tensor'\\n    assert not result.is_floating_point(), 'The output tensor should not be a floating point number'\",\n",
       "  0.0),\n",
       " ('import torch\\n\\ndef AspectRatio(gt_rbboxes):\\n    \\n    pt1, pt2, pt3, pt4 = gt_rbboxes[..., :8].chunk(4, 1)\\n    edge1 = torch.sqrt(\\n        torch.pow(pt1[..., 0] - pt2[..., 0], 2) +\\n        torch.pow(pt1[..., 1] - pt2[..., 1], 2))\\n    edge2 = torch.sqrt(\\n        torch.pow(pt2[..., 0] - pt3[..., 0], 2) +\\n        torch.pow(pt2[..., 1] - pt3[..., 1], 2))\\n\\n    edges = torch.stack([edge1, edge2], dim=1)\\n\\n    width, _ = torch.max(edges, 1)\\n    height, _ = torch.min(edges, 1)\\n    ratios = (width / height)\\n    return ratios',\n",
       "  'import torch\\nimport pytest\\nfrom source import AspectRatio\\n\\ndef test_AspectRatio():\\n    # Generate random tensor data\\n    gt_rbboxes = torch.rand(10, 8, 2) # (N, 4, 2)\\n\\n    # Generate expected output\\n    expected_output = AspectRatio(gt_rbboxes)\\n\\n    # Call function and get output\\n    output = AspectRatio(gt_rbboxes)\\n\\n    # Compare output and expected output\\n    assert torch.allclose(output, expected_output), \"The outputs do not match\"',\n",
       "  100.0),\n",
       " ('def circle_circle_collision(circle1, circle2):\\n    \\n    \\n    radiiDiff = (circle1.size - circle2.size) ** 2\\n    radiiSum = (circle1.size + circle2.size) ** 2\\n    positionDiff = ((circle1.position[0] - circle2.position[0]) ** 2) + ((circle1.position[1] - circle2.position[1]) ** 2)\\n                \\n    # Compare radii; if expression is satisfied, that means the circles intersect\\n    if radiiDiff <= positionDiff and positionDiff <= radiiSum:\\n        return True\\n    else:\\n        return False',\n",
       "  'import sys\\nsys.path.append(\".\") # to import source.py from the same directory\\nfrom source import Circle\\n\\ndef test_circle_circle_collision():\\n    # Creating two circles\\n    circle1 = Circle(1, (0, 0))\\n    circle2 = Circle(1, (3, 4))\\n    assert circle_circle_collision(circle1, circle2) == True\\n\\n    circle1 = Circle(2, (0, 0))\\n    circle2 = Circle(1, (1, 1))\\n    assert circle_circle_collision(circle1, circle2) == True\\n\\n    circle1 = Circle(2, (0, 0))\\n    circle2 = Circle(1, (2, 3))\\n    assert circle_circle_collision(circle1, circle2) == False\\n\\n    circle1 = Circle(3, (1, 2))\\n    circle2 = Circle(2, (3, 4))\\n    assert circle_circle_collision(circle1, circle2) == False\\n\\n    circle1 = Circle(3, (0, 0))\\n    circle2 = Circle(2, (1, 1))\\n    assert circle_circle_collision(circle1, circle2) == False',\n",
       "  14.0),\n",
       " ('def reweight(op, fe, b_current, b_new):\\n\\n    r\\n\\n    assert len(op) == len(fe), \"op and fe must be of same length\"\\n    \\n    return fe - (b_new - b_current) * op',\n",
       "  'import sys\\nsys.path.append(\\'.\\')  # Adds current directory to Python path\\nfrom source import reweight\\n\\ndef test_reweight():\\n    op = [1, 2, 3, 4]\\n    fe = [5, 6, 7, 8]\\n    b_current = 2\\n    b_new = 4\\n\\n    assert len(op) == len(fe), \"op and fe must be of same length\"\\n\\n    assert reweight(op, fe, b_current, b_new) == [4, 5, 7, 8]',\n",
       "  50.0),\n",
       " ('def receptive_field_pool(kernel, n0=1, shift_n=0, n_lyrs=1):\\n    \\n    n1 = n0 * kernel**n_lyrs\\n    if shift_n > 1:  # shift_n is the Nst pooling lyr\\n        s = 1 + 2**shift_n\\n    else:\\n        s = shift_n\\n    return n1, s',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), \\'..\\'))\\nfrom source import receptive_field_pool\\n\\ndef test_receptive_field_pool():\\n    n1, s = receptive_field_pool(2, 2, 1, 2)\\n    assert n1 == 4, \"n1 is not correctly computed\"\\n    assert s == 3, \"s is not correctly computed\"\\n\\ndef test_receptive_field_pool_shift_n_greater_than_1():\\n    n1, s = receptive_field_pool(2, 2, 3, 2)\\n    assert n1 == 8, \"n1 is not correctly computed when shift_n > 1\"\\n    assert s == 7, \"s is not correctly computed when shift_n > 1\"\\n\\ndef test_receptive_field_pool_defaults():\\n    n1, s = receptive_field_pool(2)\\n    assert n1 == 2, \"n1 is not correctly computed with default values\"\\n    assert s == 0, \"s is not correctly computed with default values\"',\n",
       "  100.0),\n",
       " (\"def nondimensionalise(rc, qc, rho, x, nature):\\n    \\n    if nature == 'time':\\n        x = x*qc/rc**3\\n    elif nature == 'area':\\n        x = x/rc**2\\n    elif nature == 'flow':\\n        x = x/qc\\n    elif nature == 'pressure':\\n        x = x*rc**4/rho/qc**2\\n    return x\",\n",
       "  'import sys\\nsys.path.append(\".\") # to import source.py from the same directory\\nfrom source import nondimensionalise  # importing the function from source.py\\nimport pytest\\n\\ndef test_nondimensionalise_time():\\n    assert nondimensionalise(1, 1, 1, 1, \\'time\\') == 1\\n\\ndef test_nondimensionalise_area():\\n    assert nondimensionalise(1, 1, 1, 1, \\'area\\') == 1\\n\\ndef test_nondimensionalise_flow():\\n    assert nondimensionalise(1, 1, 1, 1, \\'flow\\') == 1\\n\\ndef test_nondimensionalise_pressure():\\n    assert nondimensionalise(1, 1, 1, 1, \\'pressure\\') == 1',\n",
       "  100.0),\n",
       " ('def chlorides_to_NaCl(salinity_chlorides):\\n    \\n    return salinity_chlorides * 1.645',\n",
       "  'import pytest\\nimport os\\nimport source\\n\\ndef test_chlorides_to_NaCl():\\n    assert source.chlorides_to_NaCl(1) == 1.645',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef argSoftmax2d(tensor, temperature, num_samples=1, return_1d_idx=False):\\n    \\n    n = tensor.size(0)\\n    d = tensor.size(2)\\n    probs = (tensor / temperature).view(n, -1).softmax(dim=-1)\\n    m = torch.multinomial(probs, num_samples)\\n    if not return_1d_idx:\\n        return torch.cat(((m // d).view(-1, 1), (m % d).view(-1, 1)), dim=1).long()\\n    else:\\n        return torch.cat(((m // d).view(-1, 1), (m % d).view(-1, 1)), dim=1).long(), m.long()',\n",
       "  'import pytest\\nimport torch\\nfrom source import argSoftmax2d\\n\\ndef test_argSoftmax2d():\\n    tensor = torch.randn(2, 2, 2)\\n    temperature = 1.0\\n    num_samples = 1\\n    return_1d_idx = False\\n    expected_shape = torch.Size([2, 2])\\n    output, _ = argSoftmax2d(tensor, temperature, num_samples, return_1d_idx)\\n    assert output.shape == expected_shape, \"Output shape doesn\\'t match expected shape\"\\n    assert torch.allclose(output.sum(dim=1), torch.tensor([1.0, 1.0])), \"Softmax2d didn\\'t normalize output\"\\nif __name__ == \\'__main__\\':\\n    test_argSoftmax2d()',\n",
       "  89.0),\n",
       " ('def check_valid_start_instance(start_instance):\\n    \\n    if not isinstance(start_instance, (str, bool)):\\n        raise ValueError(\"The value \\'start_instance\\' should be an string or a boolean.\")\\n\\n    if isinstance(start_instance, bool):\\n        return start_instance\\n\\n    if start_instance.lower() not in [\"true\", \"false\"]:\\n        raise ValueError(\\n            f\"The value \\'start_instance\\' should be equal to \\'True\\' or \\'False\\' (case insensitive).\"\\n        )\\n\\n    return start_instance.lower() == \"true\"',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")  # To import the source.py file in the same directory\\nfrom source import check_valid_start_instance\\n\\ndef test_check_valid_start_instance_str():\\n    assert check_valid_start_instance(\"Test\") == False\\n\\ndef test_check_valid_start_instance_bool():\\n    assert check_valid_start_instance(True) == True\\n\\ndef test_check_valid_start_instance_bool_lower():\\n    assert check_valid_start_instance(\"true\") == True\\n\\ndef test_check_valid_start_instance_invalid_type():\\n    with pytest.raises(ValueError):\\n        check_valid_start_instance(123)\\n\\ndef test_check_valid_start_instance_invalid_value():\\n    with pytest.raises(ValueError):\\n        check_valid_start_instance(\"test\")',\n",
       "  88.0),\n",
       " ('import torch\\n\\ndef qrot(q, v):\\n    \\n    assert q.shape[-1] == 4\\n    assert v.shape[-1] == 3\\n    assert q.shape[:-1] == v.shape[:-1]\\n    assert q.is_cuda == v.is_cuda\\n\\n    original_shape = list(v.shape)\\n    q = q.contiguous().view(-1, 4)\\n    v = v.contiguous().view(-1, 3)\\n\\n    qvec = q[:, 1:]\\n    uv = torch.cross(qvec, v, dim=1)\\n    uuv = torch.cross(qvec, uv, dim=1)\\n    return (v + 2 * (q[:, :1] * uv + uuv)).view(original_shape)',\n",
       "  'import torch\\nimport pytest\\nfrom source import qrot\\n\\ndef test_qrot():\\n    q = torch.randn(2, 4, device=\\'cuda\\')\\n    v = torch.randn(2, 3, device=\\'cuda\\')\\n    try:\\n        qrot(q, v)\\n    except AssertionError as e:\\n        pytest.fail(f\"Assertion error: {e}\")\\n\\n    q = torch.randn(2, 4)\\n    v = torch.randn(2, 3)\\n    try:\\n        qrot(q, v)\\n    except AssertionError as e:\\n        pytest.fail(f\"Assertion error: {e}\")',\n",
       "  100.0),\n",
       " (\"def antoine(phase, A, B, C, pore_temperature='pore.temperature', **kwargs):\\n    r\\n    T = phase[pore_temperature] - 273.15\\n    value = (10**(A-B/(C+T)))/760*101325\\n    return value\",\n",
       "  '# test_antoine.py\\nimport sys\\nsys.path.append(\".\")\\nfrom source import antoine\\n\\ndef test_antoine():\\n    phase = {\\'pore.temperature\\': 300}\\n    A = 1\\n    B = 1\\n    C = 1\\n    pore_temperature = \\'pore.temperature\\'\\n    expected_value = 0.0000056554\\n    assert antoine(phase, A, B, C, pore_temperature) == expected_value',\n",
       "  40.0),\n",
       " ('def bottomFraction(image, fraction=0.05):\\n    \\n    return image[int((1 - fraction) * image.shape[0]):]',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import bottomFraction\\nimport numpy as np\\n\\ndef test_bottomFraction():\\n    # creating a dummy image\\n    image = np.random.randint(0, 255, (100, 100))\\n    \\n    # using the function and checking the shape\\n    result = bottomFraction(image)\\n    assert result.shape[0] == image.shape[0] - int(0.05 * image.shape[0]), \"The shape of the output is not correct\"\\n\\n    # using the function with fraction as argument and checking the shape\\n    result = bottomFraction(image, fraction=0.1)\\n    assert result.shape[0] == image.shape[0] - int(0.1 * image.shape[0]), \"The shape of the output is not correct when fraction is 0.1\"',\n",
       "  100.0),\n",
       " (\"import torch\\n\\ndef mu_law_expanding(x_mu, qc):\\n    # type: (Tensor, int) -> Tensor\\n    \\n    assert isinstance(x_mu, torch.Tensor), 'mu_law_expanding expects a Tensor'\\n    mu = qc - 1.\\n    if not x_mu.is_floating_point():\\n        x_mu = x_mu.to(torch.float)\\n    mu = torch.tensor(mu, dtype=x_mu.dtype)\\n    x = ((x_mu) / mu) * 2 - 1.\\n    x = torch.sign(x) * (torch.exp(torch.abs(x) * torch.log1p(mu)) - 1.) / mu\\n    return x\",\n",
       "  \"import torch\\nimport pytest\\nfrom source import mu_law_expanding\\n\\ndef test_mu_law_expanding_tensor():\\n    x_mu = torch.tensor([1., 2., 3.])\\n    qc = 2\\n    expected_output = torch.tensor([1.53846913, 2.36220105, 3.1946913])\\n    output = mu_law_expanding(x_mu, qc)\\n    assert torch.allclose(output, expected_output), 'The function did not produce the expected output'\\n\\ndef test_mu_law_expanding_integer():\\n    x_mu = torch.tensor([1, 2, 3])\\n    qc = 2\\n    expected_output = torch.tensor([1.53846913, 2.36220105, 3.1946913])\\n    output = mu_law_expanding(x_mu, qc)\\n    assert torch.allclose(output, expected_output), 'The function did not produce the expected output'\\n\\ndef test_mu_law_expanding_float():\\n    x_mu = torch.tensor([1., 2., 3.])\\n    qc = 2\\n    expected_output = torch.tensor([1.53846913, 2.36220105, 3.1946913])\\n    output = mu_law_expanding(x_mu, qc)\\n    assert torch.allclose(output, expected_output), 'The function did not produce the expected output'\\n\\ndef test_mu_law_expanding_failure():\\n    x_mu = torch.tensor([1., 2., 3.])\\n    qc = '2'\\n    output = mu_law_expanding(x_mu, qc)\\n    assert not torch.is_tensor(output), 'The function did not produce a tensor output as expected'\\n\\ndef test_mu_law_expanding_dtype():\\n    x_mu = torch.tensor([1., 2.], dtype=torch.int32)\\n    qc = 2\\n    output = mu_law_expanding(x_mu, qc)\\n    assert output.dtype == x_mu.dtype, 'The function did not preserve the dtype of the input'\",\n",
       "  100.0),\n",
       " ('def denormalize_img(images):\\n    \\n    images = ((images * 255) + 255. / 2.)\\n    return images',\n",
       "  '# test_denormalize_img.py\\nimport sys\\nsys.path.append(\".\")  # To import source.py from the same directory\\nfrom source import denormalize_img\\n\\ndef test_denormalize_img():\\n    # Test with known inputs\\n    images = [[0.0, 0.5, 1.0], [0.3, 0.6, 0.9]]\\n    expected_output = [[0.0, 127.5, 255.0], [75.75, 163.83, 237.5]]\\n    assert denormalize_img(images) == expected_output\\n\\ndef test_denormalize_img_exception():\\n    # Test with invalid input (non-list)\\n    with pytest.raises(TypeError):\\n        denormalize_img(\"not a list\")\\n\\ndef test_denormalize_img_exception():\\n    # Test with invalid input (sublist not a list or too short)\\n    with pytest.raises(ValueError):\\n        denormalize_img([[0.0], [0.5, 1.0]])',\n",
       "  67.0),\n",
       " ('import torch\\n\\ndef quaternion_to_matrix(quaternions):\\n    \\n    r, i, j, k = torch.unbind(quaternions, -1)\\n    two_s = 2.0 / (quaternions * quaternions).sum(-1)\\n\\n    o = torch.stack(\\n        (\\n            1 - two_s * (j * j + k * k),\\n            two_s * (i * j - k * r),\\n            two_s * (i * k + j * r),\\n            two_s * (i * j + k * r),\\n            1 - two_s * (i * i + k * k),\\n            two_s * (j * k - i * r),\\n            two_s * (i * k - j * r),\\n            two_s * (j * k + i * r),\\n            1 - two_s * (i * i + j * j),\\n        ),\\n        -1,\\n    )\\n    return o.reshape(quaternions.shape[:-1] + (3, 3))',\n",
       "  'import pytest\\nimport torch\\nfrom source import quaternion_to_matrix\\n\\ndef test_quaternion_to_matrix():\\n    quaternions = torch.randn(10, 4)\\n    result = quaternion_to_matrix(quaternions)\\n    assert isinstance(result, torch.Tensor)\\n    assert result.shape == quaternions.shape[:-1] + (3, 3)',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef one_hot(batched_tensor, num_class):\\n    \\n    one_hot = torch.zeros((batched_tensor.size()[0], num_class))\\n    one_hot[torch.arange(batched_tensor.size()[0]), batched_tensor.long()] = 1\\n    return one_hot',\n",
       "  'import pytest\\nimport torch\\nfrom source import one_hot  # Assuming the function is in source.py\\n\\ndef test_one_hot():\\n    tensor = torch.tensor([0, 1, 2, 3])\\n    num_class = 4\\n    assert torch.allclose(one_hot(tensor, num_class), torch.tensor([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]]))\\n\\nif __name__ == \"__main__\":\\n    pytest.main()',\n",
       "  100.0),\n",
       " ('def eigenspectrum(operator, n_qubits=None):\\n    \\n    from openfermion.transforms import get_sparse_operator\\n    from openfermion.utils import sparse_eigenspectrum\\n    sparse_operator = get_sparse_operator(operator, n_qubits)\\n    eigenspectrum = sparse_eigenspectrum(sparse_operator)\\n    return eigenspectrum',\n",
       "  '# test_source.py\\nimport pytest\\nfrom source import eigenspectrum\\n\\n# Sample test case: \\n# Test if the function returns expected output for a given input\\ndef test_eigenspectrum():\\n    operator = \"some operator\"\\n    expected_output = \"expected output\"\\n    assert eigenspectrum(operator) == expected_output',\n",
       "  33.0),\n",
       " ('def check_mse_efficiency_nu(gamma, nu, alpha, beta, y):\\n    \\n    gamma, nu, alpha, beta = gamma.detach(), nu.detach(), alpha.detach(), beta.detach()\\n    nu_1 = (nu+1)/nu\\n    return (beta*nu_1/alpha)',\n",
       "  'import pytest\\n\\ndef test_check_mse_efficiency_nu():\\n    # As we cannot provide actual tensors, we will use simple integers for this test.\\n    # Our goal is to cover the case where gamma, nu, alpha, beta, y are all tensors with the same shape\\n    # We\\'ll assume the shape of these tensors to be (3,) for simplicity\\n\\n    gamma = torch.tensor([1, 2, 3], dtype=torch.float)\\n    nu = torch.tensor([4, 5, 6], dtype=torch.float)\\n    alpha = torch.tensor([7, 8, 9], dtype=torch.float)\\n    beta = torch.tensor([10, 11, 12], dtype=torch.float)\\n    y = torch.tensor([13, 14, 15], dtype=torch.float)\\n\\n    result = check_mse_efficiency_nu(gamma, nu, alpha, beta, y)\\n    \\n    # since all tensors are simply integers here, we can directly compare the result with the expected result\\n    expected_result = torch.tensor([(11/4), (13/6), (15/9)], dtype=torch.float)\\n    \\n    # We will use close() function for comparison, as it allows a small degree of tolerance for floating point errors\\n    assert torch.allclose(result, expected_result), f\"Expected {expected_result}, but got {result}\"',\n",
       "  0.0),\n",
       " (\"def _find_sis_from_backselect(backselect_stack, threshold):\\n    \\n    if not backselect_stack:\\n        raise ValueError('backselect_stack cannot be empty.')\\n\\n    sis = []\\n    stack_iter = reversed(backselect_stack)\\n    i, value = next(stack_iter)\\n    if value < threshold:\\n        sis.append(i)\\n    for i, value in stack_iter:\\n        if value >= threshold:\\n            break\\n        else:\\n            sis.append(i)\\n\\n    return sis\",\n",
       "  'import pytest\\nfrom source import _find_sis_from_backselect\\n\\ndef test__find_sis_from_backselect():\\n    backselect_stack = [(0, 5), (1, 3), (2, 7), (3, 1), (4, 6), (5, 9), (6, 4)]\\n    threshold = 6\\n    expected_result = [0, 1, 2, 3]\\n    assert _find_sis_from_backselect(backselect_stack, threshold) == expected_result',\n",
       "  85.0),\n",
       " ('def standard_approximation(mean, std):\\n    \\n    median = mean\\n    iqr = std * 1.35\\n\\n    return median, iqr',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\"..\") # To import source.py file in the same directory\\nfrom source import standard_approximation\\n\\ndef test_standard_approximation():\\n    assert standard_approximation(1, 0)[0] == 1\\n    assert standard_approximation(1, 1)[1] == 1.35\\n    assert standard_approximation(1, 2)[1] == 2.7\\n    assert standard_approximation(1, -1)[1] == -1.35',\n",
       "  100.0),\n",
       " ('def _padded_hex(i, pad_width=4, uppercase=True):\\n    \\n    result = hex(i)[2:]  # Remove the leading \"0x\"\\n    if uppercase:\\n        result = result.upper()\\n    return result.zfill(pad_width)',\n",
       "  \"import source\\n\\ndef test_padded_hex():\\n    assert source._padded_hex(10) == 'A'\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef hard_example_mining(dist_mat, labels, return_inds=False):\\n    \\n\\n    assert len(dist_mat.size()) == 2\\n    assert dist_mat.size(0) == dist_mat.size(1)\\n    N = dist_mat.size(0)\\n\\n    # shape [N, N]\\n    is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\\n    is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\\n\\n    # `dist_ap` means distance(anchor, positive)\\n    # both `dist_ap` and `relative_p_inds` with shape [N, 1]\\n    dist_ap, relative_p_inds = torch.max(\\n        dist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\\n    # `dist_an` means distance(anchor, negative)\\n    # both `dist_an` and `relative_n_inds` with shape [N, 1]\\n    dist_an, relative_n_inds = torch.min(\\n        dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\\n    # shape [N]\\n    dist_ap = dist_ap.squeeze(1)\\n    dist_an = dist_an.squeeze(1)\\n\\n    if return_inds:\\n        # shape [N, N]\\n        ind = (labels.new().resize_as_(labels)\\n               .copy_(torch.arange(0, N).long())\\n               .unsqueeze(0).expand(N, N))\\n        # shape [N, 1]\\n        p_inds = torch.gather(\\n            ind[is_pos].contiguous().view(N, -1), 1, relative_p_inds.data)\\n        n_inds = torch.gather(\\n            ind[is_neg].contiguous().view(N, -1), 1, relative_n_inds.data)\\n        # shape [N]\\n        p_inds = p_inds.squeeze(1)\\n        n_inds = n_inds.squeeze(1)\\n        return dist_ap, dist_an, p_inds, n_inds\\n\\n    return dist_ap, dist_an',\n",
       "  \"import pytest\\nimport torch\\n\\ndef test_hard_example_mining():\\n    dist_mat = torch.rand((10, 10))\\nlabels = torch.rand((10,))\\nassert hard_example_mining(dist_mat, labels, return_inds=False)\\n    dist_mat = torch.rand((10, 10))\\n    labels = torch.rand((10,))\\n    _, _, p_inds, n_inds = hard_example_mining(dist_mat, labels, return_inds=True)\\n    assert p_inds.shape == torch.Size([10])\\n    assert n_inds.shape == torch.Size([10])\\n    dist_mat = torch.rand((10, 10))\\n    labels = torch.rand((10,))\\n    dist_ap, dist_an = hard_example_mining(dist_mat, labels, return_inds=False)\\n    assert dist_ap.shape == torch.Size([10])\\n    assert dist_an.shape == torch.Size([10])\\nif __name__ == '__main__':\\n    test_hard_example_mining()\",\n",
       "  0.0),\n",
       " ('def select_span(cropped_length, original_length, center_point):\\n    \\n    original_center_point = original_length * center_point # center_point is a float 0..1\\n    ideal_center_point = cropped_length / 2\\n    leeway = original_length - cropped_length\\n    ideal_crop_origin = original_center_point - ideal_center_point\\n    clamped = min(max(ideal_crop_origin, 0), leeway)\\n    return round(clamped)',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('.')  # Adds the current directory to the Python path\\nimport source  # Replace with the correct Python file name\\n\\ndef test_select_span():\\n    assert source.select_span(100, 200, 0.5) == 50\\n    assert source.select_span(100, 200, 0.75) == 75\\n    assert source.select_span(100, 200, 1) == 100\\n    assert source.select_span(50, 100, 0.5) == 25\\n    assert source.select_span(50, 100, 0.25) == 0\",\n",
       "  100.0),\n",
       " ('def to_two_line_notation(permutation):\\n    r\\n    n = len(permutation)\\n    return tuple(range(n)), permutation',\n",
       "  \"# Import the function that we're going to test\\nfrom source import to_two_line_notation\\n\\n# Define a test case\\ndef test_to_two_line_notation():\\n    # Define the input\\n    permutation = [1,2,3]\\n    \\n    # Call the function with the input\\n    result = to_two_line_notation(permutation)\\n    \\n    # Make an assertion about the result\\n    assert result == (tuple(range(len(permutation))), permutation)\",\n",
       "  50.0),\n",
       " ('def getNegBinomParams(mu, alpha):\\n    \\n    var = mu + alpha * mu ** 2\\n    p = (var - mu) / var\\n    r = mu ** 2 / (var - mu)\\n    return r, p',\n",
       "  '# import the function to test from source.py\\nfrom source import getNegBinomParams\\nimport pytest\\n\\n# create a test case for the function\\ndef test_getNegBinomParams():\\n    # create assertions to test the function\\n    assert getNegBinomParams(1, 1) == (0.5, 0.5)\\n    assert getNegBinomParams(2, 1) == (0.75, 0.25)\\n    assert getNegBinomParams(3, 2) == (1.0, 0.0)\\n    assert getNegBinomParams(4, 2) == (1.3333333333333334, 0.3333333333333334)\\n    assert getNegBinomParams(0, 1) == (0.0, 1.0)',\n",
       "  100.0),\n",
       " ('def pad(size, padding):\\n    \\n    width = size[0] + padding.left + padding.right\\n    height = size[1] + padding.top + padding.bottom\\n    return (width, height)',\n",
       "  '# test_pad.py\\nimport sys\\nsys.path.append(\"..\") # to include the parent directory in the import path\\n\\nimport pytest\\nfrom source import pad\\nfrom source import Padding \\n\\nclass TestPad:\\n\\n    @pytest.fixture\\n    def size(self):\\n        return (5, 7)\\n\\n    @pytest.fixture\\n    def padding(self):\\n        return Padding(2, 3)\\n\\n    def test_pad_width(self, size, padding):\\n        result = pad(size, padding)\\n        assert result[0] == size[0] + padding.left + padding.right\\n\\n    def test_pad_height(self, size, padding):\\n        result = pad(size, padding)\\n        assert result[1] == size[1] + padding.top + padding.bottom',\n",
       "  25.0),\n",
       " ('def linear_derivative(y):\\n    \\n    return 1',\n",
       "  \"import sys\\nsys.path.append('.')\\nimport source\\n\\ndef test_linear_derivative():\\n    result = source.linear_derivative([1, 2, 3, 4, 5])\\n    assert result == [1, 2, 3, 4, 5], 'The function linear_derivative() did not return the expected output'\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef hard_example_mining(dist_mat, labels, return_inds=False):\\n    \\n\\n    assert len(dist_mat.size()) == 2\\n    assert dist_mat.size(0) == dist_mat.size(1)\\n    N = dist_mat.size(0)\\n\\n    # shape [N, N]\\n    is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\\n    is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\\n\\n    # `dist_ap` means distance(anchor, positive)\\n    # both `dist_ap` and `relative_p_inds` with shape [N, 1]\\n    dist_ap, relative_p_inds = torch.max(\\n        dist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\\n    # `dist_an` means distance(anchor, negative)\\n    # both `dist_an` and `relative_n_inds` with shape [N, 1]\\n    dist_an, relative_n_inds = torch.min(\\n        dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\\n    # shape [N]\\n    dist_ap = dist_ap.squeeze(1)\\n    dist_an = dist_an.squeeze(1)\\n\\n    if return_inds:\\n        # shape [N, N]\\n        ind = (labels.new().resize_as_(labels)\\n               .copy_(torch.arange(0, N).long())\\n               .unsqueeze(0).expand(N, N))\\n        # shape [N, 1]\\n        p_inds = torch.gather(\\n            ind[is_pos].contiguous().view(N, -1), 1, relative_p_inds.data)\\n        n_inds = torch.gather(\\n            ind[is_neg].contiguous().view(N, -1), 1, relative_n_inds.data)\\n        # shape [N]\\n        p_inds = p_inds.squeeze(1)\\n        n_inds = n_inds.squeeze(1)\\n        return dist_ap, dist_an, p_inds, n_inds\\n\\n    return dist_ap, dist_an',\n",
       "  'import pytest\\nimport torch\\n\\ndef test_hard_example_mining():\\n    # Create mock data\\n    dist_mat = torch.randn(10, 10)\\n    labels = torch.randn(10).long()\\n\\n    # Call the function with the mock data\\n    dist_ap, dist_an = hard_example_mining(dist_mat, labels)\\n\\n    # Check if the function returned the expected output\\n    assert isinstance(dist_ap, torch.Tensor)\\n    assert isinstance(dist_an, torch.Tensor)\\n    assert dist_ap.shape == dist_an.shape\\n    assert dist_ap.shape == torch.Size([10])\\n\\n    # Test with `return_inds` = True\\n    dist_ap, dist_an, p_inds, n_inds = hard_example_mining(dist_mat, labels, return_inds=True)\\n\\n    # Check if the function returned the expected output\\n    assert isinstance(p_inds, torch.Tensor)\\n    assert isinstance(n_inds, torch.Tensor)\\n    assert p_inds.shape == n_inds.shape\\n    assert p_inds.shape == torch.Size([10])',\n",
       "  0.0),\n",
       " ('def clamp_value(value, max_value=1.0, min_value=0.0):\\n    \\n    return max(min(value, max_value), min_value)',\n",
       "  'import pytest\\nfrom source import clamp_value\\n\\ndef test_clamp_value():\\n    assert clamp_value(0.5) == 0.5\\n    assert clamp_value(2) == 1.0\\n    assert clamp_value(-1) == 0.0',\n",
       "  100.0),\n",
       " ('def sample_to_seconds(sample, sampling_rate=16000):\\n    \\n\\n    return sample / sampling_rate',\n",
       "  '# test_source.py\\n\\nimport pytest\\nfrom source import sample_to_seconds\\n\\ndef test_sample_to_seconds():\\n    result = sample_to_seconds(1000, 16000)\\n    assert result == 0.0625, \"The function did not return the expected result.\"',\n",
       "  100.0),\n",
       " ('def rgbToHsv(r, g, b):\\n    \\n    r /= 255.0\\n    g /= 255.0\\n    b /= 255.0\\n\\n    mx = max(r, g, b)\\n    mn = min(r, g, b)\\n    v = mx\\n    d = mx - mn\\n    s = 0 if mx == 0 else d / mx\\n    h = None\\n\\n    if mx == mn:\\n        h = 0\\n        # achromatic\\n        return [h, s, v]\\n\\n    if r == mx:\\n        h = (60 * ((g - b) / d) + 360) % 360\\n    if g == mx:\\n        h = (60 * ((b - r) / d) + 120) % 360\\n    if b == mx:\\n        h = (60 * ((r - g) / d) + 240) % 360\\n\\n    return [round(h) % 360, s, v]',\n",
       "  'import pytest\\nfrom source import rgbToHsv\\n\\ndef test_rgbToHsv_red():\\n    assert rgbToHsv(255, 0, 0) == [0, 1, 1]\\n\\ndef test_rgbToHsv_green():\\n    assert rgbToHsv(0, 255, 0) == [120, 1, 1]\\n\\ndef test_rgbToHsv_blue():\\n    assert rgbToHsv(0, 0, 255) == [240, 1, 1]\\n\\ndef test_rgbToHsv_white():\\n    assert rgbToHsv(255, 255, 255) == [0, 0, 1]\\n\\ndef test_rgbToHsv_black():\\n    assert rgbToHsv(0, 0, 0) == [0, 0, 0]\\n\\ndef test_rgbToHsv_yellow():\\n    assert rgbToHsv(255, 255, 0) == [60, 1, 1]\\n\\ndef test_rgbToHsv_cyan():\\n    assert rgbToHsv(0, 255, 255) == [180, 1, 1]\\n\\ndef test_rgbToHsv_magenta():\\n    assert rgbToHsv(255, 0, 255) == [300, 1, 1]',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef hard_example_mining(dist_mat, labels, return_inds=False):\\n    \\n\\n    assert len(dist_mat.size()) == 2\\n    assert dist_mat.size(0) == dist_mat.size(1)\\n    N = dist_mat.size(0)\\n\\n    # shape [N, N]\\n    is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\\n    is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\\n\\n    # `dist_ap` means distance(anchor, positive)\\n    # both `dist_ap` and `relative_p_inds` with shape [N, 1]\\n    dist_ap, relative_p_inds = torch.max(\\n        dist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\\n    # `dist_an` means distance(anchor, negative)\\n    # both `dist_an` and `relative_n_inds` with shape [N, 1]\\n    dist_an, relative_n_inds = torch.min(\\n        dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\\n    # shape [N]\\n    dist_ap = dist_ap.squeeze(1)\\n    dist_an = dist_an.squeeze(1)\\n\\n    if return_inds:\\n        # shape [N, N]\\n        ind = (labels.new().resize_as_(labels)\\n               .copy_(torch.arange(0, N).long())\\n               .unsqueeze(0).expand(N, N))\\n        # shape [N, 1]\\n        p_inds = torch.gather(\\n            ind[is_pos].contiguous().view(N, -1), 1, relative_p_inds.data)\\n        n_inds = torch.gather(\\n            ind[is_neg].contiguous().view(N, -1), 1, relative_n_inds.data)\\n        # shape [N]\\n        p_inds = p_inds.squeeze(1)\\n        n_inds = n_inds.squeeze(1)\\n        return dist_ap, dist_an, p_inds, n_inds\\n\\n    return dist_ap, dist_an',\n",
       "  'import torch\\nimport pytest\\n\\nfrom source import hard_example_mining\\n\\ndef test_hard_example_mining():\\n    # set a random seed for reproducibility\\n    torch.manual_seed(1234)\\n    # generate random inputs\\n    dist_mat = torch.randn(10, 10)\\n    labels = torch.randint(0, 2, (10,))\\n    # call the function with these inputs\\n    _, _ = hard_example_mining(dist_mat, labels)\\n    # add more tests here',\n",
       "  42.0),\n",
       " (\"def axPos2figPos(ax, value, direction='x'):\\n    \\n    if direction == 'x':\\n        point1 = (ax.get_xlim()[0], ax.get_position().xmin)\\n        point2 = (ax.get_xlim()[1], ax.get_position().xmax)\\n    else:\\n        point1 = (ax.get_ylim()[0], ax.get_position().ymin)\\n        point2 = (ax.get_ylim()[1], ax.get_position().ymax)\\n    delta = (point2[1]-point1[1])/(point2[0]-point1[0])\\n    x0 = point2[1] - (delta*point2[0])\\n\\n    return x0 + delta*value\",\n",
       "  \"import pytest\\nimport matplotlib.pyplot as plt\\nimport source\\n\\ndef test_axPos2figPos_x():\\n    fig, ax = plt.subplots()\\n    ax.set_xlim([0, 1])\\n    ax.set_ylim([0, 1])\\n    ax.set_position([0, 0, 1, 1])\\n    assert source.axPos2figPos(ax, 0.5, 'x') == 0.5\\n\\ndef test_axPos2figPos_y():\\n    fig, ax = plt.subplots()\\n    ax.set_xlim([0, 1])\\n    ax.set_ylim([0, 1])\\n    ax.set_position([0, 0, 1, 1])\\n    assert source.axPos2figPos(ax, 0.5, 'y') == 0.5\",\n",
       "  100.0),\n",
       " ('import numpy\\n\\ndef draw_mask_over_image(image: numpy.ndarray, mask: numpy.ndarray, color=None):\\n    \\n    if color is None:\\n        color = [70, 253, 52]\\n\\n    colormask = image.copy()\\n    colormask[mask[:, :] == True] = color\\n\\n    return colormask',\n",
       "  'import numpy\\nimport pytest\\nfrom source import draw_mask_over_image\\n\\ndef test_draw_mask_over_image():\\n    image = numpy.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\\n    mask = numpy.array([[True, False, True], [False, True, False], [True, False, True]])\\n    expected_output = numpy.array([[70, 253, 52], [0, 1, 70], [70, 253, 52]])\\n    with pytest.raises(RuntimeError):\\n        output = draw_mask_over_image(image, mask)\\n    assert numpy.array_equal(output, expected_output)\\n\\ndef test_draw_mask_over_image_with_color():\\n    image = numpy.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\\n    mask = numpy.array([[True, False, True], [False, True, False], [True, False, True]])\\n    color = [255, 0, 0]\\n    expected_output = numpy.array([[255, 0, 0], [0, 1, 255], [255, 0, 0]])\\n    output = draw_mask_over_image(image, mask, color)\\n    assert numpy.array_equal(output, expected_output)',\n",
       "  86.0),\n",
       " ('def bsm_vega(S0, K, T, r, sigma):\\n    \\n    from math import log, sqrt\\n    from scipy import stats\\n\\n    S0 = float(S0)\\n\\n    d1 = (log(S0 / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * sqrt(T))\\n    vega = S0 * stats.norm.pdf(d1, 0.0, 1.0) * sqrt(T)\\n    return vega',\n",
       "  \"import pytest\\nimport math\\nimport scipy.stats as stats\\n\\ndef test_bsm_vega():\\n    import __main__ as test_module\\n    S0 = 100\\n    K = 100\\n    with pytest.raises(AttributeError):\\n        T = 1\\n    r = 0.05\\n    sigma = 0.2\\n    result = test_module.bsm_vega(S0, K, T, r, sigma)\\n    with pytest.raises(AttributeError):\\n        assert math.isclose(result, 0.05, abs_tol=0.01), 'Vega test failed'\",\n",
       "  0.0),\n",
       " (\"def dense_prediction_cell_hparams():\\n  \\n  return {\\n      'reduction_size': 256,\\n      'dropout_on_concat_features': True,\\n      'dropout_on_projection_features': False,\\n      'dropout_keep_prob': 0.9,\\n      'concat_channels': 256,\\n      'conv_rate_multiplier': 1,\\n  }\",\n",
       "  \"import source\\n\\ndef test_dense_prediction_cell_hparams():\\n    expected_output = {'reduction_size': 256, 'dropout_on_concat_features': True, 'dropout_on_projection_features': False, 'dropout_keep_prob': 0.9, 'concat_channels': 256, 'conv_rate_multiplier': 1}\\n    assert source.dense_prediction_cell_hparams() == expected_output\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef symmetric_epipolar_distance(pts1, pts2, fundamental_mat):\\n    \\n\\n    line_1 = torch.bmm(pts1, fundamental_mat)\\n    line_2 = torch.bmm(pts2, fundamental_mat.permute(0, 2, 1))\\n\\n    scalar_product = (pts2 * line_1).sum(2)\\n\\n    ret = scalar_product.abs() * (\\n        1 / line_1[:, :, :2].norm(2, 2) + 1 / line_2[:, :, :2].norm(2, 2)\\n    )\\n\\n    return ret',\n",
       "  \"import torch\\nimport numpy as np\\nimport sys\\nsys.path.append('.')\\nimport source  # Assuming the source code file is in same directory\\n\\ndef test_symmetric_epipolar_distance():\\n    pts1 = torch.rand((10, 1, 2))\\n    pts2 = torch.rand((10, 1, 2))\\n    fundamental_mat = torch.rand((1, 3, 3))\\n\\n    # Since we are assuming that the function works with 2D points pts1 and pts2,\\n    # let's create a random fundamental matrix for 2D points\\n    fundamental_mat = torch.Tensor([[[1, 0, 0], [0, 1, 0], [0, 0, 1]]])\\n\\n    res = source.symmetric_epipolar_distance(pts1, pts2, fundamental_mat)\\n    \\n    # Since we are dealing with floating point numbers, we can't compare the tensors directly.\\n    # Let's convert them to numpy arrays and compare the shapes, which should be the same.\\n    assert isinstance(res, torch.Tensor)\\n    assert res.shape == pts1.shape\",\n",
       "  43.0),\n",
       " ('import torch\\n\\ndef rotation_matrix_to_quaternion(rotation_matrix, eps=1e-6):\\n    \\n    if not torch.is_tensor(rotation_matrix):\\n        raise TypeError(\"Input type is not a torch.Tensor. Got {}\".format(\\n            type(rotation_matrix)))\\n\\n    if len(rotation_matrix.shape) > 3:\\n        raise ValueError(\\n            \"Input size must be a three dimensional tensor. Got {}\".format(\\n                rotation_matrix.shape))\\n    if not rotation_matrix.shape[-2:] == (3, 3):\\n        raise ValueError(\\n            \"Input size must be a N x 3 x 3  tensor. Got {}\".format(\\n                rotation_matrix.shape))\\n\\n    rmat_t = torch.transpose(rotation_matrix, 1, 2)\\n\\n    mask_d2 = rmat_t[:, 2, 2] < eps\\n\\n    mask_d0_d1 = rmat_t[:, 0, 0] > rmat_t[:, 1, 1]\\n    mask_d0_nd1 = rmat_t[:, 0, 0] < -rmat_t[:, 1, 1]\\n\\n    t0 = 1 + rmat_t[:, 0, 0] - rmat_t[:, 1, 1] - rmat_t[:, 2, 2]\\n    q0 = torch.stack([rmat_t[:, 1, 2] - rmat_t[:, 2, 1],\\n                      t0, rmat_t[:, 0, 1] + rmat_t[:, 1, 0],\\n                      rmat_t[:, 2, 0] + rmat_t[:, 0, 2]], -1)\\n    t0_rep = t0.repeat(4, 1).t()\\n\\n    t1 = 1 - rmat_t[:, 0, 0] + rmat_t[:, 1, 1] - rmat_t[:, 2, 2]\\n    q1 = torch.stack([rmat_t[:, 2, 0] - rmat_t[:, 0, 2],\\n                      rmat_t[:, 0, 1] + rmat_t[:, 1, 0],\\n                      t1, rmat_t[:, 1, 2] + rmat_t[:, 2, 1]], -1)\\n    t1_rep = t1.repeat(4, 1).t()\\n\\n    t2 = 1 - rmat_t[:, 0, 0] - rmat_t[:, 1, 1] + rmat_t[:, 2, 2]\\n    q2 = torch.stack([rmat_t[:, 0, 1] - rmat_t[:, 1, 0],\\n                      rmat_t[:, 2, 0] + rmat_t[:, 0, 2],\\n                      rmat_t[:, 1, 2] + rmat_t[:, 2, 1], t2], -1)\\n    t2_rep = t2.repeat(4, 1).t()\\n\\n    t3 = 1 + rmat_t[:, 0, 0] + rmat_t[:, 1, 1] + rmat_t[:, 2, 2]\\n    q3 = torch.stack([t3, rmat_t[:, 1, 2] - rmat_t[:, 2, 1],\\n                      rmat_t[:, 2, 0] - rmat_t[:, 0, 2],\\n                      rmat_t[:, 0, 1] - rmat_t[:, 1, 0]], -1)\\n    t3_rep = t3.repeat(4, 1).t()\\n\\n    mask_c0 = mask_d2 * mask_d0_d1\\n    mask_c1 = mask_d2 * (~mask_d0_d1)\\n    mask_c2 = (~mask_d2) * mask_d0_nd1\\n    mask_c3 = (~mask_d2) * (~mask_d0_nd1)\\n    mask_c0 = mask_c0.view(-1, 1).type_as(q0)\\n    mask_c1 = mask_c1.view(-1, 1).type_as(q1)\\n    mask_c2 = mask_c2.view(-1, 1).type_as(q2)\\n    mask_c3 = mask_c3.view(-1, 1).type_as(q3)\\n\\n    q = q0 * mask_c0 + q1 * mask_c1 + q2 * mask_c2 + q3 * mask_c3\\n    q /= torch.sqrt(t0_rep * mask_c0 + t1_rep * mask_c1 +  # noqa\\n                    t2_rep * mask_c2 + t3_rep * mask_c3)  # noqa\\n    q *= 0.5\\n    return q',\n",
       "  'import torch\\nimport pytest\\n\\ndef test_rotation_matrix_to_quaternion():\\n    # Create a tensor with random rotation matrix\\n    rotation_matrix = torch.rand((10, 3, 3))\\n\\n    # Call the function and get the result\\n    result = rotation_matrix_to_quaternion(rotation_matrix)\\n\\n    # Assertion\\n    assert torch.allclose(result, torch.rand_like(result))  # Replace with actual comparison\\n\\n# Run the test\\npytest.main([\"-k\", \"test_rotation_matrix_to_quaternion\"])',\n",
       "  0.0),\n",
       " ('def transform_before_surface(interface, ray_seg):\\n    \\n    if interface.decenter:\\n        # get transformation info after surf\\n        r, t = interface.decenter.tform_before_surf()\\n        if r is None:\\n            b4_pt, b4_dir = (ray_seg[0] - t), ray_seg[1]\\n        else:\\n            rt = r.transpose()\\n            b4_pt, b4_dir = rt.dot(ray_seg[0] - t), rt.dot(ray_seg[1])\\n    else:\\n        b4_pt, b4_dir = ray_seg[0], ray_seg[1]\\n\\n    return b4_pt, b4_dir',\n",
       "  '# source.py\\n\\nclass Interface:\\n    def __init__(self, decenter=None):\\n        self.decenter = decenter\\n\\n    def tform_before_surf(self):\\n        # This method is a placeholder and should be replaced with actual functionality\\n        return None, None\\n\\ndef transform_before_surface(interface, ray_seg):\\n    if interface.decenter:\\n        r, t = interface.decenter.tform_before_surf()\\n        if r is None:\\n            b4_pt, b4_dir = (ray_seg[0] - t), ray_seg[1]\\n        else:\\n            rt = r.transpose()\\n            b4_pt, b4_dir = rt.dot(ray_seg[0] - t), rt.dot(ray_seg[1])\\n    else:\\n        b4_pt, b4_dir = ray_seg[0], ray_seg[1]\\n\\n    return b4_pt, b4_dir',\n",
       "  0.0),\n",
       " ('import torch\\n\\ndef quaternion_to_matrix(quaternions):\\n    \\n    r, i, j, k = torch.unbind(quaternions, -1)\\n    two_s = 2.0 / (quaternions * quaternions).sum(-1)\\n\\n    o = torch.stack(\\n        (\\n            1 - two_s * (j * j + k * k),\\n            two_s * (i * j - k * r),\\n            two_s * (i * k + j * r),\\n            two_s * (i * j + k * r),\\n            1 - two_s * (i * i + k * k),\\n            two_s * (j * k - i * r),\\n            two_s * (i * k - j * r),\\n            two_s * (j * k + i * r),\\n            1 - two_s * (i * i + j * j),\\n        ),\\n        -1,\\n    )\\n    return o.reshape(quaternions.shape[:-1] + (3, 3))',\n",
       "  \"import torch\\nimport sys\\nsys.path.insert(0, '../') # Assuming source.py is in the same directory\\nfrom source import quaternion_to_matrix\\n\\ndef test_quaternion_to_matrix():\\n    quaternions = torch.tensor([[1.0, 2.0, 3.0, 4.0]])\\n    result = quaternion_to_matrix(quaternions)\\n    expected_result = torch.tensor(\\n        [\\n            [1.0, -2.0, 3.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0],\\n            [0.0, 2.0, -3.0, 4.0, -1.0, 0.0, 0.0, 0.0, 0.0],\\n            [0.0, 0.0, 3.0, 4.0, 1.0, -2.0, 0.0, 0.0, 0.0],\\n            [0.0, 0.0, 0.0, 1.0, 0.0, 2.0, -3.0, 4.0, -1.0],\\n            [0.0, 0.0, 0.0, 0.0, -1.0, 2.0, 3.0, 4.0, 0.0],\\n            [0.0, 0.0, 0.0, 0.0, 0.0, -2.0, 1.0, 3.0, 4.0],\\n            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 4.0, -1.0],\\n            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0],\\n        ]\\n    )\\n    assert torch.allclose(result, expected_result)\",\n",
       "  100.0),\n",
       " ('def relative_error(x_0: float, x: float):\\n    r\\n    return (x_0 - x) / x',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\") # add the directory containing source.py to the path\\nimport source \\n\\ndef test_relative_error():\\n    assert source.relative_error(10, 10) == 0, \"Test failed with input values of 10, 10\"\\n    assert source.relative_error(10, 5) == 0.5, \"Test failed with input values of 10, 5\"\\n    assert source.relative_error(1000, 500) == 0.2, \"Test failed with input values of 1000, 500\"\\n    assert source.relative_error(1, 0.00001) == 0.00001, \"Test failed with input values of 1, 0.00001\"\\n    assert source.relative_error(0.00001, 1) == 0.00001, \"Test failed with input values of 0.00001, 1\"\\n    assert source.relative_error(0, 0) == 0, \"Test failed with input values of 0, 0\"',\n",
       "  67.0),\n",
       " ('def harmonic_oscillator(grids, k=1.):\\n  \\n  vp = 0.5 * k * grids ** 2\\n  return vp',\n",
       "  \"import os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\nimport source as src\\nimport pytest\\n\\ndef test_harmonic_oscillator():\\n    grids = 2\\n    with pytest.raises(AttributeError):\\n        expected = 0.5 * 1 * grids ** 2\\n    with pytest.raises(AttributeError):\\n        assert src.harmonic_oscillator(grids) == expected\",\n",
       "  33.0),\n",
       " (\"def gen_wavelength_header(h, axis, ax_num = 3):\\n    \\n    h['NAXIS%d'%ax_num] = len(axis)\\n    h['CRPIX%d'%ax_num] = 1\\n    h['CRVAL%d'%ax_num] = axis[0]\\n    h['CDELT%d'%ax_num] = axis[1]-axis[0]\\n    h['CUNIT%d'%ax_num] = 'Angstroms'\\n    h['CTYPE%d'%ax_num] = 'LINEAR'\\n    return h\",\n",
       "  \"import pytest\\nimport os\\nimport sys\\nsys.path.append(os.path.join(os.path.dirname(__file__), '..'))\\nfrom source import gen_wavelength_header\\n\\ndef test_gen_wavelength_header():\\n    h = {'key': 'value'}\\n    axis = [4.0, 5.0]\\n    result = gen_wavelength_header(h, axis)\\n    assert len(result) == 6  # Checks that the dictionary has 6 elements after modification\",\n",
       "  100.0),\n",
       " (\"def lengths(x, lower_field='start', upper_field='stop'):\\n    \\n    return 1 + x[upper_field] - x[lower_field]\",\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\"..\") # to import source.py file in the same directory\\nfrom source import lengths\\n\\ndef test_lengths():\\n    x = {\\'start\\': 5, \\'stop\\': 7}\\n    assert lengths(x) == 3',\n",
       "  100.0),\n",
       " ('import numpy\\n\\ndef cross_truncate(indices, bound, norm):\\n    r\\n    assert norm >= 0, \"negative L_p norm not allowed\"\\n    bound = numpy.asfarray(bound)\\n    if numpy.any(bound < 0):\\n        out = numpy.zeros((len(indices),), dtype=bool)\\n    elif norm == 0:\\n        out = numpy.sum(indices > 0, axis=-1) <= 1\\n        out[numpy.any(indices > bound, axis=-1)] = False\\n    elif norm == numpy.inf:\\n        out = numpy.max(indices/bound, axis=-1) <= 1\\n    else:\\n        out = numpy.sum((indices/bound)**norm, axis=-1)**(1./norm) <= 1\\n    if not numpy.any(bound < 0):\\n        out[numpy.all(indices == 0, axis=-1)] = True\\n    return out',\n",
       "  'import numpy\\nimport pytest\\nfrom source import cross_truncate\\n\\ndef test_cross_truncate():\\n    indices = numpy.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n    bound = numpy.array([-1, -2, -3])\\n    norm = 1\\n    assert numpy.array_equal(cross_truncate(indices, bound, norm), numpy.array([False, False, False], dtype=bool))\\n\\ndef test_cross_truncate_2():\\n    indices = numpy.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n    bound = numpy.array([1, 2, 3])\\n    norm = numpy.inf\\n    assert numpy.array_equal(cross_truncate(indices, bound, norm), numpy.array([False, False, False], dtype=bool))\\n\\ndef test_cross_truncate_3():\\n    indices = numpy.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n    bound = numpy.array([-1, -2, -3])\\n    norm = 0\\n    assert numpy.array_equal(cross_truncate(indices, bound, norm), numpy.array([False, False, False], dtype=bool))\\n\\ndef test_cross_truncate_4():\\n    indices = numpy.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\n    bound = numpy.array([1, 1, 1])\\n    norm = 1\\n    assert numpy.array_equal(cross_truncate(indices, bound, norm), numpy.array([True, True, True], dtype=bool))',\n",
       "  19.0),\n",
       " ('def invert_two_by_two(a, b, c, d, e, f):\\n    r\\n    denom = a * d - b * c\\n    first = (d * e - b * f) / denom\\n    second = (a * f - c * e) / denom\\n\\n    return first, second',\n",
       "  \"from source import *\\nimport pytest\\nimport sys\\nsys.path.append('.')\\nfrom source import invert_two_by_two\\n\\ndef test_invert_two_by_two():\\n    a, b, c, d, e, f = (1, 2, 3, 4, 5, 6)\\n    expected = (1, 2)\\n    with pytest.raises(NameError):\\n        assert invert_two_by_two(a, b, c, d, e, f) == expected\",\n",
       "  33.0),\n",
       " (\"import torch\\n\\ndef pad_sparse(sparse, length=None):\\n    \\n    row = torch.LongTensor(sparse.row)\\n    col = torch.LongTensor(sparse.col)\\n    values = torch.from_numpy(sparse.data)\\n    if length is None:\\n        shape = sparse.shape\\n    else:\\n        shape = (length, length)\\n\\n    return {\\n        'row': row, 'col': col, 'values': values,\\n        'size': torch.Size(shape)}\",\n",
       "  'import os\\nimport pytest\\nfrom source import pad_sparse  # Importing from the source.py file\\n\\ndef test_pad_sparse():\\n    # Assuming that there is a method \\'row\\', \\'col\\', \\'data\\', and \\'shape\\' in your \\'sparse\\' object.\\n    sparse = {\"row\": [1, 2, 3], \"col\": [0, 2, 1], \"data\": [1, 2, 3], \"shape\": (3, 3)}\\n\\n    # Calling the function with the \\'sparse\\' object\\n    result = pad_sparse(sparse)\\n\\n    # Asserting if the returned dictionary keys are correct\\n    assert set(result.keys()) == {\\'row\\', \\'col\\', \\'values\\', \\'size\\'}\\n\\n    # Asserting if the \\'row\\' tensor is correct\\n    assert torch.equal(result[\\'row\\'], torch.tensor([1, 2, 3]))\\n\\n    # Asserting if the \\'col\\' tensor is correct\\n    assert torch.equal(result[\\'col\\'], torch.tensor([0, 2, 1]))\\n\\n    # Asserting if the \\'values\\' tensor is correct\\n    assert torch.equal(result[\\'values\\'], torch.tensor([1, 2, 3]))\\n\\n    # Asserting if the \\'size\\' tuple is correct\\n    assert result[\\'size\\'] == (3, 3)',\n",
       "  33.0),\n",
       " ('def sample_to_seconds(sample, sampling_rate=16000):\\n    \\n\\n    return sample / sampling_rate',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\") # to import source.py from the same directory\\nfrom source import sample_to_seconds\\n\\ndef test_sample_to_seconds():\\n    assert sample_to_seconds(16000, 16000) == 1.0',\n",
       "  100.0),\n",
       " ('def get_plot_of_confidence_intervals_labels(time_type):\\n    \\n    if time_type == \"w\":\\n        title = \"Mean Waiting time over number of iterations\"\\n    elif time_type == \"s\":\\n        title = \"Mean Service time over number of iterations\"\\n    elif time_type == \"b\":\\n        title = \"Mean time Blocked over number of iterations\"\\n    else:\\n        title = \" \"\\n\\n    x_axis = \"Number of trials\"\\n    y_axis = \"Means of times\"\\n\\n    return title, x_axis, y_axis',\n",
       "  'import pytest\\nfrom source import get_plot_of_confidence_intervals_labels\\n\\nclass TestGetPlotOfConfidenceIntervalsLabels:\\n\\n    def test_w(self):\\n        assert get_plot_of_confidence_intervals_labels(\"w\") == (\"Mean Waiting time over number of iterations\", \"Number of trials\", \"Means of times\")\\n\\n    def test_s(self):\\n        assert get_plot_of_confidence_intervals_labels(\"s\") == (\"Mean Service time over number of iterations\", \"Number of trials\", \"Means of times\")\\n\\n    def test_b(self):\\n        assert get_plot_of_confidence_intervals_labels(\"b\") == (\"Mean time Blocked over number of iterations\", \"Number of trials\", \"Means of times\")\\n\\n    def test_invalid(self):\\n        assert get_plot_of_confidence_intervals_labels(\"invalid\") == (\" \", \"Number of trials\", \"Means of times\")',\n",
       "  100.0),\n",
       " ('def torch_interpolation(pos, img):\\n    \\n    pos_floor = pos.floor().long()\\n    uv = pos - pos_floor\\n    ij = pos_floor\\n    get_img = lambda b, a: img[a.clamp(0, img.size(0) - 1), b.clamp(0, img.size(0) - 1)]\\n    return (\\n            ((1 - uv[0]) * (1 - uv[1])).unsqueeze(-1) * get_img(ij[0], ij[1]) +\\n            ((1 - uv[0]) * uv[1]).unsqueeze(-1) * get_img(ij[0], ij[1] + 1) +\\n            (uv[0] * (1 - uv[1])).unsqueeze(-1) * get_img(ij[0] + 1, ij[1]) +\\n            (uv[0] * uv[1]).unsqueeze(-1) * get_img(ij[0] + 1, ij[1] + 1)\\n    )',\n",
       "  'import pytest\\nimport torch\\nfrom source import torch_interpolation\\n\\n@pytest.fixture\\ndef input_data():\\n    pos = torch.tensor([[1.2, 2.3], [2.4, 3.5], [3.6, 4.7]])\\n    img = torch.tensor([[[1, 1, 1], [2, 2, 2], [3, 3, 3]],\\n                        [[4, 4, 4], [5, 5, 5], [6, 6, 6]],\\n                        [[7, 7, 7], [8, 8, 8], [9, 9, 9]]])\\n    return pos, img\\n\\ndef test_torch_interpolation(input_data):\\n    pos, img = input_data\\n    result = torch_interpolation(pos, img)\\n    expected = torch.tensor([[2.25, 2.77], [3.0, 3.55], [3.33, 3.88]])\\n    assert torch.allclose(result, expected), \"Expected output does not match the function output\"',\n",
       "  100.0),\n",
       " ('def recall(q, a):\\n    \\n    return (a == q.reshape(-1, 1)).any(1).mean()',\n",
       "  'from source import *\\nimport pytest\\nimport numpy as np\\nfrom source import recall\\n\\ndef test_recall():\\n    q = np.array([[1, 0, 1], [0, 1, 1]])\\n    a = np.array([[1, 0, 1], [0, 1, 0]])\\n    with pytest.raises(ValueError):\\n        assert recall(q, a) == 1.0',\n",
       "  100.0),\n",
       " ('def inverse(xs, scale, x0):\\n  \\n  ys = scale / (xs - x0)\\n  return ys',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\".\")  # To import source.py from the same directory\\nimport pytest\\nfrom source import inverse\\n\\ndef test_inverse():\\n  xs = 10\\n  scale = 20\\n  x0 = 5\\n  \\n  expected_output = scale / (xs - x0)\\n  \\n  assert inverse(xs, scale, x0) == expected_output',\n",
       "  100.0),\n",
       " ('def get_trajectory(states_or_stdevs, idx_dim, idx_q):\\n    \\n    assert idx_dim < states_or_stdevs.shape[1], \"Spatial coordinate index out of bounds\"\\n    assert (\\n        idx_q < states_or_stdevs.shape[2]\\n    ), \"Derivative coordinate index out of bounds\"\\n    traj = states_or_stdevs[:, idx_dim, idx_q]\\n    return traj',\n",
       "  'import pytest\\nfrom source import get_trajectory # Assuming the function is defined in source.py\\nimport numpy as np\\n\\nclass TestGetTrajectory:\\n\\n    def test_out_of_bounds(self):\\n        states_or_stdevs = np.ones((10,10,10)) # Creating a 3D array for demo, replace with actual data\\n        idx_dim = 15\\n        idx_q = 5\\n\\n        with pytest.raises(AssertionError) as e:\\n            get_trajectory(states_or_stdevs, idx_dim, idx_q)\\n        assert \"Spatial coordinate index out of bounds\" in str(e.value)\\n\\n    def test_valid_input(self):\\n        states_or_stdevs = np.ones((10,10,10)) # Creating a 3D array for demo, replace with actual data\\n        idx_dim = 5\\n        idx_q = 5\\n\\n        traj = get_trajectory(states_or_stdevs, idx_dim, idx_q)\\n        assert traj.shape == states_or_stdevs.shape[0], \"The trajectory should have the same number of time steps as the states_or_stdevs array\"',\n",
       "  100.0),\n",
       " ('def remove_geometry(fc):\\n    \\n    return fc.select([\".*\"], None, False)',\n",
       "  \"import os\\nimport pytest\\nfrom source import remove_geometry\\n\\ndef test_remove_geometry():\\n    full_path = os.path.abspath(__file__)\\n    current_dir = os.path.dirname(full_path)\\n    fc = {'features': [1, 2, 3, 4, 5]}\\n    with pytest.raises(AttributeError):\\n        fc = remove_geometry(fc)\\n    assert len(fc.get('features', [])) == 0\",\n",
       "  100.0),\n",
       " ('def get_maximum_with_tolerance(value, tolerance):\\n    \\n    return value * (1 + tolerance)',\n",
       "  '# test_source.py\\nimport pytest\\nimport sys\\nsys.path.append(\".\") # This line is to import source.py from the same directory\\nfrom source import get_maximum_with_tolerance\\n\\ndef test_get_maximum_with_tolerance():\\n    assert get_maximum_with_tolerance(10, 0.1) == 10.1',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef hard_example_mining(dist_mat, labels, return_inds=False):\\n    \\n\\n    assert len(dist_mat.size()) == 2\\n    assert dist_mat.size(0) == dist_mat.size(1)\\n    N = dist_mat.size(0)\\n\\n    # shape [N, N]\\n    is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\\n    is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\\n\\n    # `dist_ap` means distance(anchor, positive)\\n    # both `dist_ap` and `relative_p_inds` with shape [N, 1]\\n    dist_ap, relative_p_inds = torch.max(\\n        dist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\\n    # `dist_an` means distance(anchor, negative)\\n    # both `dist_an` and `relative_n_inds` with shape [N, 1]\\n    dist_an, relative_n_inds = torch.min(\\n        dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\\n    # shape [N]\\n    dist_ap = dist_ap.squeeze(1)\\n    dist_an = dist_an.squeeze(1)\\n\\n    if return_inds:\\n        # shape [N, N]\\n        ind = (labels.new().resize_as_(labels)\\n               .copy_(torch.arange(0, N).long())\\n               .unsqueeze(0).expand(N, N))\\n        # shape [N, 1]\\n        p_inds = torch.gather(\\n            ind[is_pos].contiguous().view(N, -1), 1, relative_p_inds.data)\\n        n_inds = torch.gather(\\n            ind[is_neg].contiguous().view(N, -1), 1, relative_n_inds.data)\\n        # shape [N]\\n        p_inds = p_inds.squeeze(1)\\n        n_inds = n_inds.squeeze(1)\\n        return dist_ap, dist_an, p_inds, n_inds\\n\\n    return dist_ap, dist_an',\n",
       "  'import pytest\\nimport torch\\n\\ndef test_hard_example_mining():\\n    # mockup data\\n    dist_mat = torch.randn(10, 10)\\n    labels = torch.randint(0, 2, (10,))\\n\\n    # call the function\\n    dist_ap, dist_an = hard_example_mining(dist_mat, labels)\\n    \\n    # perform assertions\\n    assert dist_ap.shape == (10,)\\n    assert dist_an.shape == (10,)',\n",
       "  0.0),\n",
       " ('def is_float(value):\\n    \\n    return isinstance(value, float)',\n",
       "  \"import sys\\nsys.path.append('.')  # Adds source.py to the path\\nimport source  # Importing the source.py module\\n\\ndef test_is_float():\\n    assert source.is_float(10.2) == True\",\n",
       "  100.0),\n",
       " ('def xp_mol(xp_mass, M_lc, M_hc):\\n          \\n    return (xp_mass * M_hc) / ((xp_mass * M_hc) + (M_lc - M_lc * xp_mass))',\n",
       "  '# source.py\\ndef xp_mol(xp_mass, M_lc, M_hc):\\n    return (xp_mass * M_hc) / ((xp_mass * M_hc) + (M_lc - M_lc * xp_mass))\\n\\n\\n# test_source.py\\nimport pytest\\nfrom .source import xp_mol\\n\\ndef test_xp_mol():\\n    # Arrange\\n    xp_mass, M_lc, M_hc = 1, 2, 3\\n    expected_result = 2 / ((1 * 3) + (2 - 2 * 1))\\n\\n    # Act\\n    result = xp_mol(xp_mass, M_lc, M_hc)\\n\\n    # Assert\\n    assert result == expected_result',\n",
       "  0.0),\n",
       " ('def M2_string(params, errors):\\n    \\n    d0, z0, Theta, M2, zR = params\\n    d0_std, z0_std, Theta_std, M2_std, zR_std = errors\\n    s = \\'\\'\\n    s += \"       M^2 = %.2f ± %.2f\\\\n\" % (M2, M2_std)\\n    s += \"\\\\n\"\\n    s += \"       d_0 = %.0f ± %.0f µm\\\\n\" % (d0*1e6, d0_std*1e6)\\n    s += \"       w_0 = %.0f ± %.0f µm\\\\n\" % (d0/2*1e6, d0_std/2*1e6)\\n    s += \"\\\\n\"\\n    s += \"       z_0 = %.0f ± %.0f mm\\\\n\" % (z0*1e3, z0_std*1e3)\\n    s += \"       z_R = %.0f ± %.0f mm\\\\n\" % (zR*1e3, zR_std*1e3)\\n    s += \"\\\\n\"\\n    s += \"     Theta = %.2f ± %.2f mrad\\\\n\" % (Theta*1e3, Theta_std*1e3)\\n    return s',\n",
       "  'import pytest\\nfrom source import M2_string\\n\\ndef test_M2_string():\\n    params = (100, 0, 0.5, 20, 5)\\n    errors = (10, 2, 0.05, 15, 3)\\n    result = M2_string(params, errors)\\n    assert result == \"\"\"       M^2 = 20.00 ± 15.00\\n\\n       d_0 = 100000000 ± 10000000 µm\\n       w_0 = 50000000 ± 5000000 µm\\n\\n       z_0 = 0 ± 2000 mm\\n       z_R = 5000 ± 3000 mm\\n\\n     Theta = 500.00 ± 50.00 mrad\\n\"\"\"',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef hard_example_mining(dist_mat, labels, return_inds=False):\\n    \\n\\n    assert len(dist_mat.size()) == 2\\n    assert dist_mat.size(0) == dist_mat.size(1)\\n    N = dist_mat.size(0)\\n\\n    # shape [N, N]\\n    is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\\n    is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\\n\\n    # `dist_ap` means distance(anchor, positive)\\n    # both `dist_ap` and `relative_p_inds` with shape [N, 1]\\n    dist_ap, relative_p_inds = torch.max(\\n        dist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\\n    # `dist_an` means distance(anchor, negative)\\n    # both `dist_an` and `relative_n_inds` with shape [N, 1]\\n    dist_an, relative_n_inds = torch.min(\\n        dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\\n    # shape [N]\\n    dist_ap = dist_ap.squeeze(1)\\n    dist_an = dist_an.squeeze(1)\\n\\n    if return_inds:\\n        # shape [N, N]\\n        ind = (labels.new().resize_as_(labels)\\n               .copy_(torch.arange(0, N).long())\\n               .unsqueeze(0).expand(N, N))\\n        # shape [N, 1]\\n        p_inds = torch.gather(\\n            ind[is_pos].contiguous().view(N, -1), 1, relative_p_inds.data)\\n        n_inds = torch.gather(\\n            ind[is_neg].contiguous().view(N, -1), 1, relative_n_inds.data)\\n        # shape [N]\\n        p_inds = p_inds.squeeze(1)\\n        n_inds = n_inds.squeeze(1)\\n        return dist_ap, dist_an, p_inds, n_inds\\n\\n    return dist_ap, dist_an',\n",
       "  'import pytest\\nimport torch\\nfrom source import hard_example_mining\\n\\n@pytest.fixture\\ndef setup_data():\\n    dist_mat = torch.randn(4, 4)\\n    labels = torch.tensor([0, 1, 2, 0])\\n    return (dist_mat, labels)\\n\\ndef test_hard_example_mining(setup_data):\\n    dist_mat, labels = setup_data\\n    dist_ap, dist_an = hard_example_mining(dist_mat, labels)\\n    assert type(dist_ap) == torch.Tensor\\n    assert type(dist_an) == torch.Tensor\\n    assert dist_ap.shape == dist_an.shape\\n    assert dist_ap.shape == torch.Size([4])',\n",
       "  42.0),\n",
       " ('def interactions(G, nbunch=None, t=None):\\n    \\n    return G.interactions(nbunch, t=t)',\n",
       "  'import sys\\nsys.path.append(\".\")\\nimport source  # Assuming source.py is in the same directory\\n\\ndef test_interactions():\\n    G = source.G()\\n    assert len(G.interactions()) > 0',\n",
       "  50.0),\n",
       " ('def mul(mean1, var1, mean2, var2):\\n    \\n\\n    mean = (var1*mean2 + var2*mean1) / (var1 + var2)\\n    var = 1 / (1/var1 + 1/var2)\\n    return (mean, var)',\n",
       "  '# -*- coding: utf-8 -*-\\n\\nimport pytest\\nimport sys\\nsys.path.append(\\'.\\')  # To import source.py from the same directory\\nfrom source import mul\\n\\ndef test_mul():\\n    # Given\\n    mean1, var1 = 5, 1\\n    mean2, var2 = 2, 2\\n\\n    # When\\n    result = mul(mean1, var1, mean2, var2)\\n\\n    # Then\\n    assert result[0] == 3.5, \"The mean is not correct\"\\n    assert result[1] == 2.5, \"The variance is not correct\"',\n",
       "  100.0),\n",
       " ('def hex_to_rgb(hex_color):\\n    \\n    if not isinstance(hex_color, str):\\n        raise TypeError(\"\\'hex_color\\' must be of type \\'str\\'.\")\\n    if len(hex_color) != 6:\\n        raise ValueError(\"\\'hex_color\\' must 6 characters in length \"\\n                         \"(excluding \\'#\\') e.g. FF1919.\")\\n\\n    r = int(hex_color[0:2], base=16)\\n    g = int(hex_color[2:4], base=16)\\n    b = int(hex_color[4:6], base=16)\\n\\n    return r, g, b',\n",
       "  \"import pytest\\nfrom source import hex_to_rgb\\n\\ndef test_hex_to_rgb():\\n    assert hex_to_rgb('#FF1919') == (255, 25, 25)\",\n",
       "  44.0),\n",
       " ('def nd_denormalize(x, mu, sigma):\\n    \\n    return (x * sigma) + mu',\n",
       "  'import pytest\\nimport source\\n\\ndef test_nd_denormalize():\\n    x = 10\\n    mu = 0\\n    sigma = 1\\n    assert source.nd_denormalize(x, mu, sigma) == x',\n",
       "  100.0),\n",
       " ('def __det3x3__(a):\\n    \\n    val  = +a[0] * (a[4] * a[8] - a[7] * a[5])\\n    val += -a[1] * (a[3] * a[8] - a[6] * a[5])\\n    val += +a[2] * (a[3] * a[7] - a[6] * a[4])\\n    return val',\n",
       "  'import sys\\nsys.path.append(\"..\") # to import the module from parent directory\\nfrom source import __det3x3__\\n\\ndef test_det3x3__():\\n    assert __det3x3__([[1,2,3],[4,5,6],[7,8,9]]) == -6',\n",
       "  40.0),\n",
       " ('import torch\\n\\ndef hard_example_mining(dist_mat, labels, return_inds=False):\\n    \\n\\n    assert len(dist_mat.size()) == 2\\n    assert dist_mat.size(0) == dist_mat.size(1)\\n    N = dist_mat.size(0)\\n\\n    # shape [N, N]\\n    is_pos = labels.expand(N, N).eq(labels.expand(N, N).t())\\n    is_neg = labels.expand(N, N).ne(labels.expand(N, N).t())\\n\\n    # `dist_ap` means distance(anchor, positive)\\n    # both `dist_ap` and `relative_p_inds` with shape [N, 1]\\n    dist_ap, relative_p_inds = torch.max(\\n        dist_mat[is_pos].contiguous().view(N, -1), 1, keepdim=True)\\n    # `dist_an` means distance(anchor, negative)\\n    # both `dist_an` and `relative_n_inds` with shape [N, 1]\\n    dist_an, relative_n_inds = torch.min(\\n        dist_mat[is_neg].contiguous().view(N, -1), 1, keepdim=True)\\n    # shape [N]\\n    dist_ap = dist_ap.squeeze(1)\\n    dist_an = dist_an.squeeze(1)\\n\\n    if return_inds:\\n        # shape [N, N]\\n        ind = (labels.new().resize_as_(labels)\\n               .copy_(torch.arange(0, N).long())\\n               .unsqueeze(0).expand(N, N))\\n        # shape [N, 1]\\n        p_inds = torch.gather(\\n            ind[is_pos].contiguous().view(N, -1), 1, relative_p_inds.data)\\n        n_inds = torch.gather(\\n            ind[is_neg].contiguous().view(N, -1), 1, relative_n_inds.data)\\n        # shape [N]\\n        p_inds = p_inds.squeeze(1)\\n        n_inds = n_inds.squeeze(1)\\n        return dist_ap, dist_an, p_inds, n_inds\\n\\n    return dist_ap, dist_an',\n",
       "  'import torch\\nimport pytest\\nfrom source import hard_example_mining\\n\\ndef test_hard_example_mining():\\n    # Testing with random tensors\\n    dist_mat = torch.randn(10, 10)\\n    labels = torch.randint(0, 2, (10,))\\n\\n    # Testing with return_inds=True\\n    dist_ap, dist_an, p_inds, n_inds = hard_example_mining(dist_mat, labels, return_inds=True)\\n    assert dist_ap.shape == torch.Size([10])\\n    assert dist_an.shape == torch.Size([10])\\n    assert p_inds.shape == torch.Size([10])\\n    assert n_inds.shape == torch.Size([10])\\n\\n    # Testing with return_inds=False\\n    dist_ap, dist_an = hard_example_mining(dist_mat, labels, return_inds=False)\\n    assert dist_ap.shape == torch.Size([10])\\n    assert dist_an.shape == torch.Size([10])\\n\\n    # Testing exception for incorrect shape of dist_mat\\n    with pytest.raises(AssertionError):\\n        hard_example_mining(torch.randn(10, 20), labels)\\n\\n    # Testing exception for incorrect shape of labels\\n    with pytest.raises(AssertionError):\\n        hard_example_mining(torch.randn(20, 20), torch.randint(0, 2, (15,)))',\n",
       "  42.0),\n",
       " (\"def get_conf(case):\\n    \\n\\n    if case == 'channel':\\n\\n        nx = 64                 # Grid points in the streamwise direction for the low resolution data\\n        ny = 32                 # Grid points in the wall-normal direction for the low resolution data\\n        res = 1/512/0.013       # Resolution\\n        channels = 2            # Number of output channels\\n        n_samples_test = 1856   # Number of testing samples\\n        n_samples_train = 10000 # Number of training samples\\n        max_samples_per_tf = 50 # Maximum number of samples per tfrecord file\\n\\n    if case == 'pinball':\\n\\n        nx = 36                 # Grid points in the streamwise direction for the low resolution data\\n        ny = 12                 # Grid points in the wall-normal direction for the low resolution data\\n        res = 1                 # Resolution\\n        channels = 2            # Number of output channels\\n        n_samples_test = 737    # Number of testing samples\\n        n_samples_train = 4000  # Number of training samples\\n        max_samples_per_tf = 50 # Maximum number of samples per tfrecord file\\n\\n    if case == 'exptbl':\\n\\n        nx = 31                 # Grid points in the streamwise direction for the low resolution data\\n        ny = 32                 # Grid points in the wall-normal direction for the low resolution data\\n        res = 1                # Resolution\\n        channels = 2\\n        n_samples_test = 2000   # Number of testing samples\\n        n_samples_train = 30000 # Number of training samples\\n        max_samples_per_tf = 1000 # Maximum number of samples per tfrecord file\\n\\n    if case == 'sst':\\n\\n        nx = 90                 # Grid points in the streamwise direction for the low resolution data\\n        ny = 45                 # Grid points in the wall-normal direction for the low resolution data\\n        res = 1                # Resolution\\n        channels = 1            # Number of output channels\\n        n_samples_test = 1305   # Number of testing samples\\n        n_samples_train = 6000 # Number of training samples\\n        max_samples_per_tf = 50 # Maximum number of samples per tfrecord file\\n\\n    if case == 'buoyancy':\\n\\n        nx = 64                 # Grid points in the streamwise direction for the low resolution data\\n        ny = 64                 # Grid points in the wall-normal direction for the low resolution data\\n        res = 1                 # Resolution\\n        channels = 1            # Number of output channels\\n        n_samples_test = 3000   # Number of testing samples\\n        n_samples_train = 12000 # Number of training samples\\n        max_samples_per_tf = 50 # Maximum number of samples per tfrecord file\\n\\n    return nx, ny, res, channels, n_samples_test, n_samples_train, max_samples_per_tf\",\n",
       "  \"import pytest\\nfrom .source import get_conf\\n\\nclass TestGetConf:\\n\\n    def test_channel(self):\\n        case = 'channel'\\n        nx, ny, res, channels, n_samples_test, n_samples_train, max_samples_per_tf = get_conf(case)\\n        assert nx == 64, 'Test failed for case: {}, expected nx = 64'.format(case)\\n\\n    def test_pinball(self):\\n        case = 'pinball'\\n        nx, ny, res, channels, n_samples_test, n_samples_train, max_samples_per_tf = get_conf(case)\\n        assert nx == 36, 'Test failed for case: {}, expected nx = 36'.format(case)\\n\\n    def test_exptbl(self):\\n        case = 'exptbl'\\n        nx, ny, res, channels, n_samples_test, n_samples_train, max_samples_per_tf = get_conf(case)\\n        assert nx == 31, 'Test failed for case: {}, expected nx = 31'.format(case)\\n\\n    def test_sst(self):\\n        case = 'sst'\\n        nx, ny, res, channels, n_samples_test, n_samples_train, max_samples_per_tf = get_conf(case)\\n        assert nx == 90, 'Test failed for case: {}, expected nx = 90'.format(case)\\n\\n    def test_buoyancy(self):\\n        case = 'buoyancy'\\n        nx, ny, res, channels, n_samples_test, n_samples_train, max_samples_per_tf = get_conf(case)\\n        assert nx == 64, 'Test failed for case: {}, expected nx = 64'.format(case)\",\n",
       "  0.0),\n",
       " ('def matrix_add_symmetric(M, M_sym):\\n    \\n    M[0, 0] += M_sym[0]\\n    M[0, 1] += M_sym[1]\\n    M[1, 0] += M_sym[1]\\n    M[0, 2] += M_sym[2]\\n    M[2, 0] += M_sym[2]\\n\\n    M[1, 1] += M_sym[3]\\n    M[1, 2] += M_sym[4]\\n    M[2, 1] += M_sym[4]\\n\\n    M[2, 2] += M_sym[5]\\n\\n    return M',\n",
       "  '# test_matrix_add_symmetric.py\\n\\nimport sys\\nsys.path.append(\".\")  # Adds the current directory to the Python path\\n\\nimport pytest\\nimport source  # Importing the source file\\n\\ndef test_matrix_add_symmetric():\\n    M = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\\n    M_sym = [10, 11, 12, 13, 14, 15]\\n\\n    result = source.matrix_add_symmetric(M, M_sym)\\n    assert result == [[11, 14, 17], [15, 18, 21], [22, 25, 28]], \"Expected result is [[11, 14, 17], [15, 18, 21], [22, 25, 28]], but the actual result is {}\".format(result)',\n",
       "  18.0),\n",
       " ('def get_normalization_min_max(array, norm_min, norm_max):\\n    \\n    if norm_min is None:\\n        norm_min = array.min()\\n    if norm_max is None:\\n        norm_max = array.max()\\n\\n    return norm_min, norm_max',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\"..\") # This adds the parent directory into the import path\\nfrom source import get_normalization_min_max\\n\\ndef test_get_normalization_min_max():\\n    array = [1, 2, 3, 4, 5]\\n    norm_min, norm_max = get_normalization_min_max(array, None, None)\\n    assert norm_min == 1, \"Failure when norm_min is None\"',\n",
       "  50.0),\n",
       " (\"def prepare_source_valid_lengths(F, valid_length, query_data, num_heads: int):\\n    \\n    # (batch * heads,)\\n    att_valid_length = F.repeat(valid_length, repeats=num_heads, axis=0)\\n    att_valid_length = F.broadcast_like(F.expand_dims(att_valid_length, axis=1),\\n                                        query_data,\\n                                        lhs_axes=(1,), rhs_axes=(1,))\\n    return F.cast(att_valid_length, dtype='int32')\",\n",
       "  'import pytest\\nimport numpy as np\\nfrom source import prepare_source_valid_lengths\\n\\nclass TestSourceValidLengths:\\n    def test_valid_lengths(self):\\n        # Mock data\\n        valid_length = 10\\n        query_data = np.random.randint(1, 100, size=(10, 10)) # You can replace this with mock data\\n        num_heads = 2\\n\\n        # Call the function\\n        result = prepare_source_valid_lengths(query_data, valid_length, num_heads)\\n\\n        # Assertion\\n        assert result.shape == (num_heads, query_data.shape[0]), \"Shapes do not match\"\\n\\n\\nif __name__ == \"__main__\":\\n    pytest.main()',\n",
       "  25.0),\n",
       " ('def faces_from_sphere_vertices(vertices):\\n    \\n    from scipy.spatial import Delaunay\\n    return Delaunay(vertices).convex_hull',\n",
       "  '# source.py\\nimport numpy as np\\nfrom scipy.spatial import Delaunay\\n\\ndef faces_from_sphere_vertices(vertices):\\n    return Delaunay(vertices).convex_hull\\n\\n# test_source.py\\nimport pytest\\nfrom source import faces_from_sphere_vertices\\n\\ndef test_faces_from_sphere_vertices():\\n    vertices = np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1]])\\n    faces = np.array([[0, 1, 2], [1, 2, 3]])\\n    assert np.array_equal(faces_from_sphere_vertices(vertices), faces)\\n\\nif __name__ == \"__main__\":\\n    test_faces_from_sphere_vertices()',\n",
       "  100.0),\n",
       " ('def dims_to_targetshape(data_dims, batch_size=None, placeholder=False):\\n  \\n  if batch_size is not None and batch_size != 0:\\n    shape = [batch_size]\\n  elif placeholder is True:\\n    shape = [None]\\n  else:\\n    shape = [-1]\\n  shape.extend(data_dims)\\n\\n  return shape',\n",
       "  \"import sys\\nsys.path.append('.')\\nfrom source import dims_to_targetshape\\n\\ndef test_dims_to_targetshape():\\n    assert dims_to_targetshape([1, 2, 3]) == [1, 2, 3]\\n    assert dims_to_targetshape([1, 2, 3], batch_size=5) == [5, 1, 2, 3]\\n    assert dims_to_targetshape([1, 2, 3], placeholder=True) == [None, 1, 2, 3]\\n    assert dims_to_targetshape([1, 2, 3], batch_size=0) == [-1, 1, 2, 3]\",\n",
       "  75.0),\n",
       " ('def curvature(cs, x):\\n    \\n    second_deriv = cs(x, 2)\\n    deriv = cs(x, 1)\\n    return second_deriv/(1.0 + deriv**2)**1.5',\n",
       "  \"import pytest\\nimport sys\\nsys.path.append('..')\\nfrom source import CubicSpline\\n\\ndef test_curvature():\\n    cs = CubicSpline()\\n    x = 1.0\\n    assert abs(curvature(cs, x) - 0.5) < 1e-06, 'The curvature does not match the expected value'\",\n",
       "  25.0),\n",
       " ('def get_thresholds(df, top=10):\\n    \\n    sorted_df = df.sort_values(\"bitscore\", ascending=False)\\n    topscore = sorted_df.groupby(level=0).first().bitscore\\n    thresholds = (topscore * ((100 - top)) / 100).to_dict()\\n    return thresholds',\n",
       "  \"import pytest\\nimport pandas as pd\\nfrom source import get_thresholds\\n\\ndef test_get_thresholds(capsys):\\n    df = pd.DataFrame({\\n        'sequence': ['seq1', 'seq2', 'seq3', 'seq4', 'seq5'],\\n        'bitscore': [234, 345, 123, 456, 567]\\n    })\\n    assert get_thresholds(df) == {'seq1': 234, 'seq2': 345, 'seq3': 123, 'seq4': 456, 'seq5': 567}\",\n",
       "  100.0),\n",
       " ('def get_maximum_with_tolerance(value, tolerance):\\n    \\n    return value * (1 + tolerance)',\n",
       "  'import pytest\\nimport sys\\nsys.path.append(\".\") # this is to import source.py file \\nfrom source import get_maximum_with_tolerance\\n\\ndef test_get_maximum_with_tolerance():\\n    assert get_maximum_with_tolerance(5, 0) == 5\\n    assert get_maximum_with_tolerance(5, 0.1) == 5.1\\n    assert get_maximum_with_tolerance(5, 0.9) == 5.9\\n    assert get_maximum_with_tolerance(5, 1) == 6\\n    assert get_maximum_with_tolerance(5, -0.1) == 4.9\\n    assert get_maximum_with_tolerance(5, -0.9) == 4.1\\n    assert get_maximum_with_tolerance(5, -1) == 4\\n    assert get_maximum_with_tolerance(5, 1.5) == 7',\n",
       "  100.0),\n",
       " ('def drawCutoffHoriz(ax,y,cl=\"r\",lb=\"\",ls=\"--\",lw=.5):\\n    \\n\\n    #get old limits\\n    limits = ax.get_xlim()\\n\\n    #print line\\n    ax.hlines(y,xmin=limits[0],xmax=limits[1],color=cl,label=lb,linestyle=ls,\\n                linewidth=lw)\\n\\n    #Returning\\n    return ax',\n",
       "  'import pytest\\nfrom matplotlib.figure import Figure\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\n# Importing source.py file\\nfrom source import drawCutoffHoriz\\n\\nclass TestDrawCutoffHoriz:\\n    \\n    # Test function drawCutoffHoriz\\n    def test_drawCutoffHoriz(self):\\n        # Creating a new figure\\n        fig = Figure()\\n        # Adding an axis\\n        ax = fig.add_subplot(111)\\n        # Setting up some random data\\n        y = np.random.rand(10)\\n        # Calling function\\n        drawCutoffHoriz(ax, y, cl=\"r\", lb=\"\", ls=\"--\", lw=.5)\\n        # Asserting that the line is added to the axes\\n        assert \\'line\\' in ax.lines, \"Line not added to axes\"\\n\\nif __name__ == \"__main__\":\\n    pytest.main()',\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef ndc_rays(H, W, focal, near, rays_o, rays_d):\\n    \\n    # Shift ray origins to near plane\\n    t = -(near + rays_o[...,2]) / rays_d[...,2]\\n    rays_o = rays_o + t[...,None] * rays_d\\n\\n    # Projection\\n    o0 = -1./(W/(2.*focal)) * rays_o[...,0] / rays_o[...,2]\\n    o1 = -1./(H/(2.*focal)) * rays_o[...,1] / rays_o[...,2]\\n    o2 = 1. + 2. * near / rays_o[...,2]\\n\\n    d0 = -1./(W/(2.*focal)) * (rays_d[...,0]/rays_d[...,2] - rays_o[...,0]/rays_o[...,2])\\n    d1 = -1./(H/(2.*focal)) * (rays_d[...,1]/rays_d[...,2] - rays_o[...,1]/rays_o[...,2])\\n    d2 = -2. * near / rays_o[...,2]\\n\\n    rays_o = torch.stack([o0,o1,o2], -1)\\n    rays_d = torch.stack([d0,d1,d2], -1)\\n\\n    return rays_o, rays_d',\n",
       "  'import torch\\nimport pytest\\n\\nfrom source import ndc_rays\\n\\ndef test_ndc_rays():\\n    # Create some inputs\\n    H = 640\\n    W = 480\\n    focal = 500\\n    near = 1.0\\n    rays_o = torch.tensor([0, 0, 1]).view(1, 1, 1, 3)\\n    rays_d = torch.tensor([0, 0, -1]).view(1, 1, 1, 3)\\n\\n    # Run the function and get the output\\n    rays_o_out, rays_d_out = ndc_rays(H, W, focal, near, rays_o, rays_d)\\n\\n    # Create an expected output (this value is calculated by running the code manually)\\n    expected_rays_o = torch.tensor([-1.17275220, 1.17275220, 1.0]).view(1, 1, 1, 3)\\n    expected_rays_d = torch.tensor([-0.00022161, 0.00022161, -1.0]).view(1, 1, 1, 3)\\n\\n    # Assert that the output is as expected\\n    assert torch.allclose(rays_o_out, expected_rays_o)\\n    assert torch.allclose(rays_d_out, expected_rays_d)\\n\\nif __name__ == \"__main__\":\\n    test_ndc_rays()',\n",
       "  100.0),\n",
       " ('def distance_km(point1, point2):\\n    \\n    return ((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2) ** 0.5 * 111',\n",
       "  '# test_source.py\\n\\nimport source  # assuming source.py is in the same directory\\n\\ndef test_distance_km():\\n    point1 = (0, 0)\\n    point2 = (1, 1)\\n    expected_result = ((1 - 1) ** 2 + (1 - 1) ** 2) ** 0.5 * 111\\n    assert abs(source.distance_km(point1, point2) - expected_result) < 0.001',\n",
       "  100.0),\n",
       " ('def _get_dist(k_labels, t_labels, dists, index):\\n    \\n    return dists[index[k_labels], index[t_labels]]',\n",
       "  \"import pytest\\n\\nfrom source import _get_dist\\n\\ndef test_get_dist():\\n    # Create a test data\\n    valid_index = {'A': 0, 'B': 1, 'C': 2}\\n    valid_dists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\\n\\n    # Test valid labels\\n    assert _get_dist('B', 'A', valid_dists, valid_index) == 1  # Distance between 'A' and 'B' is 1\\n    assert _get_dist('C', 'B', valid_dists, valid_index) == 2  # Distance between 'B' and 'C' is 2\\n\\n    # Test invalid labels\\n    with pytest.raises(KeyError):\\n        _get_dist('D', 'A', valid_dists, valid_index)  # Label 'D' is not in the index\",\n",
       "  100.0),\n",
       " ('import torch\\n\\ndef convert_to_one_hot(tensor, num_classes=None):\\n  \\n  t = tensor.type(torch.LongTensor).view(-1, 1)\\n  if num_classes is None:\\n    num_classes = int(torch.max(t)) + 1\\n\\n  t_one_hot = torch.zeros(t.size()[0], num_classes).scatter_(1, t, 1)\\n  t_one_hot = t_one_hot.view(*tensor.shape, -1)\\n  return t_one_hot',\n",
       "  '# test_source.py\\n\\nimport pytest\\nimport torch\\nfrom source import convert_to_one_hot\\n\\ndef test_convert_to_one_hot():\\n    tensor = torch.tensor([1, 2, 3, 4, 5])\\n    result = convert_to_one_hot(tensor)\\n    expected_output_shape = torch.Size([5, 6]) # Assuming the maximum value is 5 in the input tensor\\n    assert result.shape == expected_output_shape, \"The shape of the output does not match expected\"\\n\\n    tensor_with_classes = torch.tensor([1, 2, 3, 4, 5])\\n    result_with_classes = convert_to_one_hot(tensor_with_classes, num_classes=7)\\n    expected_output_shape_with_classes = torch.Size([5, 7])\\n    assert result_with_classes.shape == expected_output_shape_with_classes, \"The shape of the output does not match expected\"',\n",
       "  100.0),\n",
       " ('def extrapolate_cost(cost, discount_rate, time_span_1, time_span_2):\\n    \\n    growth_rate = 1.0 + discount_rate\\n    value_decay_1 = pow(growth_rate, -time_span_2)\\n    value_decay_2 = pow(growth_rate, -time_span_1)\\n\\n    try:\\n        return cost * (1.0 - value_decay_1) / (1.0 - value_decay_2)\\n    except ZeroDivisionError:\\n        return cost',\n",
       "  '# test_source.py\\nimport pytest\\n\\ndef test_extrapolate_cost():\\n    assert abs(extrapolate_cost(100, 0.05, 2, 3) - 75.792463982982984) < 0.0001\\n    assert abs(extrapolate_cost(100, 0.05, 3, 2) - 85.71428571428571) < 0.0001\\n    assert abs(extrapolate_cost(100, 0.05, 1, 1) - 95.83876338512984) < 0.0001\\n    assert abs(extrapolate_cost(100, 0.1, 2, 3) - 75.0) < 0.0001\\n    assert abs(extrapolate_cost(100, 0.1, 3, 2) - 85.0) < 0.0001\\n    assert abs(extrapolate_cost(100, 0.1, 1, 1) - 95.0) < 0.0001\\n    assert abs(extrapolate_cost(100, 0.0, 2, 3) - 100) < 0.0001\\n    assert abs(extrapolate_cost(100, 0.0, 3, 2) - 100) < 0.0001\\n    assert abs(extrapolate_cost(100, 0.0, 1, 1) - 100) < 0.0001\\n    assert abs(extrapolate_cost(100, 0.2, 2, 3) - 85.105105105105105) < 0.0001\\n    assert abs(extrapolate_cost(100, 0.2, 3, 2) - 91.30469304693047) < 0.0001\\n    assert abs(extrapolate_cost(100, 0.2, 1, 1) - 98.26086956521724) < 0.0001\\n    assert extrapolate_cost(100, 1.0, 2, 3) == 0.0\\n    assert extrapolate_cost(100, 1.0, 3, 2) == 0.0\\n    assert extrapolate_cost(100, 1.0, 1, 1) == 0.0',\n",
       "  0.0),\n",
       " ('def air_core_inductor(coil_diameter: float, coil_length: float, turns: int):\\n    r\\n    k1 = (1000*coil_diameter*coil_diameter) * (turns*turns)\\n    k2 = (457418*coil_diameter) + (1016127*coil_length)\\n    return  k1/k2',\n",
       "  '# test_source.py\\nimport sys\\nsys.path.append(\"..\") # to import source.py file in the same directory\\nimport source # import the source file\\n\\ndef test_air_core_inductor():\\n    # test with known values\\n    result = source.air_core_inductor(1, 1, 1)\\n    assert result == 0.0001538461538461539, \"Test Failed: The function did not return the expected result.\"',\n",
       "  40.0),\n",
       " (\"import torch\\n\\ndef quat2mat(quaternion):\\n    \\n    # convert quat convention\\n    inds = torch.tensor([3, 0, 1, 2])\\n    input_shape = quaternion.shape[:-1]\\n    q = quaternion.reshape(-1, 4)[:, inds]\\n\\n    # Conduct dot product\\n    n = torch.bmm(q.unsqueeze(1), q.unsqueeze(-1)).squeeze(-1).squeeze(-1)        # shape (-1)\\n    idx = torch.nonzero(n).reshape(-1)\\n    q_ = q.clone()          # Copy so we don't have inplace operations that fail to backprop\\n    q_[idx, :] = q[idx, :] * torch.sqrt(2.0 / n[idx].unsqueeze(-1))\\n\\n    # Conduct outer product\\n    q2 = torch.bmm(q_.unsqueeze(-1), q_.unsqueeze(1)).squeeze(-1).squeeze(-1)        # shape (-1, 4 ,4)\\n\\n    # Create return array\\n    ret = torch.eye(3, 3, device=q.device).reshape(1, 3, 3).repeat(torch.prod(torch.tensor(input_shape)), 1, 1)\\n    ret[idx, :, :] = torch.stack([\\n        torch.stack([1.0 - q2[idx, 2, 2] - q2[idx, 3, 3], q2[idx, 1, 2] - q2[idx, 3, 0], q2[idx, 1, 3] + q2[idx, 2, 0]], dim=-1),\\n        torch.stack([q2[idx, 1, 2] + q2[idx, 3, 0], 1.0 - q2[idx, 1, 1] - q2[idx, 3, 3], q2[idx, 2, 3] - q2[idx, 1, 0]], dim=-1),\\n        torch.stack([q2[idx, 1, 3] - q2[idx, 2, 0], q2[idx, 2, 3] + q2[idx, 1, 0], 1.0 - q2[idx, 1, 1] - q2[idx, 2, 2]], dim=-1),\\n    ], dim=1)\\n\\n    # Reshape and return output\\n    ret = ret.reshape(list(input_shape) + [3, 3])\\n    return ret\",\n",
       "  'import torch\\nimport pytest\\nfrom source import quat2mat\\n\\ndef test_quat2mat():\\n    quaternion = torch.tensor([0, 1, 0, 0])  # A simple case with 0 rotation around the x-axis\\n    expected_output = torch.tensor([[[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]])  # Expected output for a rotation around the x-axis\\n    output = quat2mat(quaternion)\\n    assert torch.allclose(output, expected_output, atol=1e-6), \\'Output does not match expected result\\'\\n\\nif __name__ == \"__main__\":\\n    pytest.main()',\n",
       "  64.0),\n",
       " ('def get_pad_tuple3d(padding, kernel):\\n    \\n    # compute the padding size\\n    if isinstance(padding, (tuple, list)):\\n        pad_h = padding[0] * 2\\n        pad_w = padding[1] * 2\\n        pad_d = padding[2] * 2\\n    elif isinstance(padding, int):\\n        pad_d = pad_w = pad_h = padding * 2\\n    elif padding == \"VALID\":\\n        pad_h = 0\\n        pad_w = 0\\n        pad_d = 0\\n    elif padding == \"SAME\":\\n        pad_h = kernel[0] - 1\\n        pad_w = kernel[1] - 1\\n        pad_d = kernel[2] - 1\\n    else:\\n        raise ValueError(\"Unknown padding option %s\" % padding)\\n    pad_top = (pad_h + 1) // 2\\n    pad_left = (pad_w + 1) // 2\\n    pad_front = (pad_d + 1) // 2\\n    return pad_front, pad_top, pad_left, pad_d - pad_front, pad_h - pad_top, pad_w - pad_left',\n",
       "  'import sys\\nsys.path.insert(0, \\'..\\') # to import source.py from the parent directory\\nfrom source import get_pad_tuple3d\\n\\ndef test_get_pad_tuple3d():\\n    assert get_pad_tuple3d(1, (3,4,5)) == (0, 1, 1, 2, 3, 3)\\n    assert get_pad_tuple3d(2, (3,4,5)) == (1, 2, 2, 4, 4, 4)\\n    assert get_pad_tuple3d(\"SAME\", (3,4,5)) == (1, 1, 1, 3, 3, 3)\\n    assert get_pad_tuple3d(\"VALID\", (3,4,5)) == (0, 0, 0, 2, 2, 2)\\n    assert get_pad_tuple3d([1,2,1], (3,4,5)) == (0, 1, 1, 2, 3, 3)\\n    assert get_pad_tuple3d(3, \"SAME\") == (1, 1, 1, 2, 2, 2)\\n    assert get_pad_tuple3d(\"SAME\", \"VALID\") == (0, 0, 0, 1, 1, 1)',\n",
       "  40.0),\n",
       " ('def euclid_pyth_triplet(n, m):\\n    \\n    if n % 2 == 0 or m % 2 == 0:\\n        raise SyntaxError(\"n or m is even and should be odd\")\\n    a = n * m\\n    b = (m**2 - n**2) / 2\\n    c = (m**2 + n**2) / 2\\n    return [int(a), int(b), int(c)]',\n",
       "  '# Import the source file\\nimport source \\n\\ndef test_euclid_pyth_triplet():\\n    # Test the function with known values\\n    assert source.euclid_pyth_triplet(3, 5) == [60, 81, 144]\\n    assert source.euclid_pyth_triplet(4, 7) == [120, 241, 144]\\n    assert source.euclid_pyth_triplet(1, 2) == [2, 1, 2]\\n    assert source.euclid_pyth_triplet(5, 11) == [220, 1771, 1441]',\n",
       "  86.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov = pd.DataFrame(results_cov).dropna()\n",
    "df_cov.columns = [\"original_code\", \"pytest_code\", \"coverage\"]\n",
    "df_cov = df_cov.sort_values(by=\"coverage\", ascending=False).reset_index(drop=True)\n",
    "df_cov[df_cov.coverage > 85].to_csv(\"5k_6k_dataset_results.csv\", index=False)\n",
    "# df_cov[df_cov.coverage > 95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# test_source.py\n",
      "import pytest\n",
      "import os\n",
      "import sys\n",
      "sys.path.append(os.path.join(os.path.dirname(__file__), '..')) # This line is to import the parent directory, where source.py is located\n",
      "from source import calculate_acceleration\n",
      "\n",
      "# Test 1: Check if function returns acceleration when input is a list and the correct number of elements are given\n",
      "def test_calculate_acceleration1():\n",
      "    result = calculate_acceleration([1, 2, 3, 4], 1)\n",
      "    assert result == 20, \"Failure: The function did not return the expected value\"\n",
      "\n",
      "# Test 2: Check if function raises a TypeError when the input is not a list\n",
      "def test_calculate_acceleration2():\n",
      "    with pytest.raises(TypeError):\n",
      "        result = calculate_acceleration(\"string\", 1)\n",
      "\n",
      "# Test 3: Check if function raises a ValueError when the list does not contain enough elements\n",
      "def test_calculate_acceleration3():\n",
      "    with pytest.raises(ValueError):\n",
      "        result = calculate_acceleration([1], 1)\n",
      "\n",
      "# Test 4: Check if function raises a ValueError when the list contains too many elements\n",
      "def test_calculate_acceleration4():\n",
      "    with pytest.raises(ValueError):\n",
      "        result = calculate_acceleration([1, 2, 3, 4, 5], 1)\n"
     ]
    }
   ],
   "source": [
    "print(df_cov[df_cov.coverage > 90].iloc[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.pytest_code.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def time_to_sample_number(seconds, frequency):\n",
      "    \n",
      "    return seconds * frequency + 0.5\n"
     ]
    }
   ],
   "source": [
    "idx = 1200\n",
    "\n",
    "print(results[idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Import the function to test\n",
      "from source import time_to_sample_number\n",
      "\n",
      "# Define a test case\n",
      "def test_time_to_sample_number():\n",
      "    # Test with known input\n",
      "    assert time_to_sample_number(10, 2) == 30\n",
      "    # Test with another known input\n",
      "    assert time_to_sample_number(5, 3) == 15\n",
      "    # Test with zero frequency\n",
      "    assert time_to_sample_number(10, 0) == 0\n",
      "    # Test with negative frequency\n",
      "    assert time_to_sample_number(10, -2) == -5\n"
     ]
    }
   ],
   "source": [
    "print(results[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 100.0,\n",
       " 'stdout': '============================= test session starts =============================\\nplatform win32 -- Python 3.11.5, pytest-7.4.0, pluggy-1.0.0 -- C:\\\\ProgramData\\\\anaconda3\\\\python.exe\\ncachedir: .pytest_cache\\nrootdir: c:\\\\Master\\\\Disertatie\\\\data\\\\trial_error_generation\\\\tmp\\nplugins: cov-4.1.0, anyio-4.2.0\\ncollecting ... collected 1 item\\n\\ntest_source.py::test_time_to_sample_number FAILED                        [100%]\\n\\n================================== FAILURES ===================================\\n_________________________ test_time_to_sample_number __________________________\\n\\n    def test_time_to_sample_number():\\n        # Test with known input\\n>       assert time_to_sample_number(10, 2) == 30\\nE       assert 20.5 == 30\\nE        +  where 20.5 = time_to_sample_number(10, 2)\\n\\ntest_source.py:7: AssertionError\\n\\n---------- coverage: platform win32, python 3.11.5-final-0 -----------\\nName        Stmts   Miss  Cover   Missing\\n-----------------------------------------\\nsource.py       2      0   100%\\n-----------------------------------------\\nTOTAL           2      0   100%\\n\\n=========================== short test summary info ===========================\\nFAILED test_source.py::test_time_to_sample_number - assert 20.5 == 30\\n============================== 1 failed in 0.07s ==============================\\n',\n",
       " 'stderr': '',\n",
       " 'failed_assertions': 1,\n",
       " 'fails': 'FAILED test_source.py::test_time_to_sample_number - assert 20.5 == 30'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_pytest(results[idx][0], results[idx][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = results[idx][0]\n",
    "test_result = results[idx][1]\n",
    "\n",
    "from adjustment_utils import modify_pytest_code\n",
    "\n",
    "adjusted = modify_pytest_code(code, test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 100.0,\n",
       " 'stdout': '============================= test session starts =============================\\nplatform win32 -- Python 3.11.5, pytest-7.4.0, pluggy-1.0.0 -- C:\\\\ProgramData\\\\anaconda3\\\\python.exe\\ncachedir: .pytest_cache\\nrootdir: c:\\\\Master\\\\Disertatie\\\\data\\\\trial_error_generation\\\\tmp\\nplugins: cov-4.1.0, anyio-4.2.0\\ncollecting ... collected 1 item\\n\\ntest_source.py::test_time_to_sample_number PASSED                        [100%]\\n\\n---------- coverage: platform win32, python 3.11.5-final-0 -----------\\nName        Stmts   Miss  Cover   Missing\\n-----------------------------------------\\nsource.py       2      0   100%\\n-----------------------------------------\\nTOTAL           2      0   100%\\n\\n\\n============================== 1 passed in 0.03s ==============================\\n',\n",
       " 'stderr': '',\n",
       " 'failed_assertions': 0,\n",
       " 'fails': ''}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_pytest(results[idx][0], adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from source import scale_from_internal\n",
      "import pytest\n",
      "\n",
      "def test_scale_from_internal():\n",
      "    with pytest.raises(TypeError):\n",
      "        assert scale_from_internal([1, 2, 3], 2, 1) == [2, 4, 5]\n",
      "    assert scale_from_internal([4, 5, 6], None, 3) == [7, 8, 9]\n",
      "    assert scale_from_internal([7, 8, 9], 1, None) == [8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "print(adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "all = pd.concat([pd.read_csv(f\"generated/{file}\") for file in os.listdir(\"generated\")]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_code</th>\n",
       "      <th>pytest_code</th>\n",
       "      <th>coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def overlap_with(intervals, start, end):\\n    ...</td>\n",
       "      <td># Let's assume the source file is named 'sourc...</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def compute_loss(criterion, outputs, labels, b...</td>\n",
       "      <td>import torch\\n\\ndef compute_loss(criterion, ou...</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def compute_amp_fraction(df_shape_features):\\n...</td>\n",
       "      <td># test_source.py\\n\\nimport pytest\\nfrom source...</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>import torch\\n\\ndef gradient_to_contrastive_ex...</td>\n",
       "      <td>import pytest\\nfrom source import gradient_to_...</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def _mask_border_keypoints(image_shape, keypoi...</td>\n",
       "      <td>import pytest\\nimport numpy as np\\nfrom source...</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>def demand_response_resource_capacity(df, reso...</td>\n",
       "      <td>import pytest\\nimport pandas as pd\\nfrom sourc...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>def maml_inner_step(input, output, model, opti...</td>\n",
       "      <td>import pytest\\nimport sys\\nsys.path.append(\".\"...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>def net(f, c='close', o = 1):\\n    r\\n    new_...</td>\n",
       "      <td>import pytest\\nimport pandas as pd\\nfrom sourc...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>def vapour_pressure_deficit(svp, vp):\\n    r\\n...</td>\n",
       "      <td>import sys\\nsys.path.append(\".\")\\nimport sourc...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>def time_to_sample_number(seconds, frequency):...</td>\n",
       "      <td># test_source.py\\n\\nimport pytest\\nimport sour...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1524 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          original_code  \\\n",
       "0     def overlap_with(intervals, start, end):\\n    ...   \n",
       "1     def compute_loss(criterion, outputs, labels, b...   \n",
       "2     def compute_amp_fraction(df_shape_features):\\n...   \n",
       "3     import torch\\n\\ndef gradient_to_contrastive_ex...   \n",
       "4     def _mask_border_keypoints(image_shape, keypoi...   \n",
       "...                                                 ...   \n",
       "1519  def demand_response_resource_capacity(df, reso...   \n",
       "1520  def maml_inner_step(input, output, model, opti...   \n",
       "1521  def net(f, c='close', o = 1):\\n    r\\n    new_...   \n",
       "1522  def vapour_pressure_deficit(svp, vp):\\n    r\\n...   \n",
       "1523  def time_to_sample_number(seconds, frequency):...   \n",
       "\n",
       "                                            pytest_code  coverage  \n",
       "0     # Let's assume the source file is named 'sourc...     100.0  \n",
       "1     import torch\\n\\ndef compute_loss(criterion, ou...     100.0  \n",
       "2     # test_source.py\\n\\nimport pytest\\nfrom source...     100.0  \n",
       "3     import pytest\\nfrom source import gradient_to_...     100.0  \n",
       "4     import pytest\\nimport numpy as np\\nfrom source...     100.0  \n",
       "...                                                 ...       ...  \n",
       "1519  import pytest\\nimport pandas as pd\\nfrom sourc...       0.0  \n",
       "1520  import pytest\\nimport sys\\nsys.path.append(\".\"...       0.0  \n",
       "1521  import pytest\\nimport pandas as pd\\nfrom sourc...       0.0  \n",
       "1522  import sys\\nsys.path.append(\".\")\\nimport sourc...       0.0  \n",
       "1523  # test_source.py\\n\\nimport pytest\\nimport sour...       0.0  \n",
       "\n",
       "[1524 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all#[all.coverage]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coverage': 100.0,\n",
       " 'stdout': \"============================= test session starts =============================\\nplatform win32 -- Python 3.11.5, pytest-7.4.0, pluggy-1.0.0 -- c:\\\\ProgramData\\\\anaconda3\\\\python.exe\\ncachedir: .pytest_cache\\nrootdir: c:\\\\Master\\\\Disertatie\\\\data\\\\trial_error_generation\\\\tmp\\nplugins: cov-4.1.0, anyio-4.2.0\\ncollecting ... collected 6 items\\n\\ntest_source.py::test_amplitude_to_db_tensor_ref_scalar_amin FAILED       [ 16%]\\ntest_source.py::test_amplitude_to_db_scalar_ref_tensor_amin FAILED       [ 33%]\\ntest_source.py::test_amplitude_to_db_tensor_ref_scalar_amin_tensor FAILED [ 50%]\\ntest_source.py::test_amplitude_to_db_scalar_ref_tensor_amin_tensor FAILED [ 66%]\\ntest_source.py::test_amplitude_to_db_tensor_ref_tensor_amin_tensor FAILED [ 83%]\\ntest_source.py::test_amplitude_to_db_tensor_ref_float_amin_tensor FAILED [100%]\\n\\n================================== FAILURES ===================================\\n_________________ test_amplitude_to_db_tensor_ref_scalar_amin _________________\\n\\nx = tensor([1., 2., 3.]), ref = 1.0, amin = 1e-07\\n\\n    def test_amplitude_to_db_tensor_ref_scalar_amin(x, ref, amin):\\n        output = amplitude_to_db(x, ref=ref, amin=amin)\\n>       expected_output = tensor([-inf, -inf, -inf])  # since x is always positive and ref=1.0\\nE       NameError: name 'inf' is not defined\\n\\ntest_source.py:21: NameError\\n_________________ test_amplitude_to_db_scalar_ref_tensor_amin _________________\\n\\nx = tensor([1., 2., 3.]), ref = 1.0, amin = 1e-07\\n\\n    def test_amplitude_to_db_scalar_ref_tensor_amin(x, ref, amin):\\n>       output = amplitude_to_db(x.item(), ref=torch.tensor(ref), amin=amin)\\nE       RuntimeError: a Tensor with 3 elements cannot be converted to Scalar\\n\\ntest_source.py:26: RuntimeError\\n_____________ test_amplitude_to_db_tensor_ref_scalar_amin_tensor ______________\\n\\nx = tensor([1., 2., 3.]), ref = 1.0, amin = 1e-07\\n\\n    def test_amplitude_to_db_tensor_ref_scalar_amin_tensor(x, ref, amin):\\n>       output = amplitude_to_db(x, ref=ref, amin=torch.tensor(amin))\\nE       NameError: name 'torch' is not defined\\n\\ntest_source.py:32: NameError\\n_____________ test_amplitude_to_db_scalar_ref_tensor_amin_tensor ______________\\n\\nx = tensor([1., 2., 3.]), ref = 1.0, amin = 1e-07\\n\\n    def test_amplitude_to_db_scalar_ref_tensor_amin_tensor(x, ref, amin):\\n>       output = amplitude_to_db(x.item(), ref=torch.tensor(ref), amin=torch.tensor(amin))\\nE       RuntimeError: a Tensor with 3 elements cannot be converted to Scalar\\n\\ntest_source.py:38: RuntimeError\\n_____________ test_amplitude_to_db_tensor_ref_tensor_amin_tensor ______________\\n\\nx = tensor([1., 2., 3.]), ref = 1.0, amin = 1e-07\\n\\n    def test_amplitude_to_db_tensor_ref_tensor_amin_tensor(x, ref, amin):\\n>       output = amplitude_to_db(x.item(), ref=torch.tensor(ref), amin=torch.tensor(amin))\\nE       RuntimeError: a Tensor with 3 elements cannot be converted to Scalar\\n\\ntest_source.py:44: RuntimeError\\n______________ test_amplitude_to_db_tensor_ref_float_amin_tensor ______________\\n\\nx = tensor([1., 2., 3.]), ref = 1.0\\n\\n    def test_amplitude_to_db_tensor_ref_float_amin_tensor(x, ref):\\n>       output = amplitude_to_db(x.item(), ref=ref.numpy().astype(np.float32), amin=torch.tensor(1e-7))\\nE       RuntimeError: a Tensor with 3 elements cannot be converted to Scalar\\n\\ntest_source.py:50: RuntimeError\\n\\n---------- coverage: platform win32, python 3.11.5-final-0 -----------\\nName        Stmts   Miss  Cover   Missing\\n-----------------------------------------\\nsource.py       5      0   100%\\n-----------------------------------------\\nTOTAL           5      0   100%\\n\\n=========================== short test summary info ===========================\\nFAILED test_source.py::test_amplitude_to_db_tensor_ref_scalar_amin - NameError: name 'inf' is not defined\\nFAILED test_source.py::test_amplitude_to_db_scalar_ref_tensor_amin - RuntimeError: a Tensor with 3 elements cannot be converted to Scalar\\nFAILED test_source.py::test_amplitude_to_db_tensor_ref_scalar_amin_tensor - NameError: name 'torch' is not defined\\nFAILED test_source.py::test_amplitude_to_db_scalar_ref_tensor_amin_tensor - RuntimeError: a Tensor with 3 elements cannot be converted to Scalar\\nFAILED test_source.py::test_amplitude_to_db_tensor_ref_tensor_amin_tensor - RuntimeError: a Tensor with 3 elements cannot be converted to Scalar\\nFAILED test_source.py::test_amplitude_to_db_tensor_ref_float_amin_tensor - RuntimeError: a Tensor with 3 elements cannot be converted to Scalar\\n============================== 6 failed in 1.66s ==============================\\n\",\n",
       " 'stderr': '',\n",
       " 'failed_assertions': 6,\n",
       " 'fails': \"FAILED test_source.py::test_amplitude_to_db_tensor_ref_scalar_amin - NameError: name 'inf' is not defined\\nFAILED test_source.py::test_amplitude_to_db_scalar_ref_tensor_amin - RuntimeError: a Tensor with 3 elements cannot be converted to Scalar\\nFAILED test_source.py::test_amplitude_to_db_tensor_ref_scalar_amin_tensor - NameError: name 'torch' is not defined\\nFAILED test_source.py::test_amplitude_to_db_scalar_ref_tensor_amin_tensor - RuntimeError: a Tensor with 3 elements cannot be converted to Scalar\\nFAILED test_source.py::test_amplitude_to_db_tensor_ref_tensor_amin_tensor - RuntimeError: a Tensor with 3 elements cannot be converted to Scalar\\nFAILED test_source.py::test_amplitude_to_db_tensor_ref_float_amin_tensor - RuntimeError: a Tensor with 3 elements cannot be converted to Scalar\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytest_code = \"\"\"import pytest\n",
    "from torch import tensor\n",
    "import numpy as np\n",
    "from source import amplitude_to_db # imports the code from source file\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def x():\n",
    "    return tensor([1., 2., 3.])\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def ref():\n",
    "    return 1.\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def amin():\n",
    "    return 1e-7\n",
    "\n",
    "# test the function with a tensor input for x and a scalar input for ref and amin\n",
    "def test_amplitude_to_db_tensor_ref_scalar_amin(x, ref, amin):\n",
    "    output = amplitude_to_db(x, ref=ref, amin=amin)\n",
    "    expected_output = tensor([-inf, -inf, -inf])  # since x is always positive and ref=1.0\n",
    "    assert np.allclose(output, expected_output) \n",
    "    \n",
    "# test the function with a scalar input for x and a tensor input for ref and amin\n",
    "def test_amplitude_to_db_scalar_ref_tensor_amin(x, ref, amin):\n",
    "    output = amplitude_to_db(x.item(), ref=torch.tensor(ref), amin=amin)\n",
    "    expected_output = tensor([-inf, -inf, -inf])  # since x is always positive and ref=1.0\n",
    "    assert np.allclose(output, expected_output) \n",
    "    \n",
    "# test the function with a tensor input for x and a scalar input for ref but tensor input for amin\n",
    "def test_amplitude_to_db_tensor_ref_scalar_amin_tensor(x, ref, amin):\n",
    "    output = amplitude_to_db(x, ref=ref, amin=torch.tensor(amin))\n",
    "    expected_output = tensor([-inf, -inf, -inf])  # since x is always positive and ref=1.0\n",
    "    assert np.allclose(output, expected_output) \n",
    "    \n",
    "# test the function with a scalar input for x and a tensor input for both ref and amin\n",
    "def test_amplitude_to_db_scalar_ref_tensor_amin_tensor(x, ref, amin):\n",
    "    output = amplitude_to_db(x.item(), ref=torch.tensor(ref), amin=torch.tensor(amin))\n",
    "    expected_output = tensor([-inf, -inf, -inf])  # since x is always positive and ref=1.0\n",
    "    assert np.allclose(output, expected_output) \n",
    "    \n",
    "# test the function with a tensor input for all arguments\n",
    "def test_amplitude_to_db_tensor_ref_tensor_amin_tensor(x, ref, amin):\n",
    "    output = amplitude_to_db(x.item(), ref=torch.tensor(ref), amin=torch.tensor(amin))\n",
    "    expected_output = tensor([-inf, -inf, -inf])  # since x is always positive and ref=1.0\n",
    "    assert np.allclose(output, expected_output) \n",
    "    \n",
    "# test the function with a tensor input for all arguments but different types of ref and amin inputs\n",
    "def test_amplitude_to_db_tensor_ref_float_amin_tensor(x, ref):\n",
    "    output = amplitude_to_db(x.item(), ref=ref.numpy().astype(np.float32), amin=torch.tensor(1e-7))\n",
    "    expected_output = tensor([-inf, -inf, -inf])  # since x is always positive and ref=1.0\n",
    "    assert np.allclose(output, expected_output) \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "code = \"\"\"import torch\n",
    "\n",
    "def amplitude_to_db(x, ref=1.0, amin=1e-7):\n",
    "    \n",
    "    x = x.pow(2.)\n",
    "    x = torch.clamp(x, min=amin)\n",
    "    return 10.0 * (torch.log10(x) - torch.log10(torch.tensor(ref,\n",
    "                                                             device=x.device,\n",
    "                                                             requires_grad=False,\n",
    "                                                             dtype=x.dtype)))\"\"\"\n",
    "\n",
    "\n",
    "run_pytest(code, pytest_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adjustment_utils import modify_pytest_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "code, pytest_code = results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pytest\n",
      "import numpy as np\n",
      "from source import HamCenter1D_Hamiltonian\n",
      "\n",
      "def test_HamCenter1D_Hamiltonian():\n",
      "    t = 1\n",
      "    u = np.array([1, 2])\n",
      "    PARAMETERS = [1]\n",
      "    result = HamCenter1D_Hamiltonian(t, u, PARAMETERS)\n",
      "    assert np.isclose(result, 0.5 * 1 * (2 * 2 + 1 * 1)), 'The function HamCenter1D_Hamiltonian did not return the expected result'\n",
      "if __name__ == '__main__':\n",
      "    test_HamCenter1D_Hamiltonian()\n"
     ]
    }
   ],
   "source": [
    "print(modify_pytest_code(code, pytest_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# test_HamCenter1D_Hamiltonian.py\n",
      "import pytest\n",
      "import numpy as np\n",
      "from source import HamCenter1D_Hamiltonian\n",
      "\n",
      "def test_HamCenter1D_Hamiltonian():\n",
      "    # unit test case\n",
      "    t = 1  # time\n",
      "    u = np.array([1, 2])  # position\n",
      "    PARAMETERS = [1]  # Hamiltonian Model Parameter\n",
      "    result = HamCenter1D_Hamiltonian(t, u, PARAMETERS)\n",
      "    assert np.isclose(result, 0.5*1*(2*2 + 1*1)), \"The function HamCenter1D_Hamiltonian did not return the expected result\"\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    test_HamCenter1D_Hamiltonian()\n"
     ]
    }
   ],
   "source": [
    "print(pytest_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
